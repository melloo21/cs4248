{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T07:37:54.860515Z",
     "start_time": "2024-04-15T07:37:53.503517Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "from src.extraction.jsonl_data_reader import JsonlDataReader\n",
    "\n",
    "seed = 7\n",
    "random_state = RandomState(seed=seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_data = JsonlDataReader(file_name='train.jsonl').read()\n",
    "dev_data = JsonlDataReader(file_name='dev.jsonl').read()\n",
    "test_data = JsonlDataReader(file_name='test.jsonl').read()\n",
    "\n",
    "stage_order = ['preprocessing', 'tokenizer', 'post_tokenizer', 'vectorizer', 'model']\n",
    "stage_order_map = dict(zip(stage_order[:-1], stage_order[1:]))"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T03:05:26.818631Z",
     "start_time": "2024-04-15T03:05:25.795633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.vectorizer.sk_tfidf_vectorizer import SkTfidfVectorizer\n",
    "from src.vectorizer.w2v_vectorizer import W2vVectorizer\n",
    "from src.models.abstract_model import AbstractModel\n",
    "from src.vectorizer.lsi_vectorizer import LsiVectorizer\n",
    "from src.vectorizer.sk_count_vectorizer import SkCountVectorizer\n",
    "from src.post_tokenizer.phraser_merger import PhraserMerger\n",
    "from src.post_tokenizer.null_post_tokenizer import NullPostTokenizer\n",
    "from src.tokenize.sentence_piece_tokenizer import SentencePieceTokenizer\n",
    "from src.tokenize.null_tokenizer import NullTokenizer\n",
    "from src.tokenize.spacy_tokenizer import SpacyTokenizer\n",
    "from src.preprocessing.simple_preprocessor import SimplePreprocessor\n",
    "\n",
    "stage_config = {\n",
    "    'preprocessing': {\n",
    "        SimplePreprocessor: {\n",
    "            'remove_citations': (True, False),\n",
    "            'remove_duplicates': (True, False),\n",
    "        }\n",
    "    },\n",
    "    'tokenizer': {\n",
    "        SpacyTokenizer: {\n",
    "            'replace_numbers': (True, False),\n",
    "            'remove_stopwords': (True, False),\n",
    "            'lemmatize': (True, False),\n",
    "        },\n",
    "        NullTokenizer: {},\n",
    "        SentencePieceTokenizer: {\n",
    "            'vocab_size': (5000, 10000),\n",
    "        },\n",
    "    },\n",
    "    'post_tokenizer': {\n",
    "        NullPostTokenizer: {},\n",
    "        PhraserMerger: {\n",
    "            'num_gram': (1, 2),\n",
    "        },\n",
    "    },\n",
    "    'vectorizer': {\n",
    "        SkTfidfVectorizer: {\n",
    "            'ngram_range': ((1, 1), (1, 2)),\n",
    "            'binary': (True, False),\n",
    "        },\n",
    "        SkCountVectorizer: {\n",
    "            'ngram_range': ((1, 1), (1, 2)),\n",
    "            'binary': (True, False),\n",
    "        },\n",
    "        W2vVectorizer: {},\n",
    "        LsiVectorizer: {},\n",
    "    },\n",
    "    'model': {\n",
    "        AbstractModel: {},\n",
    "    }\n",
    "}"
   ],
   "id": "2639e651e4c335dd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T03:05:26.834634Z",
     "start_time": "2024-04-15T03:05:26.819634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stage_config = {\n",
    "    'preprocessing': {\n",
    "        SimplePreprocessor: {}\n",
    "    },\n",
    "    'tokenizer': {\n",
    "        SpacyTokenizer: {}\n",
    "    },\n",
    "    'post_tokenizer': {\n",
    "        NullPostTokenizer: {},\n",
    "    },\n",
    "    'vectorizer': {\n",
    "        SkTfidfVectorizer: {}\n",
    "    },\n",
    "    'model': {\n",
    "        AbstractModel: {},\n",
    "    }\n",
    "}"
   ],
   "id": "8873bdd89a91fab7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T06:37:39.563114Z",
     "start_time": "2024-04-15T06:37:39.539114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def run_preprocessing(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running preprocessing: {run_class}: {args=}, {current_state=}')\n",
    "    current_obj = run_class(**args)\n",
    "    preprocessed_train = current_obj.preprocess(train_data)\n",
    "    preprocessed_dev = current_obj.preprocess(dev_data)\n",
    "    preprocessed_test = current_obj.preprocess(test_data)\n",
    "    return (preprocessed_train, preprocessed_dev, preprocessed_test), current_obj\n",
    "\n",
    "\n",
    "def run_tokenizer(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running tokenizer: {run_class}: {args=}, {current_state=}')\n",
    "    tokenizer = run_class(\n",
    "        **args\n",
    "    )\n",
    "    train, dev, test = prev_output\n",
    "    tokenizer.fit(train)\n",
    "    results = tuple(tokenizer.tokenize(x) for x in prev_output)\n",
    "    return results, tokenizer\n",
    "\n",
    "\n",
    "def run_post_tokenizer(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running post_tokenizer: {run_class}: {args=}, {current_state=}')\n",
    "    post_tokenizer = run_class(**args)\n",
    "    train, dev, test = prev_output\n",
    "    post_tokenizer.fit(train)\n",
    "    results = tuple(post_tokenizer.transform(x) for x in prev_output)\n",
    "    return results, post_tokenizer\n",
    "\n",
    "\n",
    "def run_vectorizer(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running vectorizer: {run_class}: {args=}, {current_state=}')\n",
    "    vectorizer = run_class(**args)\n",
    "    train, dev, test = prev_output\n",
    "    vectorizer.fit(train)\n",
    "    results = tuple(vectorizer.transform(x) for x in prev_output)\n",
    "    return results, vectorizer\n",
    "\n",
    "\n",
    "def run_model(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running model: {run_class}: {args=}, {current_state=}')\n",
    "    \n",
    "    models = {\n",
    "        # 'LR': LogisticRegression(max_iter=2000),\n",
    "        'LR_10': LogisticRegression(max_iter=2000, C=10),\n",
    "    }\n",
    "    train, dev, test = prev_output\n",
    "\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        model_result = {'model_name': model_name}\n",
    "        model.fit(train.vectors, train.label_indices)\n",
    "        y_pred_train = model.predict(train.vectors)\n",
    "        score = f1_score(train.label_indices, y_pred_train, average='macro')\n",
    "        model_result['train_f1'] = score\n",
    "\n",
    "        y_pred_dev = model.predict(dev.vectors)\n",
    "        score = f1_score(dev.label_indices, y_pred_dev, average='macro')\n",
    "        model_result['dev_f1'] = score\n",
    "\n",
    "        y_pred_test = model.predict(test.vectors)\n",
    "        score = f1_score(test.label_indices, y_pred_test, average='macro')\n",
    "        model_result['test_f1'] = score\n",
    "        results.append(model_result)\n",
    "        print(f'{model_result=}')\n",
    "\n",
    "    return results, None\n",
    "\n",
    "\n",
    "def run(stage: str, run_class, args: dict, prev_output=None, current_state=None, current_models=None) -> list[dict]:\n",
    "    print(f'{run_class.__name__} {args=}')\n",
    "    if current_state is None:\n",
    "        current_state = dict()\n",
    "    current_state = {**current_state, stage: run_class.__name__}\n",
    "    if current_models is None:\n",
    "        current_models = dict()\n",
    "    results = []\n",
    "    next_stage = stage_order_map.get(stage)\n",
    "\n",
    "    run_func = {\n",
    "        'preprocessing': run_preprocessing,\n",
    "        'tokenizer': run_tokenizer,\n",
    "        'post_tokenizer': run_post_tokenizer,\n",
    "        'vectorizer': run_vectorizer,\n",
    "        'model': run_model,\n",
    "    }\n",
    "\n",
    "    if not args:\n",
    "        result, model = run_func[stage](run_class, args, prev_output, current_state, current_models)\n",
    "        new_models = {**current_models, stage: model}\n",
    "        if next_stage is not None:\n",
    "            for next_class, next_args in stage_config[next_stage].items():\n",
    "                new_state = current_state\n",
    "                run_result = run(next_stage, next_class, next_args, result, new_state, new_models)\n",
    "                results.extend(run_result)\n",
    "            return results\n",
    "        else:\n",
    "            result_state = [{**current_state, **result_row} for result_row in result]\n",
    "            return result_state\n",
    "\n",
    "    argument_permutations = list(product(*args.values()))\n",
    "    for values in argument_permutations:\n",
    "        new_arg = dict(zip(args.keys(), values))\n",
    "        result, model = run_func[stage](run_class, new_arg, prev_output, current_state, current_models)\n",
    "        new_models = {**current_models, stage: model}\n",
    "        if next_stage is not None:\n",
    "            for next_class, next_args in stage_config[next_stage].items():\n",
    "                new_state = {**current_state, **new_arg}\n",
    "                run_result = run(next_stage, next_class, next_args, result, new_state, new_models)\n",
    "                results.extend(run_result)\n",
    "        else:\n",
    "            result_state = [{**current_state, **result_row} for result_row in result]\n",
    "            results.extend(result_state)\n",
    "    return results\n",
    "\n"
   ],
   "id": "64808093f611525e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T06:38:14.833235Z",
     "start_time": "2024-04-15T06:37:41.794235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    results = run('preprocessing', SimplePreprocessor, {}, None)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "id": "123d0eca2a1e8331",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimplePreprocessor args={}\n",
      "running preprocessing: <class 'src.preprocessing.simple_preprocessor.SimplePreprocessor'>: args={}, current_state={'preprocessing': 'SimplePreprocessor'}\n",
      "SpacyTokenizer args={}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={}, current_state={'preprocessing': 'SimplePreprocessor', 'tokenizer': 'SpacyTokenizer'}\n",
      "NullPostTokenizer args={}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'preprocessing': 'SimplePreprocessor', 'tokenizer': 'SpacyTokenizer', 'post_tokenizer': 'NullPostTokenizer'}\n",
      "SkTfidfVectorizer args={}\n",
      "running vectorizer: <class 'src.vectorizer.sk_tfidf_vectorizer.SkTfidfVectorizer'>: args={}, current_state={'preprocessing': 'SimplePreprocessor', 'tokenizer': 'SpacyTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkTfidfVectorizer'}\n",
      "AbstractModel args={}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'preprocessing': 'SimplePreprocessor', 'tokenizer': 'SpacyTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkTfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR_10', 'train_f1': 1.0, 'dev_f1': 0.813123989880431, 'test_f1': 0.8177940226137995}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        preprocessing       tokenizer     post_tokenizer         vectorizer  \\\n",
       "0  SimplePreprocessor  SpacyTokenizer  NullPostTokenizer  SkTfidfVectorizer   \n",
       "\n",
       "           model model_name  train_f1    dev_f1   test_f1  \n",
       "0  AbstractModel      LR_10       1.0  0.813124  0.817794  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>post_tokenizer</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>dev_f1</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>SkTfidfVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR_10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.813124</td>\n",
       "      <td>0.817794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T19:46:39.972279Z",
     "start_time": "2024-04-14T19:46:39.861281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.utils.path_getter import PathGetter\n",
    "\n",
    "results_df.to_parquet(PathGetter.get_data_directory()/'experiments2.parquet')"
   ],
   "id": "96564c4815c35f9a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T07:37:57.036605Z",
     "start_time": "2024-04-15T07:37:56.865738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.utils.path_getter import PathGetter\n",
    "\n",
    "results_df = pd.read_parquet(PathGetter.get_data_directory()/'experiments2.parquet')"
   ],
   "id": "eca264871fae67b6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T07:38:01.241750Z",
     "start_time": "2024-04-15T07:38:01.216750Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.sort_values(by=['dev_f1'], ascending=False)",
   "id": "b0a7c0cf57aae4c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            pretokenizer  remove_citations  remove_duplicates  \\\n",
       "1052  SimplePreprocessor             False              False   \n",
       "1172  SimplePreprocessor             False              False   \n",
       "607   SimplePreprocessor              True              False   \n",
       "1103  SimplePreprocessor             False              False   \n",
       "182   SimplePreprocessor              True               True   \n",
       "...                  ...               ...                ...   \n",
       "628   SimplePreprocessor              True              False   \n",
       "298   SimplePreprocessor              True               True   \n",
       "1278  SimplePreprocessor             False              False   \n",
       "958   SimplePreprocessor             False               True   \n",
       "1288  SimplePreprocessor             False              False   \n",
       "\n",
       "                   tokenizer replace_numbers remove_stopwords lemmatize  \\\n",
       "1052          SpacyTokenizer            True            False      True   \n",
       "1172          SpacyTokenizer           False            False      True   \n",
       "607   SentencePieceTokenizer            None             None      None   \n",
       "1103          SpacyTokenizer            True            False     False   \n",
       "182           SpacyTokenizer           False            False      True   \n",
       "...                      ...             ...              ...       ...   \n",
       "628   SentencePieceTokenizer            None             None      None   \n",
       "298   SentencePieceTokenizer            None             None      None   \n",
       "1278  SentencePieceTokenizer            None             None      None   \n",
       "958   SentencePieceTokenizer            None             None      None   \n",
       "1288  SentencePieceTokenizer            None             None      None   \n",
       "\n",
       "         post_tokenizer         vectorizer ngram_range binary          model  \\\n",
       "1052  NullPostTokenizer  SkTfidfVectorizer      [1, 2]   True  AbstractModel   \n",
       "1172  NullPostTokenizer  SkTfidfVectorizer      [1, 2]   True  AbstractModel   \n",
       "607   NullPostTokenizer  SkCountVectorizer      [1, 2]  False  AbstractModel   \n",
       "1103      PhraserMerger  SkTfidfVectorizer      [1, 2]  False  AbstractModel   \n",
       "182   NullPostTokenizer  SkTfidfVectorizer      [1, 2]   True  AbstractModel   \n",
       "...                 ...                ...         ...    ...            ...   \n",
       "628       PhraserMerger      W2vVectorizer        None   None  AbstractModel   \n",
       "298       PhraserMerger      W2vVectorizer        None   None  AbstractModel   \n",
       "1278      PhraserMerger      W2vVectorizer        None   None  AbstractModel   \n",
       "958       PhraserMerger      W2vVectorizer        None   None  AbstractModel   \n",
       "1288      PhraserMerger      W2vVectorizer        None   None  AbstractModel   \n",
       "\n",
       "     model_name  train_f1    dev_f1   test_f1  num_gram  vocab_size  \n",
       "1052         LR  0.913685  0.812795  0.803838       NaN         NaN  \n",
       "1172         LR  0.909728  0.812787  0.797629       NaN         NaN  \n",
       "607          LR  0.999708  0.812498  0.796079       NaN      5000.0  \n",
       "1103         LR  0.909680  0.812417  0.810676       2.0         NaN  \n",
       "182          LR  0.910774  0.812396  0.809648       NaN         NaN  \n",
       "...         ...       ...       ...       ...       ...         ...  \n",
       "628          LR  0.341098  0.344971  0.310504       2.0      5000.0  \n",
       "298          LR  0.361394  0.331895  0.326152       2.0      5000.0  \n",
       "1278         LR  0.344085  0.324265  0.309044       1.0      5000.0  \n",
       "958          LR  0.294744  0.295712  0.282799       2.0      5000.0  \n",
       "1288         LR  0.293498  0.285431  0.277823       2.0      5000.0  \n",
       "\n",
       "[1320 rows x 18 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pretokenizer</th>\n",
       "      <th>remove_citations</th>\n",
       "      <th>remove_duplicates</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>replace_numbers</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>lemmatize</th>\n",
       "      <th>post_tokenizer</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>binary</th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>dev_f1</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>num_gram</th>\n",
       "      <th>vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>SkTfidfVectorizer</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>True</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.913685</td>\n",
       "      <td>0.812795</td>\n",
       "      <td>0.803838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>SkTfidfVectorizer</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>True</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.909728</td>\n",
       "      <td>0.812787</td>\n",
       "      <td>0.797629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>SkCountVectorizer</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>False</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.999708</td>\n",
       "      <td>0.812498</td>\n",
       "      <td>0.796079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>SkTfidfVectorizer</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>False</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.909680</td>\n",
       "      <td>0.812417</td>\n",
       "      <td>0.810676</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>SkTfidfVectorizer</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>True</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.910774</td>\n",
       "      <td>0.812396</td>\n",
       "      <td>0.809648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.341098</td>\n",
       "      <td>0.344971</td>\n",
       "      <td>0.310504</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.361394</td>\n",
       "      <td>0.331895</td>\n",
       "      <td>0.326152</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.344085</td>\n",
       "      <td>0.324265</td>\n",
       "      <td>0.309044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.294744</td>\n",
       "      <td>0.295712</td>\n",
       "      <td>0.282799</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.293498</td>\n",
       "      <td>0.285431</td>\n",
       "      <td>0.277823</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 18 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (cs4248 project)",
   "language": "python",
   "name": "cs4248_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

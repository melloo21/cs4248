{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T03:19:44.688426Z",
     "start_time": "2024-04-08T03:19:42.636429Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.extraction.jsonl_data_reader import JsonlDataReader\n",
    "\n",
    "train_data = JsonlDataReader(file_name='train.jsonl').read()\n",
    "dev_data = JsonlDataReader(file_name='dev.jsonl').read()\n",
    "test_data = JsonlDataReader(file_name='test.jsonl').read()\n",
    "\n",
    "stage_order = ['pretokenizer', 'tokenizer', 'post_tokenizer', 'vectorizer', 'model']\n",
    "stage_order_map = dict(zip(stage_order[:-1], stage_order[1:]))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T03:19:45.612427Z",
     "start_time": "2024-04-08T03:19:44.689430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.vectorizer.w2v_vectorizer import W2vVectorizer\n",
    "from src.models.abstract_model import AbstractModel\n",
    "from src.vectorizer.lsi_vectorizer import LsiVectorizer\n",
    "from src.vectorizer.sk_count_vectorizer import SkCountVectorizer\n",
    "from src.vectorizer.tfidf_vectorizer import TfidfVectorizer\n",
    "from src.post_tokenizer.phraser_merger import PhraserMerger\n",
    "from src.post_tokenizer.null_post_tokenizer import NullPostTokenizer\n",
    "from src.tokenize.sentence_piece_tokenizer import SentencePieceTokenizer\n",
    "from src.tokenize.null_tokenizer import NullTokenizer\n",
    "from src.tokenize.spacy_tokenizer import SpacyTokenizer\n",
    "from src.preprocessing.simple_preprocessor import SimplePreprocessor\n",
    "\n",
    "stage_config = {\n",
    "    'preprocessing': {\n",
    "        SimplePreprocessor: {\n",
    "            'remove_citations': (True, False),\n",
    "            'remove_duplicates': (True, False),\n",
    "        }\n",
    "    },\n",
    "    'tokenizer': {\n",
    "        SpacyTokenizer: {\n",
    "            'replace_numbers': (True, False),\n",
    "            'remove_stopwords': (True, False),\n",
    "        },\n",
    "        NullTokenizer: {},\n",
    "        SentencePieceTokenizer: {\n",
    "            'vocab_size': (5000, 10000),\n",
    "        },\n",
    "    },\n",
    "    'post_tokenizer': {\n",
    "        NullPostTokenizer: {},\n",
    "        PhraserMerger: {\n",
    "            'num_gram': (1, 2),\n",
    "        },\n",
    "    },\n",
    "    'vectorizer': {\n",
    "        TfidfVectorizer: {},\n",
    "        SkCountVectorizer: {\n",
    "            'ignore_preprocessing': (True, False),\n",
    "        },\n",
    "        W2vVectorizer: {},\n",
    "        LsiVectorizer: {},\n",
    "        # FastTextW2vVectorizer: {},\n",
    "    },\n",
    "    'model': {\n",
    "        AbstractModel: {},\n",
    "    }\n",
    "}"
   ],
   "id": "2639e651e4c335dd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T03:19:45.676446Z",
     "start_time": "2024-04-08T03:19:45.613428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def run_preprocessing(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running preprocessing: {run_class}: {args=}, {current_state=}')\n",
    "    current_obj = run_class(**args)\n",
    "    preprocessed_train = current_obj.preprocess(train_data)\n",
    "    preprocessed_dev = current_obj.preprocess(dev_data)\n",
    "    preprocessed_test = current_obj.preprocess(test_data)\n",
    "    return (preprocessed_train, preprocessed_dev, preprocessed_test), current_obj\n",
    "\n",
    "\n",
    "def run_tokenizer(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running tokenizer: {run_class}: {args=}, {current_state=}')\n",
    "    tokenizer = run_class(\n",
    "        **args\n",
    "    )\n",
    "    train, dev, test = prev_output\n",
    "    tokenizer.fit(train)\n",
    "    results = tuple(tokenizer.tokenize(x) for x in prev_output)\n",
    "    return results, tokenizer\n",
    "\n",
    "\n",
    "def run_post_tokenizer(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running post_tokenizer: {run_class}: {args=}, {current_state=}')\n",
    "    post_tokenizer = run_class(**args)\n",
    "    train, dev, test = prev_output\n",
    "    post_tokenizer.fit(train)\n",
    "    results = tuple(post_tokenizer.transform(x) for x in prev_output)\n",
    "    return results, post_tokenizer\n",
    "\n",
    "\n",
    "def run_vectorizer(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running vectorizer: {run_class}: {args=}, {current_state=}')\n",
    "    vectorizer = run_class(**args)\n",
    "    train, dev, test = prev_output\n",
    "    vectorizer.fit(train)\n",
    "    results = tuple(vectorizer.transform(x) for x in prev_output)\n",
    "    return results, vectorizer\n",
    "\n",
    "\n",
    "def run_model(run_class, args, prev_output, current_state, current_models):\n",
    "    print(f'running model: {run_class}: {args=}, {current_state=}')\n",
    "    models = {\n",
    "        'LR': LogisticRegression(max_iter=2000),\n",
    "        # 'svm': SVC(kernel='rbf'),\n",
    "    }\n",
    "    train, dev, test = prev_output\n",
    "\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        model_result = {'model_name': model_name}\n",
    "        model.fit(train.vectors, train.label_indices)\n",
    "        y_pred_train = model.predict(train.vectors)\n",
    "        score = f1_score(train.label_indices, y_pred_train, average='macro')\n",
    "        model_result['train_f1'] = score\n",
    "\n",
    "        y_pred_dev = model.predict(dev.vectors)\n",
    "        score = f1_score(dev.label_indices, y_pred_dev, average='macro')\n",
    "        model_result['dev_f1'] = score\n",
    "\n",
    "        y_pred_test = model.predict(test.vectors)\n",
    "        score = f1_score(test.label_indices, y_pred_test, average='macro')\n",
    "        model_result['test_f1'] = score\n",
    "        results.append(model_result)\n",
    "        print(f'{model_result=}')\n",
    "\n",
    "    return results, None\n",
    "\n",
    "\n",
    "def run(stage: str, run_class, args: dict, prev_output=None, current_state=None, current_models=None) -> list[dict]:\n",
    "    if current_state is None:\n",
    "        current_state = dict()\n",
    "    current_state = {**current_state, stage: run_class.__name__}\n",
    "    if current_models is None:\n",
    "        current_models = dict()\n",
    "    results = []\n",
    "    next_stage = stage_order_map.get(stage)\n",
    "\n",
    "    run_func = {\n",
    "        'pretokenizer': run_preprocessing,\n",
    "        'tokenizer': run_tokenizer,\n",
    "        'post_tokenizer': run_post_tokenizer,\n",
    "        'vectorizer': run_vectorizer,\n",
    "        'model': run_model,\n",
    "    }\n",
    "\n",
    "    if not args:\n",
    "        result, model = run_func[stage](run_class, args, prev_output, current_state, current_models)\n",
    "        new_models = {**current_models, stage: model}\n",
    "        if next_stage is not None:\n",
    "            for next_class, next_args in stage_config[next_stage].items():\n",
    "                new_state = current_state\n",
    "                run_result = run(next_stage, next_class, next_args, result, new_state, new_models)\n",
    "                results.extend(run_result)\n",
    "            return results\n",
    "        else:\n",
    "            result_state = [{**current_state, **result_row} for result_row in result]\n",
    "            return result_state\n",
    "\n",
    "    argument_permutations = list(product(*args.values()))\n",
    "    for values in argument_permutations:\n",
    "        new_arg = dict(zip(args.keys(), values))\n",
    "        result, model = run_func[stage](run_class, new_arg, prev_output, current_state, current_models)\n",
    "        new_models = {**current_models, stage: model}\n",
    "        if next_stage is not None:\n",
    "            for next_class, next_args in stage_config[next_stage].items():\n",
    "                new_state = {**current_state, **new_arg}\n",
    "                run_result = run(next_stage, next_class, next_args, result, new_state, new_models)\n",
    "                results.extend(run_result)\n",
    "        else:\n",
    "            result_state = [{**current_state, **result_row} for result_row in result]\n",
    "            results.extend(result_state)\n",
    "    return results\n",
    "\n"
   ],
   "id": "64808093f611525e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T05:31:05.718258Z",
     "start_time": "2024-04-08T03:19:45.678447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = run('pretokenizer', SimplePreprocessor, {\n",
    "    'remove_citations': (True, False),\n",
    "    'remove_duplicates': (True, False),\n",
    "}, None)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "id": "123d0eca2a1e8331",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running preprocessing: <class 'src.preprocessing.simple_preprocessor.SimplePreprocessor'>: args={'remove_citations': True, 'remove_duplicates': True}, current_state={'pretokenizer': 'SimplePreprocessor'}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': True, 'remove_stopwords': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.870372458157158, 'dev_f1': 0.7361468186595522, 'test_f1': 0.7234525763976967}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8067378232163117, 'dev_f1': 0.6740916345794465, 'test_f1': 0.6288266039004315}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7531704588618823, 'test_f1': 0.7374555141359469}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.663951197610339, 'dev_f1': 0.6318703339121413, 'test_f1': 0.6354059119468157}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.728142367059796, 'dev_f1': 0.7103481176960939, 'test_f1': 0.6901484174999825}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8784983509241822, 'dev_f1': 0.7657474691122285, 'test_f1': 0.7390963405314027}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8196356714368359, 'dev_f1': 0.6868539274996469, 'test_f1': 0.6733361375095881}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7567400642172316, 'test_f1': 0.7402708234134193}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6612134084484934, 'dev_f1': 0.6421697010680942, 'test_f1': 0.6444266486732159}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7365694954165108, 'dev_f1': 0.7229087796335217, 'test_f1': 0.7116138627118381}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8802102332963445, 'dev_f1': 0.7645412322288777, 'test_f1': 0.7352611432146975}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8188152338105471, 'dev_f1': 0.673300115250786, 'test_f1': 0.6742040050270183}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7579397804890604, 'test_f1': 0.7386308321712652}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6549990350785424, 'dev_f1': 0.6402288853726662, 'test_f1': 0.644776721327169}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7347042011017777, 'dev_f1': 0.7243502806465884, 'test_f1': 0.7147618200415781}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': True, 'remove_stopwords': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8865196928160425, 'dev_f1': 0.7692254688784544, 'test_f1': 0.7729442092408966}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8340909464969837, 'dev_f1': 0.7195815518445882, 'test_f1': 0.6815591590889393}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7997243202297858, 'test_f1': 0.7935539083091866}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6618545809208781, 'dev_f1': 0.6628119439995678, 'test_f1': 0.6658968927795063}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7733757278915232, 'dev_f1': 0.7567271525449696, 'test_f1': 0.7651844187726473}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8962285264698693, 'dev_f1': 0.7991408849765557, 'test_f1': 0.7993655630065192}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8565454886867846, 'dev_f1': 0.7391725921137686, 'test_f1': 0.7100877190035143}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7959388226278857, 'test_f1': 0.7850565618552516}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6430649469662008, 'dev_f1': 0.6454219979757391, 'test_f1': 0.6337572350171338}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7917920574543085, 'dev_f1': 0.7572181281884159, 'test_f1': 0.7806559814141444}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8960469302298214, 'dev_f1': 0.7843109301018645, 'test_f1': 0.7975062097241513}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8544608159127912, 'dev_f1': 0.7344472080371812, 'test_f1': 0.7139960589366524}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7956099443643447, 'test_f1': 0.7804004366681131}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6082251177601726, 'dev_f1': 0.630419483051062, 'test_f1': 0.6078305202305035}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7842860671074919, 'dev_f1': 0.7555220239917785, 'test_f1': 0.7789241957085179}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': False, 'remove_stopwords': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8721392552891923, 'dev_f1': 0.7285770386898432, 'test_f1': 0.7156991967956584}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8129929540514684, 'dev_f1': 0.6629491199213079, 'test_f1': 0.6382525022343971}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7507918315498762, 'test_f1': 0.733030327883899}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6654556956891087, 'dev_f1': 0.6412047367724063, 'test_f1': 0.6375356238138535}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7224613999154025, 'dev_f1': 0.6865182591174145, 'test_f1': 0.6812820973609363}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8804137079912949, 'dev_f1': 0.7647064744634261, 'test_f1': 0.738524502355819}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.828491110557327, 'dev_f1': 0.6732234799554373, 'test_f1': 0.6860970867899895}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7530267709337174, 'test_f1': 0.7364381486979892}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6524013415556683, 'dev_f1': 0.6307260540332051, 'test_f1': 0.6291200096692487}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.734637612482663, 'dev_f1': 0.7167088985516122, 'test_f1': 0.7047048774177945}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8809012298384434, 'dev_f1': 0.7621357381988746, 'test_f1': 0.7290754624551056}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8304752696863043, 'dev_f1': 0.6737233622783155, 'test_f1': 0.6746834029725582}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7539101440798688, 'test_f1': 0.7350369488631067}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.648706648230903, 'dev_f1': 0.6124692436473429, 'test_f1': 0.6274107771517187}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7325628184771755, 'dev_f1': 0.6976475729953768, 'test_f1': 0.701986185812922}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': False, 'remove_stopwords': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8854460502340656, 'dev_f1': 0.7672659173681852, 'test_f1': 0.7705241980128905}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8411243238758074, 'dev_f1': 0.7148410737183367, 'test_f1': 0.6745684020554794}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.8038430521737321, 'test_f1': 0.7931243357339813}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6683352665489609, 'dev_f1': 0.6664953712038143, 'test_f1': 0.6723490653014833}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7736480052400138, 'dev_f1': 0.7646367619179172, 'test_f1': 0.7602928996826135}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8984743653359389, 'dev_f1': 0.795038439450838, 'test_f1': 0.7978926744187155}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8625427635437748, 'dev_f1': 0.7263684961163953, 'test_f1': 0.7095631835357863}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.8009329508566139, 'test_f1': 0.7930516483802341}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6263777060086584, 'dev_f1': 0.6278223368163408, 'test_f1': 0.6323001150515898}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.788252649262196, 'dev_f1': 0.7644299545430506, 'test_f1': 0.7825303871405986}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8982032632480728, 'dev_f1': 0.7845245707076876, 'test_f1': 0.7968965869736615}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8656650553905633, 'dev_f1': 0.7274760163844144, 'test_f1': 0.7166331469776002}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7908139310241197, 'test_f1': 0.788968199473174}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5707176233615214, 'dev_f1': 0.5754696034139494, 'test_f1': 0.5503700307664215}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7855756546933811, 'dev_f1': 0.7662244752492923, 'test_f1': 0.772893149192185}\n",
      "running tokenizer: <class 'src.tokenize.null_tokenizer.NullTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8955857027689101, 'dev_f1': 0.7874294939011272, 'test_f1': 0.7997774522265123}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9371066057822911, 'dev_f1': 0.7050388312710189, 'test_f1': 0.6937663548007681}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.8001477222355525, 'test_f1': 0.8084549693024973}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6769373772548565, 'dev_f1': 0.6463842474000487, 'test_f1': 0.7046655561039499}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7707695355777825, 'dev_f1': 0.7599105036212784, 'test_f1': 0.7659140369783132}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.900071564976713, 'dev_f1': 0.7824025289778714, 'test_f1': 0.8092620286050943}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9497887429969052, 'dev_f1': 0.7369006731416121, 'test_f1': 0.7192722460309146}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7929394919253038, 'test_f1': 0.8019987570614099}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6416133677259331, 'dev_f1': 0.6169814150139249, 'test_f1': 0.6377770570439584}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7791113812954439, 'dev_f1': 0.7758822343214358, 'test_f1': 0.7868261638513788}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.896357601478981, 'dev_f1': 0.7795699489135134, 'test_f1': 0.8068508579887145}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9488091114178001, 'dev_f1': 0.7235146172618158, 'test_f1': 0.715906579467675}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7985340414710734, 'test_f1': 0.7983337790421915}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5979846458147617, 'dev_f1': 0.5864135817105758, 'test_f1': 0.5932670303376905}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7788258552369177, 'dev_f1': 0.7714708920992793, 'test_f1': 0.7825119504153853}\n",
      "running tokenizer: <class 'src.tokenize.sentence_piece_tokenizer.SentencePieceTokenizer'>: args={'vocab_size': 5000}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8764884441576171, 'dev_f1': 0.7634228890245, 'test_f1': 0.7851013625996828}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8565572536296923, 'dev_f1': 0.7194297444651352, 'test_f1': 0.7278957809587331}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.7975728966291528, 'test_f1': 0.7907894293640613}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.532197188158335, 'dev_f1': 0.4810028312592293, 'test_f1': 0.5188347042599345}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.726133758179523, 'dev_f1': 0.7107899167864415, 'test_f1': 0.7225850532161688}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8937418816134688, 'dev_f1': 0.7851649913030987, 'test_f1': 0.8007472072774992}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8890075795144282, 'dev_f1': 0.7178332503409903, 'test_f1': 0.7352669849103757}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7957071983630647, 'test_f1': 0.790602220011274}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.4089996803450359, 'dev_f1': 0.3809211463553357, 'test_f1': 0.3857549849150064}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7712188425306836, 'dev_f1': 0.7573790732848734, 'test_f1': 0.7731895504212606}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8928869039422929, 'dev_f1': 0.7760445729458877, 'test_f1': 0.7999630407718644}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8907581585170742, 'dev_f1': 0.7177068160023955, 'test_f1': 0.7329180297947223}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7854240696122924, 'test_f1': 0.7910145913450064}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.37273234257862414, 'dev_f1': 0.3549101908919107, 'test_f1': 0.34194174955004947}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7697138755235288, 'dev_f1': 0.7513177230982383, 'test_f1': 0.7741955821674634}\n",
      "running tokenizer: <class 'src.tokenize.sentence_piece_tokenizer.SentencePieceTokenizer'>: args={'vocab_size': 10000}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8813588553003201, 'dev_f1': 0.7591998301656622, 'test_f1': 0.7779447427688634}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8781994823102718, 'dev_f1': 0.7082592727260139, 'test_f1': 0.6981941979784217}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998007816475948, 'dev_f1': 0.8101988532737475, 'test_f1': 0.7979161564471511}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5648682113793929, 'dev_f1': 0.5227970289886928, 'test_f1': 0.5479229784614378}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7582983122947243, 'dev_f1': 0.7314451862942709, 'test_f1': 0.7558622611114655}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8970864401814747, 'dev_f1': 0.7816996115776247, 'test_f1': 0.7968022929373838}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.903722655293474, 'dev_f1': 0.7182700387592148, 'test_f1': 0.7304505897679939}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7942740703148341, 'test_f1': 0.7957758003460779}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.44650966005815457, 'dev_f1': 0.42031952343029805, 'test_f1': 0.43461743356643184}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7784475746492561, 'dev_f1': 0.7732101112057853, 'test_f1': 0.7811880509343898}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8988211360434945, 'dev_f1': 0.7740134405242629, 'test_f1': 0.7908778224257563}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9113135436809888, 'dev_f1': 0.7220894317510217, 'test_f1': 0.7125672601772687}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7965804470197235, 'test_f1': 0.7911386335537278}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.42805148438555407, 'dev_f1': 0.4239615935317799, 'test_f1': 0.4031326763642206}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7748550774348798, 'dev_f1': 0.7556590507984824, 'test_f1': 0.7691627928187218}\n",
      "running preprocessing: <class 'src.preprocessing.simple_preprocessor.SimplePreprocessor'>: args={'remove_citations': True, 'remove_duplicates': False}, current_state={'pretokenizer': 'SimplePreprocessor'}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': True, 'remove_stopwords': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8711254765913182, 'dev_f1': 0.731500455239274, 'test_f1': 0.7254453907518252}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.7975263061243755, 'dev_f1': 0.6678015095834621, 'test_f1': 0.6269755949622837}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996010029243213, 'dev_f1': 0.7499035252543136, 'test_f1': 0.7400143456126763}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6732955158003522, 'dev_f1': 0.6466697905199624, 'test_f1': 0.6353012341466292}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7298012905088097, 'dev_f1': 0.6986499084283858, 'test_f1': 0.6851096255858161}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.879924801471398, 'dev_f1': 0.7673300586799053, 'test_f1': 0.7375635222143276}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8145991928378042, 'dev_f1': 0.6746250354856637, 'test_f1': 0.669736653961093}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9994939932236262, 'dev_f1': 0.752823300579155, 'test_f1': 0.7368237947413845}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6572544708479292, 'dev_f1': 0.6568262915560182, 'test_f1': 0.6350152303629161}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7358270071664097, 'dev_f1': 0.7127936574153783, 'test_f1': 0.7059560479011898}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8787001894490202, 'dev_f1': 0.7639630129937726, 'test_f1': 0.735225692600063}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8147171511197819, 'dev_f1': 0.6807510661363502, 'test_f1': 0.6759925530153135}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9993869660074909, 'dev_f1': 0.7540953963245792, 'test_f1': 0.7340375972296377}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6549291019822064, 'dev_f1': 0.6521771486850519, 'test_f1': 0.6394601049524266}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7340306157171711, 'dev_f1': 0.7135093595170982, 'test_f1': 0.7022029002791862}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': True, 'remove_stopwords': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8847157804890223, 'dev_f1': 0.7697864625467018, 'test_f1': 0.7766891254126053}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8360608373465417, 'dev_f1': 0.7291123763515067, 'test_f1': 0.6760142149247207}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996010449586131, 'dev_f1': 0.799197105776826, 'test_f1': 0.7946618634372987}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6724618497904755, 'dev_f1': 0.6546750295165792, 'test_f1': 0.6624014565944539}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7752006233451305, 'dev_f1': 0.7552936663220605, 'test_f1': 0.7621784706743383}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8989685301943567, 'dev_f1': 0.7983820852105081, 'test_f1': 0.7995555028070807}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8540712191581679, 'dev_f1': 0.734340070621957, 'test_f1': 0.7160178040684758}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996010449586131, 'dev_f1': 0.7988645828785054, 'test_f1': 0.7850531960622328}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6383841024403738, 'dev_f1': 0.6285698891925123, 'test_f1': 0.6372382099550918}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7911067734588334, 'dev_f1': 0.7635371379590978, 'test_f1': 0.7797287610279141}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.897284561704215, 'dev_f1': 0.7843702745175687, 'test_f1': 0.800184062205504}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8502191113553951, 'dev_f1': 0.7385814555226541, 'test_f1': 0.7235754815508684}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996787711424727, 'dev_f1': 0.7941731296650899, 'test_f1': 0.7805144501643961}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6142978675500381, 'dev_f1': 0.6020777601028228, 'test_f1': 0.5941443176369462}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7898043325114618, 'dev_f1': 0.7587568580798778, 'test_f1': 0.783655059950481}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': False, 'remove_stopwords': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8724162000095862, 'dev_f1': 0.7237832346170493, 'test_f1': 0.7144465655280697}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8083906507517488, 'dev_f1': 0.6621856927781052, 'test_f1': 0.6338273280210545}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.999708121235348, 'dev_f1': 0.7520216441579199, 'test_f1': 0.733555918456077}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6706887874589738, 'dev_f1': 0.6496911838223336, 'test_f1': 0.6252109918799061}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7257931818570675, 'dev_f1': 0.6866282853798148, 'test_f1': 0.6861818551835661}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8785741347684223, 'dev_f1': 0.7620219581489623, 'test_f1': 0.7317322700904674}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8248983547401042, 'dev_f1': 0.6816413571828687, 'test_f1': 0.6797796270631773}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9997081386798525, 'dev_f1': 0.7502349919214759, 'test_f1': 0.7365235819619437}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6532125178999472, 'dev_f1': 0.6354383259514625, 'test_f1': 0.6362611429268984}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7365132654310867, 'dev_f1': 0.7038774155397866, 'test_f1': 0.702545330777873}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8789713194606953, 'dev_f1': 0.7628605664453781, 'test_f1': 0.7277600111326724}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8253142163872488, 'dev_f1': 0.6795837226120153, 'test_f1': 0.6782122215755501}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996010869288637, 'dev_f1': 0.7564168806796362, 'test_f1': 0.7367846566709758}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6465228991444699, 'dev_f1': 0.6383160817441986, 'test_f1': 0.6336246741123089}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7325594640003071, 'dev_f1': 0.685647712172022, 'test_f1': 0.7131397344071582}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': False, 'remove_stopwords': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8870838666070787, 'dev_f1': 0.7686925842119469, 'test_f1': 0.7752756612872135}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8391833321768001, 'dev_f1': 0.7111790616623059, 'test_f1': 0.6743869695342314}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9997081386798525, 'dev_f1': 0.8027246008782104, 'test_f1': 0.7930483129543006}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6621508212625997, 'dev_f1': 0.6552925631873001, 'test_f1': 0.6579377833024217}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7713595236341068, 'dev_f1': 0.7610179178630029, 'test_f1': 0.7597170359217166}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9003615257945864, 'dev_f1': 0.7939655172413792, 'test_f1': 0.7982956324068106}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8590725113030376, 'dev_f1': 0.7345246307497509, 'test_f1': 0.7095605900082584}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9997081386798525, 'dev_f1': 0.7949799948202315, 'test_f1': 0.7931956836271485}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6341361411135733, 'dev_f1': 0.6290096413124281, 'test_f1': 0.6256600772905713}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7842019317115186, 'dev_f1': 0.764495886447106, 'test_f1': 0.7812439697805295}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8970629670109863, 'dev_f1': 0.7803632107053686, 'test_f1': 0.7963427837758522}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8658763086076439, 'dev_f1': 0.7304931646016918, 'test_f1': 0.7210946370899077}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9997858719616982, 'dev_f1': 0.7940392853093519, 'test_f1': 0.7879888622500312}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5935620048587572, 'dev_f1': 0.5871930238730849, 'test_f1': 0.5750964683244428}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7878609599519727, 'dev_f1': 0.7622304712309494, 'test_f1': 0.7809423041421444}\n",
      "running tokenizer: <class 'src.tokenize.null_tokenizer.NullTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8981974159086303, 'dev_f1': 0.7838105095018211, 'test_f1': 0.7997449786622818}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9344097632638272, 'dev_f1': 0.6957052168670427, 'test_f1': 0.6886656463262142}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9997081386798525, 'dev_f1': 0.8007641637007724, 'test_f1': 0.806283166322157}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6786300625457654, 'dev_f1': 0.6382489218187543, 'test_f1': 0.6833623734185736}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7720680000794004, 'dev_f1': 0.7628136336833715, 'test_f1': 0.76888332433643}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9016453132417369, 'dev_f1': 0.7855470472810054, 'test_f1': 0.8117780742982245}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9463115969519379, 'dev_f1': 0.7460505501816015, 'test_f1': 0.7165477639510125}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9997081386798525, 'dev_f1': 0.7955164659183588, 'test_f1': 0.7995002870479618}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6223617160059184, 'dev_f1': 0.6245252256473051, 'test_f1': 0.6303924514040022}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7795991673804474, 'dev_f1': 0.7761047427389625, 'test_f1': 0.7883514616639115}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8986154645789926, 'dev_f1': 0.7841497546343361, 'test_f1': 0.8045410082072708}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9463196025145401, 'dev_f1': 0.7369831376346309, 'test_f1': 0.7110347789020018}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998929237141576, 'dev_f1': 0.7987469852391703, 'test_f1': 0.7955283704363484}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6047022149254182, 'dev_f1': 0.5825816967196278, 'test_f1': 0.5980288833017069}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7786235030708579, 'dev_f1': 0.7723640735683243, 'test_f1': 0.7858711875327512}\n",
      "running tokenizer: <class 'src.tokenize.sentence_piece_tokenizer.SentencePieceTokenizer'>: args={'vocab_size': 5000}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8708571359124608, 'dev_f1': 0.7661915907800863, 'test_f1': 0.7825607097803239}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.85857076642991, 'dev_f1': 0.7267854063875187, 'test_f1': 0.7273851605035427}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9997081386798525, 'dev_f1': 0.8095927620420137, 'test_f1': 0.7891509027315943}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5285880950604995, 'dev_f1': 0.47898797113819436, 'test_f1': 0.5110738983056937}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7320989502420988, 'dev_f1': 0.7010428248461064, 'test_f1': 0.7281551921074977}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8919084492285804, 'dev_f1': 0.7841518597649815, 'test_f1': 0.799597055148486}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8847346696600903, 'dev_f1': 0.7249199076488041, 'test_f1': 0.7271999823795122}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998929237141576, 'dev_f1': 0.7962155048692945, 'test_f1': 0.7903021916438341}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.40194158989016054, 'dev_f1': 0.3751784840527552, 'test_f1': 0.3768574184534165}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7720511347699975, 'dev_f1': 0.7591312596303194, 'test_f1': 0.764179146662487}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8957982518968204, 'dev_f1': 0.7747486158334757, 'test_f1': 0.7859447393428248}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.887011895922257, 'dev_f1': 0.7247060983799906, 'test_f1': 0.7414134496902968}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998929237141576, 'dev_f1': 0.8013885389788048, 'test_f1': 0.7913118113440761}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.3476639553591088, 'dev_f1': 0.3209105481272969, 'test_f1': 0.31920869269505275}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7728105759076461, 'dev_f1': 0.758445635576844, 'test_f1': 0.7626850706368383}\n",
      "running tokenizer: <class 'src.tokenize.sentence_piece_tokenizer.SentencePieceTokenizer'>: args={'vocab_size': 10000}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8825477798243005, 'dev_f1': 0.7656537393112653, 'test_f1': 0.7767545996841839}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8782673843077271, 'dev_f1': 0.7298901927016637, 'test_f1': 0.7026151669074875}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9997081386798525, 'dev_f1': 0.8085691538218146, 'test_f1': 0.8038961305575044}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.556716987781101, 'dev_f1': 0.51033608181351, 'test_f1': 0.5163559625579842}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7629942053702937, 'dev_f1': 0.7291659880771381, 'test_f1': 0.7514813875741867}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8976217702078658, 'dev_f1': 0.7883492358682291, 'test_f1': 0.8004341021210527}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9094993428918009, 'dev_f1': 0.7296002527474177, 'test_f1': 0.7238254091767029}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998929237141576, 'dev_f1': 0.8083248818946113, 'test_f1': 0.7968451375565849}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.4422401750112775, 'dev_f1': 0.42135397987584794, 'test_f1': 0.39872009273228537}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7813254194201299, 'dev_f1': 0.7555996467321324, 'test_f1': 0.7631805822835666}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.899329930403172, 'dev_f1': 0.7763337271032329, 'test_f1': 0.7937649045701717}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9088731908008336, 'dev_f1': 0.7295687357361366, 'test_f1': 0.7099039423259127}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9998929237141576, 'dev_f1': 0.7994478365111628, 'test_f1': 0.7891172991618185}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.4147123404329936, 'dev_f1': 0.39492108387457225, 'test_f1': 0.3864091513280136}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': True, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7797482822579752, 'dev_f1': 0.76742021591807, 'test_f1': 0.7693856693808868}\n",
      "running preprocessing: <class 'src.preprocessing.simple_preprocessor.SimplePreprocessor'>: args={'remove_citations': False, 'remove_duplicates': True}, current_state={'pretokenizer': 'SimplePreprocessor'}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': True, 'remove_stopwords': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8783128150830346, 'dev_f1': 0.7321675704100716, 'test_f1': 0.7247687248352452}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8185523278030334, 'dev_f1': 0.6601208076854063, 'test_f1': 0.6148605098442456}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7452437268292512, 'test_f1': 0.7388794477334556}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6750909577025842, 'dev_f1': 0.6456306879757695, 'test_f1': 0.6386712505963534}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7264606100671634, 'dev_f1': 0.7094252023543871, 'test_f1': 0.6725429240643089}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8874873197506665, 'dev_f1': 0.7700660929152855, 'test_f1': 0.7412110430740882}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.83321294735139, 'dev_f1': 0.6853488120041642, 'test_f1': 0.6644657267170074}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7609929333084645, 'test_f1': 0.7430564353517742}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6705286033429955, 'dev_f1': 0.6360603322288401, 'test_f1': 0.6417012763838486}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.737262009922906, 'dev_f1': 0.7060460642130334, 'test_f1': 0.6988846195247405}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8855083405296473, 'dev_f1': 0.7686429503763642, 'test_f1': 0.7389067881673768}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8278332231123396, 'dev_f1': 0.6832867893882774, 'test_f1': 0.6680631914863094}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7628547980946522, 'test_f1': 0.7407861343023289}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6495695630875521, 'dev_f1': 0.6416522892537296, 'test_f1': 0.6285578779677324}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7364013917646393, 'dev_f1': 0.7102439957678534, 'test_f1': 0.708415575901042}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': True, 'remove_stopwords': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.891268755120053, 'dev_f1': 0.7792885251279348, 'test_f1': 0.7715287668875453}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8439678072654537, 'dev_f1': 0.7197678192673777, 'test_f1': 0.6690776737593631}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7998782849239281, 'test_f1': 0.7973190375665098}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6469665902343035, 'dev_f1': 0.6260844180019509, 'test_f1': 0.64974961798744}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7745031891952491, 'dev_f1': 0.7598964750226226, 'test_f1': 0.7605605490017385}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9015512492398945, 'dev_f1': 0.8032170061981129, 'test_f1': 0.7973351969753409}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8632579068912728, 'dev_f1': 0.7272485140742403, 'test_f1': 0.7079798874928956}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8045521845297848, 'test_f1': 0.7877893226474851}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6340791887066833, 'dev_f1': 0.6203527557283475, 'test_f1': 0.6405055690532887}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7908028659594056, 'dev_f1': 0.7793942343610306, 'test_f1': 0.7826300294121039}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9024552034707808, 'dev_f1': 0.7960599917470836, 'test_f1': 0.8008034339528055}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8657823240315984, 'dev_f1': 0.7237316463895985, 'test_f1': 0.7117349394732848}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7999152567658553, 'test_f1': 0.7826358994770858}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5707398913613575, 'dev_f1': 0.5637379772878739, 'test_f1': 0.5576748831306727}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7906430573550248, 'dev_f1': 0.7630997476232245, 'test_f1': 0.7796325541813225}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': False, 'remove_stopwords': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8815143511603442, 'dev_f1': 0.7179372615747389, 'test_f1': 0.7243922483607547}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.821437309460418, 'dev_f1': 0.6605454724066454, 'test_f1': 0.6242951044728247}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7442387025170666, 'test_f1': 0.7353883424697525}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6544156155685897, 'dev_f1': 0.6258335329802311, 'test_f1': 0.636039381757972}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7134943735460313, 'dev_f1': 0.6943812173731074, 'test_f1': 0.6679754290562275}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8885318602947659, 'dev_f1': 0.7712516376121877, 'test_f1': 0.7404433673030666}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8425083078254074, 'dev_f1': 0.6752463054187192, 'test_f1': 0.6655744494966749}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7507296503732462, 'test_f1': 0.7407989847547665}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6256576170267484, 'dev_f1': 0.5974266031073711, 'test_f1': 0.6163357323909393}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.728115422957668, 'dev_f1': 0.6984796573590141, 'test_f1': 0.69228438755987}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8896324670008453, 'dev_f1': 0.7696484627421412, 'test_f1': 0.7348479236147704}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8344301262180993, 'dev_f1': 0.6768848461317081, 'test_f1': 0.6594163587609355}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7503695162975527, 'test_f1': 0.7348480956708606}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6146702615737059, 'dev_f1': 0.614471577974855, 'test_f1': 0.6139037900488415}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7317884930314449, 'dev_f1': 0.7010252451439497, 'test_f1': 0.6950396119097086}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': False, 'remove_stopwords': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.892527032353374, 'dev_f1': 0.7716635152635577, 'test_f1': 0.7697040602687958}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8487811782005478, 'dev_f1': 0.7069812256421019, 'test_f1': 0.6695411730754576}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7945611311698478, 'test_f1': 0.7989283288096712}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.655231598413725, 'dev_f1': 0.6205396211504614, 'test_f1': 0.6601751980789489}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7734636322411731, 'dev_f1': 0.7667804786005573, 'test_f1': 0.7536251575140929}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9036324178107705, 'dev_f1': 0.7955817638734403, 'test_f1': 0.8009556973802162}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8731829266992522, 'dev_f1': 0.7273340224811182, 'test_f1': 0.6994654777298145}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8013852108893857, 'test_f1': 0.7812848498143655}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.584663730173521, 'dev_f1': 0.5755416423734453, 'test_f1': 0.5923281305223695}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7826956435439424, 'dev_f1': 0.7700181682180179, 'test_f1': 0.76654150446684}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9033958334605175, 'dev_f1': 0.7918949290630707, 'test_f1': 0.7968047438684275}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8703724863940444, 'dev_f1': 0.7204429241923664, 'test_f1': 0.7040765922990267}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8021786704986756, 'test_f1': 0.779332547136052}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5431107663111548, 'dev_f1': 0.5596876068945775, 'test_f1': 0.5345928332044251}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7883516052120793, 'dev_f1': 0.7706093669142441, 'test_f1': 0.7688923003160634}\n",
      "running tokenizer: <class 'src.tokenize.null_tokenizer.NullTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9016777552592677, 'dev_f1': 0.7774767510616568, 'test_f1': 0.7917243974735061}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9448705676218637, 'dev_f1': 0.7125736339630305, 'test_f1': 0.6878482461556882}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8049515469719176, 'test_f1': 0.8053002070393376}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6658320241324222, 'dev_f1': 0.6235771646347391, 'test_f1': 0.6601094245946437}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7756186909106741, 'dev_f1': 0.758318405806418, 'test_f1': 0.7663040875048809}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9054465139753937, 'dev_f1': 0.7826060374764799, 'test_f1': 0.8063808017842318}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9575620916575524, 'dev_f1': 0.7397141463408493, 'test_f1': 0.7178198278683502}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8010570612320741, 'test_f1': 0.798576935914434}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.614176757841297, 'dev_f1': 0.6106243450849158, 'test_f1': 0.624650045493185}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7866366099475167, 'dev_f1': 0.7758793366688196, 'test_f1': 0.7884042864460546}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9025089806742096, 'dev_f1': 0.7815129986996995, 'test_f1': 0.8053230457485777}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9589007711941179, 'dev_f1': 0.7311151079136691, 'test_f1': 0.7142628566074566}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8000372595129234, 'test_f1': 0.7969591802684409}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5815943558752295, 'dev_f1': 0.5511762034767193, 'test_f1': 0.5771803265291762}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7829642764299823, 'dev_f1': 0.777374371760211, 'test_f1': 0.788354629897776}\n",
      "running tokenizer: <class 'src.tokenize.sentence_piece_tokenizer.SentencePieceTokenizer'>: args={'vocab_size': 5000}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8760410599698849, 'dev_f1': 0.7649326357503493, 'test_f1': 0.7812971987970453}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8573402272164712, 'dev_f1': 0.7276268049784793, 'test_f1': 0.7094430133818328}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8009515388286962, 'test_f1': 0.7897417684401363}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.4824112092171465, 'dev_f1': 0.4617037601707637, 'test_f1': 0.4852762457587916}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7232588998745811, 'dev_f1': 0.7169015216683161, 'test_f1': 0.7190051513524538}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8949813454405823, 'dev_f1': 0.7948949905744094, 'test_f1': 0.7863323919213142}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.893615037604563, 'dev_f1': 0.7164626735423303, 'test_f1': 0.7198235002867864}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8054881469606139, 'test_f1': 0.7897458334013107}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.35870378964961924, 'dev_f1': 0.3553323918851279, 'test_f1': 0.33448464370632364}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7734324233145436, 'dev_f1': 0.7460969288140852, 'test_f1': 0.7747476841147604}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8991775430603944, 'dev_f1': 0.784614494268228, 'test_f1': 0.7863104675349942}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8889522977701457, 'dev_f1': 0.711768382427065, 'test_f1': 0.7056552422781518}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7822829447656415, 'test_f1': 0.7851850071920556}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.2954745260595741, 'dev_f1': 0.2812395344581741, 'test_f1': 0.2706555321036371}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7708995615711441, 'dev_f1': 0.7387966573605803, 'test_f1': 0.7686926239753995}\n",
      "running tokenizer: <class 'src.tokenize.sentence_piece_tokenizer.SentencePieceTokenizer'>: args={'vocab_size': 10000}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8824798822915886, 'dev_f1': 0.7733482873044984, 'test_f1': 0.7751685645147696}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8853027642969854, 'dev_f1': 0.7308993097176345, 'test_f1': 0.6988749678972432}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7980029730217825, 'test_f1': 0.7965789678837919}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5071921516475496, 'dev_f1': 0.4750314284375509, 'test_f1': 0.4840165239398284}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7601474676581779, 'dev_f1': 0.7380777672104489, 'test_f1': 0.757942362254488}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8986505128038612, 'dev_f1': 0.7938076765449895, 'test_f1': 0.8023100767496979}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9079279420631927, 'dev_f1': 0.7423542975735419, 'test_f1': 0.725048665494851}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.8051957066012245, 'test_f1': 0.7927469517831347}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.3948809922801108, 'dev_f1': 0.39223140724166267, 'test_f1': 0.3751271082098479}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7776242958412373, 'dev_f1': 0.7576808442162034, 'test_f1': 0.7711409422598425}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9038613349642892, 'dev_f1': 0.779874683298431, 'test_f1': 0.785968308422997}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9167058319322062, 'dev_f1': 0.7216085453075863, 'test_f1': 0.7112989056471927}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 1.0, 'dev_f1': 0.7902738981657663, 'test_f1': 0.792737768846781}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.37745850934376796, 'dev_f1': 0.35815749487702536, 'test_f1': 0.34455237023792784}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': True, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7820053424865695, 'dev_f1': 0.7663850043674597, 'test_f1': 0.7707421693686675}\n",
      "running preprocessing: <class 'src.preprocessing.simple_preprocessor.SimplePreprocessor'>: args={'remove_citations': False, 'remove_duplicates': False}, current_state={'pretokenizer': 'SimplePreprocessor'}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': True, 'remove_stopwords': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8765623357632544, 'dev_f1': 0.7232003101855232, 'test_f1': 0.71891766922692}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8151841916681496, 'dev_f1': 0.6566443589293779, 'test_f1': 0.6160332423269536}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9993573212095802, 'dev_f1': 0.750694522868436, 'test_f1': 0.7366837059508754}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6657823395119536, 'dev_f1': 0.6495480903395082, 'test_f1': 0.6410794051370776}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7254238081301404, 'dev_f1': 0.708955798540332, 'test_f1': 0.6815258714811253}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8862862994422839, 'dev_f1': 0.7675030673172212, 'test_f1': 0.7390001170389976}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8292866214121895, 'dev_f1': 0.6783739213316679, 'test_f1': 0.6618199633949066}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9994647411995564, 'dev_f1': 0.7591882292370604, 'test_f1': 0.7408246618769208}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6502776021175607, 'dev_f1': 0.6288071636177975, 'test_f1': 0.6378375881027305}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7365617033350221, 'dev_f1': 0.714441129969194, 'test_f1': 0.7067145447766471}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8843702518281616, 'dev_f1': 0.7754171821789155, 'test_f1': 0.7383119899560578}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8256233276346195, 'dev_f1': 0.6726810909263552, 'test_f1': 0.6758652901204254}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9994644957893448, 'dev_f1': 0.7601681502182202, 'test_f1': 0.7388791919040906}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6482275980850352, 'dev_f1': 0.6234178843463569, 'test_f1': 0.6391617135501592}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7331286620096144, 'dev_f1': 0.7175455703549889, 'test_f1': 0.6995278543626865}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': True, 'remove_stopwords': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8902133568365641, 'dev_f1': 0.776520529897919, 'test_f1': 0.7710421172522843}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8398828432453196, 'dev_f1': 0.7220221436059712, 'test_f1': 0.6762917608185385}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9994646185707877, 'dev_f1': 0.7979950989573578, 'test_f1': 0.7945333908124607}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6571950646190162, 'dev_f1': 0.6360750449126246, 'test_f1': 0.6693672789116842}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.776004919714059, 'dev_f1': 0.7590681780939583, 'test_f1': 0.7667610071726877}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9028032946048109, 'dev_f1': 0.7988425499079096, 'test_f1': 0.7976652608271829}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8673505467586446, 'dev_f1': 0.7312229260353481, 'test_f1': 0.7127578463647412}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9994644957893448, 'dev_f1': 0.8005161706814309, 'test_f1': 0.7875832163246299}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6195575664623681, 'dev_f1': 0.5975432303718271, 'test_f1': 0.6182851604935328}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7927802705896013, 'dev_f1': 0.770294904342007, 'test_f1': 0.7826239528899194}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9025476770402866, 'dev_f1': 0.7955964645330805, 'test_f1': 0.8008347220619193}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.865166279871163, 'dev_f1': 0.7178584174037518, 'test_f1': 0.7092842339541292}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9994647411995564, 'dev_f1': 0.7976915069004319, 'test_f1': 0.7811799688850597}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.591853194464265, 'dev_f1': 0.5787927571612966, 'test_f1': 0.5817022747273749}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': True, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7965313719165928, 'dev_f1': 0.766726078508944, 'test_f1': 0.7777673822453193}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': False, 'remove_stopwords': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8789147401225308, 'dev_f1': 0.7166490238446218, 'test_f1': 0.7216003377005277}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8170407479175665, 'dev_f1': 0.6581717211483703, 'test_f1': 0.6241836883967983}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.999464372855056, 'dev_f1': 0.7457531191754799, 'test_f1': 0.7337021813243757}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.660128070730623, 'dev_f1': 0.6320207899986731, 'test_f1': 0.6231900664184072}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7158380848071942, 'dev_f1': 0.698341392614588, 'test_f1': 0.6636829485166212}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8868996192717201, 'dev_f1': 0.7648565923929821, 'test_f1': 0.7413093679830581}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8355274826524428, 'dev_f1': 0.6803084403347007, 'test_f1': 0.6660761445853335}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9995716457593291, 'dev_f1': 0.7504719097234703, 'test_f1': 0.7366066219830736}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6264325199253326, 'dev_f1': 0.60941073076887, 'test_f1': 0.6255440639228925}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7332287990565941, 'dev_f1': 0.7052550907056054, 'test_f1': 0.6947419590922111}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8878095964461394, 'dev_f1': 0.771930422155909, 'test_f1': 0.7338713167323968}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8332042776653736, 'dev_f1': 0.6690538307497244, 'test_f1': 0.6595501976160536}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9994646185707877, 'dev_f1': 0.747800663928584, 'test_f1': 0.7387790054146056}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6096985412090797, 'dev_f1': 0.595761100917569, 'test_f1': 0.604572219091292}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': True, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7300935903885776, 'dev_f1': 0.7124731964264085, 'test_f1': 0.6941274785655747}\n",
      "running tokenizer: <class 'src.tokenize.spacy_tokenizer.SpacyTokenizer'>: args={'replace_numbers': False, 'remove_stopwords': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8922061882710087, 'dev_f1': 0.7684434069736951, 'test_f1': 0.7626144522404917}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8476836306709806, 'dev_f1': 0.7029196460846036, 'test_f1': 0.6775976818267365}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9994646185707877, 'dev_f1': 0.7974007078122921, 'test_f1': 0.7948942448405854}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6603441959154054, 'dev_f1': 0.6486517192334859, 'test_f1': 0.6532976266826063}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7693750578772717, 'dev_f1': 0.7609670740666533, 'test_f1': 0.7575987784195419}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9036853718204819, 'dev_f1': 0.7998404720961055, 'test_f1': 0.7979716161173926}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8709658918502914, 'dev_f1': 0.732040681143856, 'test_f1': 0.7116404964768347}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9995717439233965, 'dev_f1': 0.8007523282324914, 'test_f1': 0.7816836660790779}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5851182084556713, 'dev_f1': 0.5891886428188774, 'test_f1': 0.6070999343921188}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7896229846343497, 'dev_f1': 0.7665613729131854, 'test_f1': 0.7702452905699039}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9018043116591778, 'dev_f1': 0.7890604888982682, 'test_f1': 0.7942675374414385}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8694566775837912, 'dev_f1': 0.7156498880388288, 'test_f1': 0.7064892058378706}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9995717439233965, 'dev_f1': 0.8007692119432207, 'test_f1': 0.7794870081914892}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5465434188582786, 'dev_f1': 0.5199470183055687, 'test_f1': 0.5455001971687142}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SpacyTokenizer', 'replace_numbers': False, 'remove_stopwords': False, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.787743556348108, 'dev_f1': 0.7584378770091397, 'test_f1': 0.7713200387773571}\n",
      "running tokenizer: <class 'src.tokenize.null_tokenizer.NullTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8997260211817268, 'dev_f1': 0.7761155681295705, 'test_f1': 0.7915982903366493}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.943286289695182, 'dev_f1': 0.7073329136967398, 'test_f1': 0.692993697128237}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9995716457593291, 'dev_f1': 0.8034484875828545, 'test_f1': 0.8025116591000092}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6660925422376435, 'dev_f1': 0.6353151415396606, 'test_f1': 0.6655964872149932}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7719927026618153, 'dev_f1': 0.763329652803337, 'test_f1': 0.7690075190095481}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9049529229611664, 'dev_f1': 0.7882567327850346, 'test_f1': 0.8086870693837241}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9566630984235598, 'dev_f1': 0.7590353157105333, 'test_f1': 0.7073223634041351}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9995717439233965, 'dev_f1': 0.7975345513368306, 'test_f1': 0.7962283582050474}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.6024719707197045, 'dev_f1': 0.5776474154155861, 'test_f1': 0.5978636403724874}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7835144006835354, 'dev_f1': 0.7680121048542102, 'test_f1': 0.7827450795980292}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9018348472290117, 'dev_f1': 0.7863489839304237, 'test_f1': 0.8063534581154856}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9541019584481981, 'dev_f1': 0.7392086505646498, 'test_f1': 0.7090344799443224}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996787711424727, 'dev_f1': 0.8028450988970036, 'test_f1': 0.794500091625895}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5663670569195925, 'dev_f1': 0.5160161111571128, 'test_f1': 0.559499117082723}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'NullTokenizer', 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7802314900282464, 'dev_f1': 0.7716933786627814, 'test_f1': 0.7861823319948028}\n",
      "running tokenizer: <class 'src.tokenize.sentence_piece_tokenizer.SentencePieceTokenizer'>: args={'vocab_size': 5000}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8732366671520394, 'dev_f1': 0.7783845453455661, 'test_f1': 0.7784780337664401}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8556550478251883, 'dev_f1': 0.7224787681679822, 'test_f1': 0.7196707868115523}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9995717439233965, 'dev_f1': 0.8083743018020065, 'test_f1': 0.7945012605208798}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.48710254945103953, 'dev_f1': 0.4570868226391814, 'test_f1': 0.4729398939803282}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7240917725030053, 'dev_f1': 0.7060464730041343, 'test_f1': 0.7108485827486719}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8966738370758218, 'dev_f1': 0.7991907701468711, 'test_f1': 0.7891860455871996}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8906124522614665, 'dev_f1': 0.7323284085991628, 'test_f1': 0.7136068016729568}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996787711424727, 'dev_f1': 0.7852564145790485, 'test_f1': 0.7886878420779982}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.3531371434529091, 'dev_f1': 0.35762180817359085, 'test_f1': 0.34084918096101696}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7692491501241933, 'dev_f1': 0.7374422742334548, 'test_f1': 0.7600069716453905}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8978272458822486, 'dev_f1': 0.7855577221138943, 'test_f1': 0.7800219597827924}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8857906519837214, 'dev_f1': 0.7233663460076488, 'test_f1': 0.7200453411526077}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996787711424727, 'dev_f1': 0.7865612173755974, 'test_f1': 0.7989330660885453}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.29933009744602884, 'dev_f1': 0.3060925633364005, 'test_f1': 0.2749045908271485}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 5000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7691220702332758, 'dev_f1': 0.7434626157818854, 'test_f1': 0.756572035639186}\n",
      "running tokenizer: <class 'src.tokenize.sentence_piece_tokenizer.SentencePieceTokenizer'>: args={'vocab_size': 10000}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer'}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.null_post_tokenizer.NullPostTokenizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.8841604271211573, 'dev_f1': 0.7636252972581578, 'test_f1': 0.7812915984765602}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.8790674389951394, 'dev_f1': 0.7354338486648496, 'test_f1': 0.7043876323050148}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9995717439233965, 'dev_f1': 0.8023078953038039, 'test_f1': 0.7991772757884185}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.5179416566192948, 'dev_f1': 0.4685713379483154, 'test_f1': 0.48762645862739756}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'NullPostTokenizer', 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7601589119912339, 'dev_f1': 0.7383857759414708, 'test_f1': 0.7531147244474852}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 1}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9011498437753103, 'dev_f1': 0.7932701742062863, 'test_f1': 0.7965376736818387}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9055523893457154, 'dev_f1': 0.7315535142965276, 'test_f1': 0.7261163922333532}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996787711424727, 'dev_f1': 0.8059744513850794, 'test_f1': 0.7898370928881322}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.39284663147895255, 'dev_f1': 0.3903612064265434, 'test_f1': 0.36454640344905825}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 1, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7835176719885798, 'dev_f1': 0.764330754568909, 'test_f1': 0.7615645356916231}\n",
      "running post_tokenizer: <class 'src.post_tokenizer.phraser_merger.PhraserMerger'>: args={'num_gram': 2}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger'}\n",
      "running vectorizer: <class 'src.vectorizer.tfidf_vectorizer.TfidfVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'TfidfVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9016449709653381, 'dev_f1': 0.7811283594079841, 'test_f1': 0.79228478904286}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': True}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': True, 'model': 'AbstractModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee_m\\.conda\\envs\\cs4248_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result={'model_name': 'LR', 'train_f1': 0.9029985217310532, 'dev_f1': 0.7179738907539507, 'test_f1': 0.7179934642884639}\n",
      "running vectorizer: <class 'src.vectorizer.sk_count_vectorizer.SkCountVectorizer'>: args={'ignore_preprocessing': False}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'SkCountVectorizer', 'ignore_preprocessing': False, 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.9996787711424727, 'dev_f1': 0.7885875102349228, 'test_f1': 0.7930141163110694}\n",
      "running vectorizer: <class 'src.vectorizer.w2v_vectorizer.W2vVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'W2vVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.37039277710418644, 'dev_f1': 0.3663877088563557, 'test_f1': 0.3378721714966256}\n",
      "running vectorizer: <class 'src.vectorizer.lsi_vectorizer.LsiVectorizer'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer'}\n",
      "running model: <class 'src.models.abstract_model.AbstractModel'>: args={}, current_state={'pretokenizer': 'SimplePreprocessor', 'remove_citations': False, 'remove_duplicates': False, 'tokenizer': 'SentencePieceTokenizer', 'vocab_size': 10000, 'post_tokenizer': 'PhraserMerger', 'num_gram': 2, 'vectorizer': 'LsiVectorizer', 'model': 'AbstractModel'}\n",
      "model_result={'model_name': 'LR', 'train_f1': 0.7806051846844161, 'dev_f1': 0.758723254343047, 'test_f1': 0.7616358317465628}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "           pretokenizer  remove_citations  remove_duplicates  \\\n",
       "0    SimplePreprocessor              True               True   \n",
       "1    SimplePreprocessor              True               True   \n",
       "2    SimplePreprocessor              True               True   \n",
       "3    SimplePreprocessor              True               True   \n",
       "4    SimplePreprocessor              True               True   \n",
       "..                  ...               ...                ...   \n",
       "415  SimplePreprocessor             False              False   \n",
       "416  SimplePreprocessor             False              False   \n",
       "417  SimplePreprocessor             False              False   \n",
       "418  SimplePreprocessor             False              False   \n",
       "419  SimplePreprocessor             False              False   \n",
       "\n",
       "                  tokenizer replace_numbers remove_stopwords  \\\n",
       "0            SpacyTokenizer            True             True   \n",
       "1            SpacyTokenizer            True             True   \n",
       "2            SpacyTokenizer            True             True   \n",
       "3            SpacyTokenizer            True             True   \n",
       "4            SpacyTokenizer            True             True   \n",
       "..                      ...             ...              ...   \n",
       "415  SentencePieceTokenizer             NaN              NaN   \n",
       "416  SentencePieceTokenizer             NaN              NaN   \n",
       "417  SentencePieceTokenizer             NaN              NaN   \n",
       "418  SentencePieceTokenizer             NaN              NaN   \n",
       "419  SentencePieceTokenizer             NaN              NaN   \n",
       "\n",
       "        post_tokenizer         vectorizer          model model_name  train_f1  \\\n",
       "0    NullPostTokenizer    TfidfVectorizer  AbstractModel         LR  0.870372   \n",
       "1    NullPostTokenizer  SkCountVectorizer  AbstractModel         LR  0.806738   \n",
       "2    NullPostTokenizer  SkCountVectorizer  AbstractModel         LR  0.999801   \n",
       "3    NullPostTokenizer      W2vVectorizer  AbstractModel         LR  0.663951   \n",
       "4    NullPostTokenizer      LsiVectorizer  AbstractModel         LR  0.728142   \n",
       "..                 ...                ...            ...        ...       ...   \n",
       "415      PhraserMerger    TfidfVectorizer  AbstractModel         LR  0.901645   \n",
       "416      PhraserMerger  SkCountVectorizer  AbstractModel         LR  0.902999   \n",
       "417      PhraserMerger  SkCountVectorizer  AbstractModel         LR  0.999679   \n",
       "418      PhraserMerger      W2vVectorizer  AbstractModel         LR  0.370393   \n",
       "419      PhraserMerger      LsiVectorizer  AbstractModel         LR  0.780605   \n",
       "\n",
       "       dev_f1   test_f1 ignore_preprocessing  num_gram  vocab_size  \n",
       "0    0.736147  0.723453                  NaN       NaN         NaN  \n",
       "1    0.674092  0.628827                 True       NaN         NaN  \n",
       "2    0.753170  0.737456                False       NaN         NaN  \n",
       "3    0.631870  0.635406                  NaN       NaN         NaN  \n",
       "4    0.710348  0.690148                  NaN       NaN         NaN  \n",
       "..        ...       ...                  ...       ...         ...  \n",
       "415  0.781128  0.792285                  NaN       2.0     10000.0  \n",
       "416  0.717974  0.717993                 True       2.0     10000.0  \n",
       "417  0.788588  0.793014                False       2.0     10000.0  \n",
       "418  0.366388  0.337872                  NaN       2.0     10000.0  \n",
       "419  0.758723  0.761636                  NaN       2.0     10000.0  \n",
       "\n",
       "[420 rows x 16 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pretokenizer</th>\n",
       "      <th>remove_citations</th>\n",
       "      <th>remove_duplicates</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>replace_numbers</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>post_tokenizer</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>dev_f1</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>ignore_preprocessing</th>\n",
       "      <th>num_gram</th>\n",
       "      <th>vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.870372</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>0.723453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>SkCountVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.806738</td>\n",
       "      <td>0.674092</td>\n",
       "      <td>0.628827</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>SkCountVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.753170</td>\n",
       "      <td>0.737456</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.663951</td>\n",
       "      <td>0.631870</td>\n",
       "      <td>0.635406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SpacyTokenizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>LsiVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.728142</td>\n",
       "      <td>0.710348</td>\n",
       "      <td>0.690148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.901645</td>\n",
       "      <td>0.781128</td>\n",
       "      <td>0.792285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>SkCountVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.902999</td>\n",
       "      <td>0.717974</td>\n",
       "      <td>0.717993</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>SkCountVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>0.788588</td>\n",
       "      <td>0.793014</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.370393</td>\n",
       "      <td>0.366388</td>\n",
       "      <td>0.337872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>LsiVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.780605</td>\n",
       "      <td>0.758723</td>\n",
       "      <td>0.761636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 16 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T05:31:05.858260Z",
     "start_time": "2024-04-08T05:31:05.719259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.utils.path_getter import PathGetter\n",
    "\n",
    "results_df.to_parquet(PathGetter.get_data_directory()/'experiments.parquet')"
   ],
   "id": "96564c4815c35f9a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T05:45:08.711034Z",
     "start_time": "2024-04-08T05:45:08.687033Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.sort_values(by=['test_f1'], ascending=False)",
   "id": "b0a7c0cf57aae4c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           pretokenizer  remove_citations  remove_duplicates  \\\n",
       "170  SimplePreprocessor              True              False   \n",
       "65   SimplePreprocessor              True               True   \n",
       "380  SimplePreprocessor             False              False   \n",
       "62   SimplePreprocessor              True               True   \n",
       "70   SimplePreprocessor              True               True   \n",
       "..                  ...               ...                ...   \n",
       "418  SimplePreprocessor             False              False   \n",
       "293  SimplePreprocessor             False               True   \n",
       "193  SimplePreprocessor              True              False   \n",
       "403  SimplePreprocessor             False              False   \n",
       "298  SimplePreprocessor             False               True   \n",
       "\n",
       "                  tokenizer replace_numbers remove_stopwords  \\\n",
       "170           NullTokenizer             NaN              NaN   \n",
       "65            NullTokenizer             NaN              NaN   \n",
       "380           NullTokenizer             NaN              NaN   \n",
       "62            NullTokenizer             NaN              NaN   \n",
       "70            NullTokenizer             NaN              NaN   \n",
       "..                      ...             ...              ...   \n",
       "418  SentencePieceTokenizer             NaN              NaN   \n",
       "293  SentencePieceTokenizer             NaN              NaN   \n",
       "193  SentencePieceTokenizer             NaN              NaN   \n",
       "403  SentencePieceTokenizer             NaN              NaN   \n",
       "298  SentencePieceTokenizer             NaN              NaN   \n",
       "\n",
       "        post_tokenizer         vectorizer          model model_name  train_f1  \\\n",
       "170      PhraserMerger    TfidfVectorizer  AbstractModel         LR  0.901645   \n",
       "65       PhraserMerger    TfidfVectorizer  AbstractModel         LR  0.900072   \n",
       "380      PhraserMerger    TfidfVectorizer  AbstractModel         LR  0.904953   \n",
       "62   NullPostTokenizer  SkCountVectorizer  AbstractModel         LR  0.999801   \n",
       "70       PhraserMerger    TfidfVectorizer  AbstractModel         LR  0.896358   \n",
       "..                 ...                ...            ...        ...       ...   \n",
       "418      PhraserMerger      W2vVectorizer  AbstractModel         LR  0.370393   \n",
       "293      PhraserMerger      W2vVectorizer  AbstractModel         LR  0.358704   \n",
       "193      PhraserMerger      W2vVectorizer  AbstractModel         LR  0.347664   \n",
       "403      PhraserMerger      W2vVectorizer  AbstractModel         LR  0.299330   \n",
       "298      PhraserMerger      W2vVectorizer  AbstractModel         LR  0.295475   \n",
       "\n",
       "       dev_f1   test_f1 ignore_preprocessing  num_gram  vocab_size  \n",
       "170  0.785547  0.811778                  NaN       1.0         NaN  \n",
       "65   0.782403  0.809262                  NaN       1.0         NaN  \n",
       "380  0.788257  0.808687                  NaN       1.0         NaN  \n",
       "62   0.800148  0.808455                False       NaN         NaN  \n",
       "70   0.779570  0.806851                  NaN       2.0         NaN  \n",
       "..        ...       ...                  ...       ...         ...  \n",
       "418  0.366388  0.337872                  NaN       2.0     10000.0  \n",
       "293  0.355332  0.334485                  NaN       1.0      5000.0  \n",
       "193  0.320911  0.319209                  NaN       2.0      5000.0  \n",
       "403  0.306093  0.274905                  NaN       2.0      5000.0  \n",
       "298  0.281240  0.270656                  NaN       2.0      5000.0  \n",
       "\n",
       "[420 rows x 16 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pretokenizer</th>\n",
       "      <th>remove_citations</th>\n",
       "      <th>remove_duplicates</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>replace_numbers</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>post_tokenizer</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>dev_f1</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>ignore_preprocessing</th>\n",
       "      <th>num_gram</th>\n",
       "      <th>vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NullTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.901645</td>\n",
       "      <td>0.785547</td>\n",
       "      <td>0.811778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NullTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.900072</td>\n",
       "      <td>0.782403</td>\n",
       "      <td>0.809262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NullTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.904953</td>\n",
       "      <td>0.788257</td>\n",
       "      <td>0.808687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NullTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NullPostTokenizer</td>\n",
       "      <td>SkCountVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.800148</td>\n",
       "      <td>0.808455</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NullTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.896358</td>\n",
       "      <td>0.779570</td>\n",
       "      <td>0.806851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.370393</td>\n",
       "      <td>0.366388</td>\n",
       "      <td>0.337872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.358704</td>\n",
       "      <td>0.355332</td>\n",
       "      <td>0.334485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.347664</td>\n",
       "      <td>0.320911</td>\n",
       "      <td>0.319209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.299330</td>\n",
       "      <td>0.306093</td>\n",
       "      <td>0.274905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>SimplePreprocessor</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>SentencePieceTokenizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PhraserMerger</td>\n",
       "      <td>W2vVectorizer</td>\n",
       "      <td>AbstractModel</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.295475</td>\n",
       "      <td>0.281240</td>\n",
       "      <td>0.270656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 16 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (cs4248 project)",
   "language": "python",
   "name": "cs4248_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

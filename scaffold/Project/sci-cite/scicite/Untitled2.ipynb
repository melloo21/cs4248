{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d392d6-e4d1-43fc-850d-d4a7044963aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/allennlp/service/predictors/__init__.py:23: FutureWarning: allennlp.service.predictors.* has been depreciated. Please use allennlp.predictors.*\n",
      "  \"Please use allennlp.predictors.*\", FutureWarning)\n",
      "/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/allennlp/service/predictors/predictor.py:6: FutureWarning: allennlp.service.predictors.* has been deprecated. Please use allennlp.predictors.*\n",
      "  \" Please use allennlp.predictors.*\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from distutils.version import StrictVersion\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from allennlp.common import Params\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import FeedForward, Seq2VecEncoder, Seq2SeqEncoder, TextFieldEmbedder, Embedding, TimeDistributed\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from overrides import overrides\n",
    "from torch.nn import Parameter, Linear\n",
    "\n",
    "from scicite.constants import  Scicite_Format_Nested_Jsonlines\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc52f832-f83d-4c38-b566-97d826dd6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @Model.register(\"scaffold_bilstm_attention_classifier\")\n",
    "class ScaffoldBilstmAttentionClassifier1(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` performs text classification for citation intents.  We assume we're given a\n",
    "    citation text, and we predict some output label.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 citation_text_encoder: Seq2SeqEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 classifier_feedforward_2: FeedForward,\n",
    "                 classifier_feedforward_3: FeedForward,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None,\n",
    "                 report_auxiliary_metrics: bool = False,\n",
    "                 predict_mode: bool = False,\n",
    "                 ) -> None:\n",
    "        \"\"\"\n",
    "        Additional Args:\n",
    "            lexicon_embedder_params: parameters for the lexicon attention model\n",
    "            use_sparse_lexicon_features: whether to use sparse (onehot) lexicon features\n",
    "            multilabel: whether the classification is multi-label\n",
    "            data_format: s2 or jurgens\n",
    "            report_auxiliary_metrics: report metrics for aux tasks\n",
    "            predict_mode: predict unlabeled examples\n",
    "        \"\"\"\n",
    "        super(ScaffoldBilstmAttentionClassifier1, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
    "        self.num_classes_sections = self.vocab.get_vocab_size(\"section_labels\")\n",
    "        self.num_classes_cite_worthiness = self.vocab.get_vocab_size(\"cite_worthiness_labels\")\n",
    "        self.citation_text_encoder = citation_text_encoder\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "        self.classifier_feedforward_2 = classifier_feedforward_2\n",
    "        self.classifier_feedforward_3 = classifier_feedforward_3\n",
    "\n",
    "        self.label_accuracy = CategoricalAccuracy()\n",
    "        self.label_f1_metrics = {}\n",
    "        self.label_f1_metrics_sections = {}\n",
    "        self.label_f1_metrics_cite_worthiness = {}\n",
    "        # for i in range(self.num_classes):\n",
    "        #     self.label_f1_metrics[vocab.get_token_from_index(index=i, namespace=\"labels\")] =\\\n",
    "        #         F1Measure(positive_label=i)\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            self.label_f1_metrics[vocab.get_token_from_index(index=i, namespace=\"labels\")] =\\\n",
    "                F1Measure(positive_label=i)\n",
    "        for i in range(self.num_classes_sections):\n",
    "            self.label_f1_metrics_sections[vocab.get_token_from_index(index=i, namespace=\"section_labels\")] =\\\n",
    "                F1Measure(positive_label=i)\n",
    "        for i in range(self.num_classes_cite_worthiness):\n",
    "            self.label_f1_metrics_cite_worthiness[vocab.get_token_from_index(index=i, namespace=\"cite_worthiness_labels\")] =\\\n",
    "                F1Measure(positive_label=i)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.attention_seq2seq = Attention(citation_text_encoder.get_output_dim())\n",
    "\n",
    "        self.report_auxiliary_metrics = report_auxiliary_metrics\n",
    "        self.predict_mode = predict_mode\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,\n",
    "                citation_text: Dict[str, torch.LongTensor],\n",
    "                labels: torch.LongTensor = None,\n",
    "                lexicon_features: Optional[torch.IntTensor] = None,\n",
    "                year_diff: Optional[torch.Tensor] = None,\n",
    "                citing_paper_id: Optional[str] = None,\n",
    "                cited_paper_id: Optional[str] = None,\n",
    "                citation_excerpt_index: Optional[str] = None,\n",
    "                citation_id: Optional[str] = None,\n",
    "                section_label: Optional[torch.Tensor] = None,\n",
    "                is_citation: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        Args:\n",
    "            citation_text: citation text of shape (batch, sent_len, embedding_dim)\n",
    "            labels: labels\n",
    "            lexicon_features: lexicon sparse features (batch, lexicon_feature_len)\n",
    "            year_diff: difference between cited and citing years\n",
    "            citing_paper_id: id of the citing paper\n",
    "            cited_paper_id: id of the cited paper\n",
    "            citation_excerpt_index: index of the excerpt\n",
    "            citation_id: unique id of the citation\n",
    "            section_label: label of the section\n",
    "            is_citation: citation worthiness label\n",
    "        \"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "        citation_text_embedding = self.text_field_embedder(citation_text)\n",
    "        print(\"citation_text_embedding\", citation_text_embedding.shape)\n",
    "        # print(\"citation_text\", citation_text) # {'elmo': tensor, 'tokens': tensor}\n",
    "        # tokens are converted into numbers using vocab\n",
    "        # print(\"citation_text_embedding\", citation_text_embedding) # tensor\n",
    "        citation_text_mask = util.get_text_field_mask(citation_text)\n",
    "\n",
    "        # shape: [batch, sent, output_dim]\n",
    "        encoded_citation_text = self.citation_text_encoder(citation_text_embedding, citation_text_mask)\n",
    "\n",
    "        # shape: [batch, output_dim]\n",
    "        attn_dist, encoded_citation_text = self.attention_seq2seq(encoded_citation_text, return_attn_distribution=True)\n",
    "\n",
    "        # In training mode, labels are the citation intents\n",
    "        # If in predict_mode, predict the citation intents\n",
    "        if labels is not None:\n",
    "            logits = self.classifier_feedforward(encoded_citation_text)\n",
    "            class_probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            output_dict = {\"logits\": logits}\n",
    "\n",
    "            loss = self.loss(logits, labels)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "            # compute F1 per label\n",
    "            for i in range(self.num_classes):\n",
    "                metric = self.label_f1_metrics[self.vocab.get_token_from_index(index=i, namespace=\"labels\")]\n",
    "                metric(class_probs, labels)\n",
    "            output_dict['labels'] = labels\n",
    "\n",
    "        if section_label is not None:  # this is the first scaffold task\n",
    "            logits = self.classifier_feedforward_2(encoded_citation_text)\n",
    "            class_probs = F.softmax(logits, dim=1)\n",
    "            output_dict = {\"logits\": logits}\n",
    "            loss = self.loss(logits, section_label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "            for i in range(self.num_classes_sections):\n",
    "                metric = self.label_f1_metrics_sections[self.vocab.get_token_from_index(index=i, namespace=\"section_labels\")]\n",
    "                metric(logits, section_label)\n",
    "\n",
    "        if is_citation is not None:  # second scaffold task\n",
    "            logits = self.classifier_feedforward_3(encoded_citation_text)\n",
    "            class_probs = F.softmax(logits, dim=1)\n",
    "            output_dict = {\"logits\": logits}\n",
    "            loss = self.loss(logits, is_citation)\n",
    "            output_dict[\"loss\"] = loss\n",
    "            for i in range(self.num_classes_cite_worthiness):\n",
    "                metric = self.label_f1_metrics_cite_worthiness[\n",
    "                    self.vocab.get_token_from_index(index=i, namespace=\"cite_worthiness_labels\")]\n",
    "                metric(logits, is_citation)\n",
    "\n",
    "        if self.predict_mode:\n",
    "            logits = self.classifier_feedforward(encoded_citation_text)\n",
    "            class_probs = F.softmax(logits, dim=1)\n",
    "            output_dict = {\"logits\": logits}\n",
    "\n",
    "        output_dict['citing_paper_id'] = citing_paper_id\n",
    "        output_dict['cited_paper_id'] = cited_paper_id\n",
    "        output_dict['citation_excerpt_index'] = citation_excerpt_index\n",
    "        output_dict['citation_id'] = citation_id\n",
    "        output_dict['attn_dist'] = attn_dist  # also return attention distribution for analysis\n",
    "        output_dict['citation_text'] = citation_text['tokens']\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        class_probabilities = F.softmax(output_dict['logits'], dim=-1)\n",
    "        predictions = class_probabilities.cpu().data.numpy()\n",
    "        argmax_indices = np.argmax(predictions, axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
    "                 for x in argmax_indices]\n",
    "        output_dict['probabilities'] = class_probabilities\n",
    "        output_dict['positive_labels'] = labels\n",
    "        output_dict['prediction'] = labels\n",
    "        citation_text = []\n",
    "        for batch_text in output_dict['citation_text']:\n",
    "            citation_text.append([self.vocab.get_token_from_index(token_id.item()) for token_id in batch_text])\n",
    "        output_dict['citation_text'] = citation_text\n",
    "        output_dict['all_labels'] = [self.vocab.get_index_to_token_vocabulary(namespace=\"labels\")\n",
    "                                     for _ in range(output_dict['logits'].shape[0])]\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metric_dict = {}\n",
    "\n",
    "        sum_f1 = 0.0\n",
    "        for name, metric in self.label_f1_metrics.items():\n",
    "            metric_val = metric.get_metric(reset)\n",
    "            metric_dict[name + '_P'] = metric_val[0]\n",
    "            metric_dict[name + '_R'] = metric_val[1]\n",
    "            metric_dict[name + '_F1'] = metric_val[2]\n",
    "            if name != 'none':  # do not consider `none` label in averaging F1\n",
    "                sum_f1 += metric_val[2]\n",
    "\n",
    "        names = list(self.label_f1_metrics.keys())\n",
    "        total_len = len(names) if 'none' not in names else len(names) - 1\n",
    "        average_f1 = sum_f1 / total_len\n",
    "        # metric_dict['combined_metric'] = (accuracy + average_f1) / 2\n",
    "        metric_dict['average_F1'] = average_f1\n",
    "\n",
    "        if self.report_auxiliary_metrics:\n",
    "            sum_f1 = 0.0\n",
    "            for name, metric in self.label_f1_metrics_sections.items():\n",
    "                metric_val = metric.get_metric(reset)\n",
    "                metric_dict['aux-sec--' + name + '_P'] = metric_val[0]\n",
    "                metric_dict['aux-sec--' + name + '_R'] = metric_val[1]\n",
    "                metric_dict['aux-sec--' + name + '_F1'] = metric_val[2]\n",
    "                if name != 'none':  # do not consider `none` label in averaging F1\n",
    "                    sum_f1 += metric_val[2]\n",
    "            names = list(self.label_f1_metrics_sections.keys())\n",
    "            total_len = len(names) if 'none' not in names else len(names) - 1\n",
    "            average_f1 = sum_f1 / total_len\n",
    "            # metric_dict['combined_metric'] = (accuracy + average_f1) / 2\n",
    "            metric_dict['aux-sec--' + 'average_F1'] = average_f1\n",
    "\n",
    "            sum_f1 = 0.0\n",
    "            for name, metric in self.label_f1_metrics_cite_worthiness.items():\n",
    "                metric_val = metric.get_metric(reset)\n",
    "                metric_dict['aux-worth--' + name + '_P'] = metric_val[0]\n",
    "                metric_dict['aux-worth--' + name + '_R'] = metric_val[1]\n",
    "                metric_dict['aux-worth--' + name + '_F1'] = metric_val[2]\n",
    "                if name != 'none':  # do not consider `none` label in averaging F1\n",
    "                    sum_f1 += metric_val[2]\n",
    "            names = list(self.label_f1_metrics_cite_worthiness.keys())\n",
    "            total_len = len(names) if 'none' not in names else len(names) - 1\n",
    "            average_f1 = sum_f1 / total_len\n",
    "            # metric_dict['combined_metric'] = (accuracy + average_f1) / 2\n",
    "            metric_dict['aux-worth--' + 'average_F1'] = average_f1\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'ScaffoldBilstmAttentionClassifier':\n",
    "        with_elmo = params.pop_bool(\"with_elmo\", False)\n",
    "        if with_elmo:\n",
    "            embedder_params = params.pop(\"elmo_text_field_embedder\")\n",
    "        else:\n",
    "            embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(embedder_params, vocab=vocab)\n",
    "        # citation_text_encoder = Seq2VecEncoder.from_params(params.pop(\"citation_text_encoder\"))\n",
    "        citation_text_encoder = Seq2SeqEncoder.from_params(params.pop(\"citation_text_encoder\"))\n",
    "        classifier_feedforward = FeedForward.from_params(params.pop(\"classifier_feedforward\"))\n",
    "        classifier_feedforward_2 = FeedForward.from_params(params.pop(\"classifier_feedforward_2\"))\n",
    "        classifier_feedforward_3 = FeedForward.from_params(params.pop(\"classifier_feedforward_3\"))\n",
    "\n",
    "        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n",
    "        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n",
    "\n",
    "        use_lexicon = params.pop_bool(\"use_lexicon_features\", False)\n",
    "        use_sparse_lexicon_features = params.pop_bool(\"use_sparse_lexicon_features\", False)\n",
    "        data_format = params.pop('data_format')\n",
    "\n",
    "        report_auxiliary_metrics = params.pop_bool(\"report_auxiliary_metrics\", False)\n",
    "\n",
    "        predict_mode = params.pop_bool(\"predict_mode\", False)\n",
    "        print(f\"pred mode: {predict_mode}\")\n",
    "\n",
    "        return cls(vocab=vocab,\n",
    "                   text_field_embedder=text_field_embedder,\n",
    "                   citation_text_encoder=citation_text_encoder,\n",
    "                   classifier_feedforward=classifier_feedforward,\n",
    "                   classifier_feedforward_2=classifier_feedforward_2,\n",
    "                   classifier_feedforward_3=classifier_feedforward_3,\n",
    "                   initializer=initializer,\n",
    "                   regularizer=regularizer,\n",
    "                   report_auxiliary_metrics=report_auxiliary_metrics,\n",
    "                   predict_mode=predict_mode)\n",
    "\n",
    "\n",
    "def new_parameter(*size):\n",
    "    out = Parameter(torch.FloatTensor(*size))\n",
    "    torch.nn.init.xavier_normal_(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" Simple multiplicative attention\"\"\"\n",
    "    def __init__(self, attention_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = new_parameter(attention_size, 1)\n",
    "\n",
    "    def forward(self, x_in, reduction_dim=-2, return_attn_distribution=False):\n",
    "        \"\"\"\n",
    "        return_attn_distribution: if True it will also return the original attention distribution\n",
    "\n",
    "        this reduces the one before last dimension in x_in to a weighted sum of the last dimension\n",
    "        e.g., x_in.shape == [64, 30, 100] -> output.shape == [64, 100]\n",
    "        Usage: You have a sentence of shape [batch, sent_len, embedding_dim] and you want to\n",
    "            represent sentence to a single vector using attention [batch, embedding_dim]\n",
    "\n",
    "        Here we use it to aggregate the lexicon-aware representation of the sentence\n",
    "        In two steps we convert [batch, sent_len, num_words_in_category, num_categories] into [batch, num_categories]\n",
    "        \"\"\"\n",
    "        # calculate attn weights\n",
    "        attn_score = torch.matmul(x_in, self.attention).squeeze()\n",
    "        # add one dimension at the end and get a distribution out of scores\n",
    "        attn_distrib = F.softmax(attn_score.squeeze(), dim=-1).unsqueeze(-1)\n",
    "        scored_x = x_in * attn_distrib\n",
    "        weighted_sum = torch.sum(scored_x, dim=reduction_dim)\n",
    "        if return_attn_distribution:\n",
    "            return attn_distrib.reshape(x_in.shape[0], -1), weighted_sum\n",
    "        else:\n",
    "            return weighted_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35af7f9a-8672-43e9-b147-b42b82b3ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params.from_file('experiment_configs/custom_config.json', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1591f-ee10-4071-ac6d-ae1ae79225f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.from_params(vocab=vocab, params=params.pop('model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d355ee0-f29e-4d9d-9a75-942fbbc481fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ef6740d3c4ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_model_state_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialization_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best.th'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_model_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_state_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "serialization_dir = \"runs/experiments-_0.05_0.05\"\n",
    "best_model_state_path = os.path.join(serialization_dir, 'best.th')\n",
    "best_model_state = torch.load(best_model_state_path)\n",
    "best_model = model\n",
    "best_model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34843264-1e59-4b00-b658-bb597d4e07ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c07ff8-f6f7-4c88-8d2c-137b3c17673f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248-scicite-torch",
   "language": "python",
   "name": "cs4248-scicite-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

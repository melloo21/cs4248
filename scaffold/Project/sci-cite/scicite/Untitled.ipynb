{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59641c2c-30ba-438c-866e-094c91351f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50)\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.token_indexers import ELMoTokenCharactersIndexer\n",
    "from allennlp.data import Token, Vocabulary\n",
    "import numpy as np\n",
    "# Create an instance of the ELMoTokenCharactersIndexer\n",
    "elmo_token_indexer = ELMoTokenCharactersIndexer()\n",
    "\n",
    "# Define some example tokens\n",
    "tokens = [Token('Hello'), Token('world'), Token('!')]\n",
    "\n",
    "# Index the tokens using ELMoTokenCharactersIndexer\n",
    "indexed_tokens = elmo_token_indexer.tokens_to_indices(tokens, Vocabulary(), 'train')\n",
    "\n",
    "# Print the indexed tokens\n",
    "print(np.array(indexed_tokens['train']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ef57a0-741c-4cc8-aa66-0e5df75931a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[259,\n",
       "  73,\n",
       "  102,\n",
       "  109,\n",
       "  109,\n",
       "  112,\n",
       "  260,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261],\n",
       " [259,\n",
       "  120,\n",
       "  112,\n",
       "  115,\n",
       "  109,\n",
       "  101,\n",
       "  260,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261],\n",
       " [259,\n",
       "  34,\n",
       "  260,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261,\n",
       "  261]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc71d6b7-5fe2-466b-af0b-6c20640546da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/kanhon/Desktop/kanhon/NUS Computing/CS4248/github_dir/cs4248/scaffold/Project/sci-cite/scicite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06443268-9bfc-4ac8-8103-0c77fde6f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data reader for AllenNLP \"\"\"\n",
    "\n",
    "\n",
    "from typing import Dict, List\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from allennlp.data import Field\n",
    "from overrides import overrides\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, MultiLabelField, ListField, ArrayField, MetadataField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, ELMoTokenCharactersIndexer\n",
    "\n",
    "from scicite.resources.lexicons import ALL_ACTION_LEXICONS, ALL_CONCEPT_LEXICONS\n",
    "from scicite.data import DataReaderJurgens\n",
    "from scicite.data import DataReaderS2, DataReaderS2ExcerptJL\n",
    "from scicite.compute_features import is_in_lexicon\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "from scicite.constants import S2_CATEGORIES, NONE_LABEL_NAME\n",
    "\n",
    "\n",
    "# @DatasetReader.register(\"scicite_datasetreader\")\n",
    "class SciciteDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads a JSON-lines file containing citations from the Semantic Scholar database, and creates a\n",
    "    dataset suitable for document classification using these papers.\n",
    "\n",
    "    The output of ``read`` is a list of ``Instance`` s with the fields:\n",
    "        citation_text: ``TextField``\n",
    "        label: ``LabelField``\n",
    "\n",
    "    where the ``label`` is derived from the methodology/comparison labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lazy : ``bool`` (optional, default=False)\n",
    "        Passed to ``DatasetReader``.  If this is ``True``, training will start sooner, but will\n",
    "        take longer per batch.  This also allows training with datasets that are too large to fit\n",
    "        in memory.\n",
    "    tokenizer : ``Tokenizer``, optional\n",
    "        Tokenizer to use to split the title and abstrct into words or other kinds of tokens.\n",
    "        Defaults to ``WordTokenizer()``.\n",
    "    token_indexers : ``Dict[str, TokenIndexer]``, optional\n",
    "        Indexers used to define input token representations. Defaults to ``{\"tokens\":\n",
    "        SingleIdTokenIndexer()}``.\n",
    "    reader_format : can be `flat` or `nested`. `flat` for flat json format and nested for\n",
    "        Json format where the each object contains multiple excerpts\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 use_lexicon_features: bool=False,\n",
    "                 use_sparse_lexicon_features: bool = False,\n",
    "                 multilabel: bool = False,\n",
    "                 with_elmo: bool = False,\n",
    "                 reader_format: str = 'flat') -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer() # using WordTokenizer() because config['tokenizer'] not specified \n",
    "        if with_elmo:\n",
    "            # self._token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "            self._token_indexers = {\"elmo\": ELMoTokenCharactersIndexer(),\n",
    "                                    \"tokens\": SingleIdTokenIndexer()}\n",
    "        else:\n",
    "            self._token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "        self.use_lexicon_features = use_lexicon_features\n",
    "        self.use_sparse_lexicon_features = use_sparse_lexicon_features\n",
    "        if self.use_lexicon_features or self.use_sparse_lexicon_features:\n",
    "            self.lexicons = {**ALL_ACTION_LEXICONS, **ALL_CONCEPT_LEXICONS}\n",
    "        self.multilabel = multilabel\n",
    "        self.reader_format = reader_format\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, jsonl_file: str):\n",
    "        if self.reader_format == 'flat':\n",
    "            reader_s2 = DataReaderS2ExcerptJL(jsonl_file)\n",
    "        elif self.reader_format == 'nested':\n",
    "            reader_s2 = DataReaderS2(jsonl_file)\n",
    "        for citation in reader_s2.read():\n",
    "            yield self.text_to_instance(\n",
    "                citation_text=citation.text,\n",
    "                intent=citation.intent,\n",
    "                citing_paper_id=citation.citing_paper_id,\n",
    "                cited_paper_id=citation.cited_paper_id,\n",
    "                citation_excerpt_index=citation.citation_excerpt_index\n",
    "            )\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self,\n",
    "                         citation_text: str,\n",
    "                         citing_paper_id: str,\n",
    "                         cited_paper_id: str,\n",
    "                         intent: List[str] = None,\n",
    "                         citing_paper_title: str = None,\n",
    "                         cited_paper_title: str = None,\n",
    "                         citing_paper_year: int = None,\n",
    "                         cited_paper_year: int = None,\n",
    "                         citing_author_ids: List[str] = None,\n",
    "                         cited_author_ids: List[str] = None,\n",
    "                         extended_context: str = None,\n",
    "                         section_number: int = None,\n",
    "                         section_title: str = None,\n",
    "                         cite_marker_begin: int = None,\n",
    "                         cite_marker_end: int = None,\n",
    "                         sents_before: List[str] = None,\n",
    "                         sents_after: List[str] = None,\n",
    "                         cleaned_cite_text: str = None,\n",
    "                         citation_excerpt_index: str = None,\n",
    "                         venue: str = None) -> Instance:  # type: ignore\n",
    "\n",
    "        citation_tokens = self._tokenizer.tokenize(citation_text)\n",
    "\n",
    "        fields = {\n",
    "            'citation_text': TextField(citation_tokens, self._token_indexers),\n",
    "        }\n",
    "\n",
    "        if self.use_sparse_lexicon_features:\n",
    "            # convert to regular string\n",
    "            sent = [token.text.lower() for token in citation_tokens]\n",
    "            lexicon_features, _ = is_in_lexicon(self.lexicons, sent)\n",
    "            fields[\"lexicon_features\"] = ListField([LabelField(feature, skip_indexing=True)\n",
    "                                                    for feature in lexicon_features])\n",
    "\n",
    "        if intent:\n",
    "            if self.multilabel:\n",
    "                fields['labels'] = MultiLabelField([S2_CATEGORIES[e] for e in intent], skip_indexing=True,\n",
    "                                                   num_labels=len(S2_CATEGORIES))\n",
    "            else:\n",
    "                if not isinstance(intent, str):\n",
    "                    raise TypeError(f\"Undefined label format. Should be a string. Got: f'{intent}'\")\n",
    "                fields['labels'] = LabelField(intent)\n",
    "\n",
    "        if citing_paper_year and cited_paper_year and \\\n",
    "                citing_paper_year > -1 and cited_paper_year > -1:\n",
    "            year_diff = citing_paper_year - cited_paper_year\n",
    "        else:\n",
    "            year_diff = -1\n",
    "        fields['year_diff'] = ArrayField(torch.Tensor([year_diff]))\n",
    "        fields['citing_paper_id'] = MetadataField(citing_paper_id)\n",
    "        fields['cited_paper_id'] = MetadataField(cited_paper_id)\n",
    "        fields['citation_excerpt_index'] = MetadataField(citation_excerpt_index)\n",
    "        fields['citation_id'] = MetadataField(f\"{citing_paper_id}>{cited_paper_id}\")\n",
    "        return Instance(fields)\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, params: Params) -> 'SciciteDatasetReader':\n",
    "        lazy = params.pop('lazy', False)\n",
    "        tokenizer = Tokenizer.from_params(params.pop('tokenizer', {}))\n",
    "        use_lexicon_features = params.pop_bool(\"use_lexicon_features\", False)\n",
    "        use_sparse_lexicon_features = params.pop_bool(\"use_sparse_lexicon_features\", False)\n",
    "        multilabel = params.pop_bool(\"multilabel\")\n",
    "        with_elmo = params.pop_bool(\"with_elmo\", False)\n",
    "        reader_format = params.pop(\"reader_format\", 'flat')\n",
    "        params.assert_empty(cls.__name__)\n",
    "        return cls(lazy=lazy, tokenizer=tokenizer,\n",
    "                   use_lexicon_features=use_lexicon_features,\n",
    "                   use_sparse_lexicon_features=use_sparse_lexicon_features,\n",
    "                   multilabel=multilabel,\n",
    "                   with_elmo=with_elmo,\n",
    "                   reader_format=reader_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f42b41b3-7869-440f-b99d-cd487546fb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[this, is, a, happy, pancake, !]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_text = \"this is a happy pancake!\"\n",
    "citation_tokens = WordTokenizer().tokenize(citation_text)\n",
    "citation_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bdf5b75-32e4-4653-a319-2701bc17a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SingleIdTokenIndexer --> one embedding for each word\n",
    "_token_indexers = {\"elmo\": ELMoTokenCharactersIndexer(),\n",
    "                                    \"tokens\": SingleIdTokenIndexer()}\n",
    "fields = {\n",
    "    'citation_text': TextField(citation_tokens, _token_indexers),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0207a2b9-a4e2-4a86-9c00-abc0d4cdb9f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-97eb23440d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m vocab = VocabularyMultitask.from_params(\n\u001b[1;32m      8\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocabulary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         (instance for key, dataset in all_datasets.items()\n\u001b[0m\u001b[1;32m     10\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m          if key in datasets_for_vocab_creation),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# from scicite.training.vocabulary_multitask import VocabularyMultitask\n",
    "\n",
    "# params = Params.from_file('experiment_configs/custom_config.json', \"\")\n",
    "# serialization_dir = './runs/test'\n",
    "# file_friendly_logging = False\n",
    "# recover = False\n",
    "# vocab = VocabularyMultitask.from_params(\n",
    "#         params.pop(\"vocabulary\", {}),\n",
    "#         (instance for key, dataset in all_datasets.items()\n",
    "#          for instance in dataset\n",
    "#          if key in datasets_for_vocab_creation),\n",
    "#         instances_aux=vocab_instances_aux\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a3ad0f6-7075-4235-bf3b-cd9f5ba5d9ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-743ae34849fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'citation_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "fields['citation_text'].index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec9c5910-4c84-46bc-a08f-90684e535335",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "future feature annotations is not defined (s3.py, line 9)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3343\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-45-291ca1bd053c>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from gensim.models.keyedvectors import KeyedVectors\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/gensim/__init__.py\"\u001b[0m, line \u001b[1;32m11\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/gensim/parsing/__init__.py\"\u001b[0m, line \u001b[1;32m4\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from .preprocessing import (  # noqa:F401\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/gensim/parsing/preprocessing.py\"\u001b[0m, line \u001b[1;32m26\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from gensim import utils\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/gensim/utils.py\"\u001b[0m, line \u001b[1;32m37\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from smart_open import open\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/smart_open/__init__.py\"\u001b[0m, line \u001b[1;32m34\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/smart_open/smart_open_lib.py\"\u001b[0m, line \u001b[1;32m35\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from smart_open import doctools\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/smart_open/doctools.py\"\u001b[0m, line \u001b[1;32m21\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from . import transport\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/smart_open/transport.py\"\u001b[0m, line \u001b[1;32m104\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    register_transport(\"smart_open.s3\")\n",
      "  File \u001b[1;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/smart_open/transport.py\"\u001b[0m, line \u001b[1;32m49\u001b[0m, in \u001b[1;35mregister_transport\u001b[0m\n    submodule = importlib.import_module(submodule)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/importlib/__init__.py\"\u001b[0;36m, line \u001b[0;32m126\u001b[0;36m, in \u001b[0;35mimport_module\u001b[0;36m\u001b[0m\n\u001b[0;31m    return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/kanhon/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/smart_open/s3.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    from __future__ import annotations\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m future feature annotations is not defined\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fdc929-4bc5-4f20-b314-6fc586ff990e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d9487607936f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"@@NUM@@\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "nlp(\"@@NUM@@\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6af467-7d38-4c81-b0bb-e3360d63e36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248-scicite-torch",
   "language": "python",
   "name": "cs4248-scicite-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc000ae-418f-462b-b9e4-3d39a91dd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdee21-1e4e-4146-b126-2e2215dfe87a",
   "metadata": {},
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ab5656-adbc-4f6f-8a3a-0b8b01c1a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.common import Params\n",
    "import os\n",
    "import logging\n",
    "from scicite.dataset_readers.citation_data_reader_scicite import SciciteDatasetReader\n",
    "from allennlp.data.instance import Instance\n",
    "\n",
    "from typing import Dict, Iterable, Tuple\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "os.environ['SEED'] = '21016'\n",
    "os.environ['PYTORCH_SEED'] = str(int(os.environ['SEED']) // 3)\n",
    "os.environ['NUMPY_SEED'] = str(int(os.environ['PYTORCH_SEED']) // 3)\n",
    "os.environ[\"elmo\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eed15e5-ffe3-44b1-be62-a7a2cb1b423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2024 22:23:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'multilabel': False, 'type': 'scicite_datasetreader', 'use_sparse_lexicon_features': False, 'with_elmo': 'true'} and extras {}\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.type = scicite_datasetreader\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {} and extras {}\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.tokenizer.type = word\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras {}\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.tokenizer.start_tokens = None\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.tokenizer.end_tokens = None\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.use_lexicon_features = False\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.use_sparse_lexicon_features = False\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.multilabel = False\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.with_elmo = true\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   dataset_reader.reader_format = flat\n"
     ]
    }
   ],
   "source": [
    "parameter_filename = \"./experiment_configs/scicite-experiment-0.05-0.05.json\"\n",
    "overrides = \"\"\n",
    "params = Params.from_file(parameter_filename, overrides)\n",
    "\n",
    "dataset_reader = DatasetReader.from_params(params.pop('dataset_reader'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84a37e50-0a37-42f6-9192-e3cfe3d59b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d2eb45d-1d4d-438b-91ec-a51688adcc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   validation_dataset_reader = None\n",
      "04/10/2024 22:23:04 - INFO - allennlp.common.params -   train_data_path = scicite_data/train.jsonl\n",
      "04/10/2024 22:23:04 - INFO - __main__ -   Reading training data from scicite_data/train.jsonl\n",
      "8243it [00:02, 3274.03it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n",
    "validation_and_test_dataset_reader: DatasetReader = dataset_reader\n",
    "if validation_dataset_reader_params is not None:\n",
    "    logger.info(\"Using a separate dataset reader to load validation and test data.\")\n",
    "    validation_and_test_dataset_reader = DatasetReader.from_params(validation_dataset_reader_params)\n",
    "\n",
    "train_data_path = params.pop('train_data_path')\n",
    "logger.info(\"Reading training data from %s\", train_data_path)\n",
    "train_data = dataset_reader.read(train_data_path)\n",
    "\n",
    "datasets: Dict[str, Iterable[Instance]] = {\"train\": train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e665f796-3254-4506-8f86-c59fb9e6b97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'citation_text': <allennlp.data.fields.text_field.TextField at 0x7f13cc548ba8>,\n",
       " 'labels': <allennlp.data.fields.label_field.LabelField at 0x7f13cc548978>,\n",
       " 'year_diff': <allennlp.data.fields.array_field.ArrayField at 0x7f13cc548ac8>,\n",
       " 'citing_paper_id': <allennlp.data.fields.metadata_field.MetadataField at 0x7f13cc548cf8>,\n",
       " 'cited_paper_id': <allennlp.data.fields.metadata_field.MetadataField at 0x7f13cc548d30>,\n",
       " 'citation_excerpt_index': <allennlp.data.fields.metadata_field.MetadataField at 0x7f13cc548e80>,\n",
       " 'citation_id': <allennlp.data.fields.metadata_field.MetadataField at 0x7f13cc548da0>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed8a15-a72f-4561-a85b-a73178ad1096",
   "metadata": {},
   "source": [
    "### Train_multitask_two_tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f407a77-e966-4528-96b5-4403fb445f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The `train_multitask` subcommand that can be used to train the model in the multitask fashion\n",
    "It requires a configuration file and a directory in\n",
    "which to write the results.\n",
    ".. code-block:: bash\n",
    "   $ allennlp train --help\n",
    "   usage: allennlp train [-h] -s SERIALIZATION_DIR [-r] [-o OVERRIDES]\n",
    "                         [--file-friendly-logging]\n",
    "                         [--include-package INCLUDE_PACKAGE]\n",
    "                         param_path\n",
    "   Train the specified model on the specified dataset.\n",
    "   positional arguments:\n",
    "   param_path            path to parameter file describing the model to be\n",
    "                           trained\n",
    "   optional arguments:\n",
    "   -h, --help            show this help message and exit\n",
    "   -s SERIALIZATION_DIR, --serialization-dir SERIALIZATION_DIR\n",
    "                           directory in which to save the model and its logs\n",
    "   -r, --recover         recover training from the state in serialization_dir\n",
    "   -o OVERRIDES, --overrides OVERRIDES\n",
    "                           a JSON structure used to override the experiment\n",
    "                           configuration\n",
    "   --include-package INCLUDE_PACKAGE\n",
    "                           additional packages to include\n",
    "   --file-friendly-logging\n",
    "                           outputs tqdm status on separate lines and slows tqdm\n",
    "                           refresh rate\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, Iterable, Tuple\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "from allennlp.commands.evaluate import evaluate\n",
    "from allennlp.commands.subcommand import Subcommand\n",
    "from allennlp.common.checks import ConfigurationError, check_for_gpu\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.util import prepare_environment, prepare_global_logging, \\\n",
    "                                 get_frozen_and_tunable_parameter_names, dump_metrics\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.archival import archive_model, CONFIG_NAME\n",
    "from allennlp.models.model import Model, _DEFAULT_WEIGHTS\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "# from scicite.training.multitask_trainer_two_tasks import MultiTaskTrainer2\n",
    "from scicite.training.vocabulary_multitask import VocabularyMultitask\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "class TrainMultiTask2(Subcommand):\n",
    "    \"\"\" Class for training the model with two scaffold tasks \"\"\"\n",
    "    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n",
    "        # pylint: disable=protected-access\n",
    "        description = '''Train the specified model on the specified dataset.'''\n",
    "        subparser = parser.add_parser(name, description=description, help='Train a model')\n",
    "\n",
    "        subparser.add_argument('param_path',\n",
    "                               type=str,\n",
    "                               help='path to parameter file describing the model to be trained')\n",
    "\n",
    "        subparser.add_argument('-s', '--serialization-dir',\n",
    "                               required=True,\n",
    "                               type=str,\n",
    "                               help='directory in which to save the model and its logs')\n",
    "\n",
    "        subparser.add_argument('-r', '--recover',\n",
    "                               action='store_true',\n",
    "                               default=False,\n",
    "                               help='recover training from the state in serialization_dir')\n",
    "\n",
    "        subparser.add_argument('-o', '--overrides',\n",
    "                               type=str,\n",
    "                               default=\"\",\n",
    "                               help='a JSON structure used to override the experiment configuration')\n",
    "\n",
    "        subparser.add_argument('--file-friendly-logging',\n",
    "                               action='store_true',\n",
    "                               default=False,\n",
    "                               help='outputs tqdm status on separate lines and slows tqdm refresh rate')\n",
    "\n",
    "        subparser.set_defaults(func=train_model_from_args)\n",
    "\n",
    "        return subparser\n",
    "\n",
    "def train_model_from_args(args: argparse.Namespace):\n",
    "    \"\"\"\n",
    "    Just converts from an ``argparse.Namespace`` object to string paths.\n",
    "    \"\"\"\n",
    "    train_model_from_file(args.param_path,\n",
    "                          args.serialization_dir,\n",
    "                          args.overrides,\n",
    "                          args.file_friendly_logging,\n",
    "                          args.recover)\n",
    "\n",
    "\n",
    "def train_model_from_file(parameter_filename: str,\n",
    "                          serialization_dir: str,\n",
    "                          overrides: str = \"\",\n",
    "                          file_friendly_logging: bool = False,\n",
    "                          recover: bool = False) -> Model:\n",
    "    \"\"\"\n",
    "    A wrapper around :func:`train_model` which loads the params from a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    param_path : ``str``\n",
    "        A json parameter file specifying an AllenNLP experiment.\n",
    "    serialization_dir : ``str``\n",
    "        The directory in which to save results and logs. We just pass this along to\n",
    "        :func:`train_model`.\n",
    "    overrides : ``str``\n",
    "        A JSON string that we will use to override values in the input parameter file.\n",
    "    file_friendly_logging : ``bool``, optional (default=False)\n",
    "        If ``True``, we make our output more friendly to saved model files.  We just pass this\n",
    "        along to :func:`train_model`.\n",
    "    recover : ``bool`, optional (default=False)\n",
    "        If ``True``, we will try to recover a training run from an existing serialization\n",
    "        directory.  This is only intended for use when something actually crashed during the middle\n",
    "        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n",
    "    \"\"\"\n",
    "    # Load the experiment config from a file and pass it to ``train_model``.\n",
    "    params = Params.from_file(parameter_filename, overrides)\n",
    "    return train_model(params, serialization_dir, file_friendly_logging, recover)\n",
    "\n",
    "\n",
    "def datasets_from_params(params: Params) -> Tuple[Dict[str, Iterable[Instance]], Dict[str, Iterable[Instance]], Dict[str, Iterable[Instance]]]:\n",
    "    \"\"\"\n",
    "    Load all the datasets specified by the config.\n",
    "    This includes the main dataset and the two scaffold auxiliary datasets\n",
    "    \"\"\"\n",
    "    dataset_reader = DatasetReader.from_params(params.pop('dataset_reader'))\n",
    "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n",
    "\n",
    "    validation_and_test_dataset_reader: DatasetReader = dataset_reader\n",
    "    if validation_dataset_reader_params is not None:\n",
    "        logger.info(\"Using a separate dataset reader to load validation and test data.\")\n",
    "        validation_and_test_dataset_reader = DatasetReader.from_params(validation_dataset_reader_params)\n",
    "\n",
    "    train_data_path = params.pop('train_data_path')\n",
    "    logger.info(\"Reading training data from %s\", train_data_path)\n",
    "    train_data = dataset_reader.read(train_data_path)\n",
    "\n",
    "    datasets: Dict[str, Iterable[Instance]] = {\"train\": train_data}\n",
    "\n",
    "    # 2. Auxillary training data.\n",
    "    dataset_reader_aux = DatasetReader.from_params(params.pop('dataset_reader_aux'))\n",
    "    train_data_path_aux = params.pop('train_data_path_aux')\n",
    "    logger.info(\"Reading auxiliary training data from %s\", train_data_path_aux)\n",
    "    train_data_aux = dataset_reader_aux.read(train_data_path_aux)\n",
    "\n",
    "    dataset_reader_aux2 = DatasetReader.from_params(params.pop('dataset_reader_aux2'))\n",
    "    train_data_path_aux2 = params.pop('train_data_path_aux2')\n",
    "    logger.info(\"Reading second auxiliary training data for from %s\", train_data_path_aux2)\n",
    "    train_data_aux2 = dataset_reader_aux2.read(train_data_path_aux2)\n",
    "\n",
    "    # If only using a fraction of the auxiliary data.\n",
    "    aux_sample_fraction = params.pop(\"aux_sample_fraction\", 1.0)\n",
    "    if aux_sample_fraction < 1.0:\n",
    "        sample_size = int(aux_sample_fraction * len(train_data_aux))\n",
    "        train_data_aux = random.sample(train_data_aux, sample_size)\n",
    "        train_data_aux2 = random.sample(train_data_aux2, sample_size)\n",
    "\n",
    "    # Balance the datasets by inflating the size of the smaller dataset to the size of the larger dataset.\n",
    "    train_size = len(train_data)\n",
    "    aux_train_size = len(train_data_aux)\n",
    "    aux2_train_size = len(train_data_aux2)\n",
    "\n",
    "    # Make second auxillary dataset the same size of the first auxiliary dataset\n",
    "    if aux2_train_size > aux_train_size:\n",
    "        train_data_aux2 = random.sample(train_data_aux2, aux_train_size)\n",
    "    else:\n",
    "        train_data_aux = random.sample(train_data_aux, aux2_train_size)\n",
    "\n",
    "    # inflate training size to be as large as auxiliary training data\n",
    "    if train_size > aux_train_size:\n",
    "        difference = train_size - aux_train_size\n",
    "        aux_sample = [random.choice(train_data_aux) for _ in range(difference)]\n",
    "        train_data_aux = train_data_aux + aux_sample\n",
    "        logger.info(\"Inflating auxiliary train data from {} to {} samples\".format(\n",
    "            aux_train_size, len(train_data_aux)))\n",
    "    else:\n",
    "        difference = aux_train_size - train_size\n",
    "        train_sample = [random.choice(train_data) for _ in range(difference)]\n",
    "        train_data = train_data + train_sample\n",
    "        logger.info(\"Inflating train data from {} to {} samples\".format(\n",
    "            train_size, len(train_data)))\n",
    "\n",
    "    datasets[\"train\"] = train_data\n",
    "    datasets_aux = {\"train_aux\": train_data_aux}\n",
    "    datasets_aux2 = {\"train_aux\": train_data_aux2}\n",
    "\n",
    "    validation_data_path = params.pop('validation_data_path', None)\n",
    "    if validation_data_path is not None:\n",
    "        logger.info(\"Reading validation data from %s\", validation_data_path)\n",
    "        validation_data = validation_and_test_dataset_reader.read(validation_data_path)\n",
    "        datasets[\"validation\"] = validation_data\n",
    "\n",
    "    # Auxiliary validation data.\n",
    "    validation_data_path_aux = params.pop('validation_data_path_aux', None)\n",
    "    if validation_data_path_aux is not None:\n",
    "        logger.info(f\"Reading auxilliary validation data from {validation_data_path_aux}\")\n",
    "        validation_data_aux = dataset_reader_aux.read(validation_data_path_aux)\n",
    "        datasets_aux[\"validation_aux\"] = validation_data_aux\n",
    "    else:\n",
    "        validation_data_aux = None\n",
    "    validation_data_path_aux2 = params.pop('validation_data_path_aux2', None)\n",
    "    if validation_data_path_aux2 is not None:\n",
    "        logger.info(f\"Reading auxilliary validation data from {validation_data_path_aux2}\")\n",
    "        validation_data_aux2 = dataset_reader_aux2.read(validation_data_path_aux2)\n",
    "        datasets_aux2[\"validation_aux\"] = validation_data_aux2\n",
    "    else:\n",
    "        validation_data_aux2 = None\n",
    "\n",
    "    test_data_path = params.pop(\"test_data_path\", None)\n",
    "    if test_data_path is not None:\n",
    "        logger.info(\"Reading test data from %s\", test_data_path)\n",
    "        test_data = validation_and_test_dataset_reader.read(test_data_path)\n",
    "        datasets[\"test\"] = test_data\n",
    "\n",
    "    # Auxillary test data\n",
    "    test_data_path_aux = params.pop(\"test_data_path_aux\", None)\n",
    "    if test_data_path_aux is not None:\n",
    "        logger.info(f\"Reading auxiliary test data from {test_data_path_aux}\")\n",
    "        test_data_aux = dataset_reader_aux.read(test_data_path_aux)\n",
    "        datasets_aux[\"test_aux\"] = test_data_aux\n",
    "    else:\n",
    "        test_data_aux = None\n",
    "\n",
    "    test_data_path_aux2 = params.pop(\"test_data_path_aux2\", None)\n",
    "    if test_data_path_aux2 is not None:\n",
    "        logger.info(f\"Reading auxillary test data from {test_data_path_aux2}\")\n",
    "        test_data_aux2 = dataset_reader_aux2.read(test_data_path_aux2)\n",
    "        datasets_aux2[\"test_aux\"] = test_data_aux2\n",
    "    else:\n",
    "        test_data_aux2 = None\n",
    "\n",
    "    return datasets, datasets_aux, datasets_aux2\n",
    "\n",
    "def create_serialization_dir(params: Params, serialization_dir: str, recover: bool) -> None:\n",
    "    \"\"\"\n",
    "    This function creates the serialization directory if it doesn't exist.  If it already exists\n",
    "    and is non-empty, then it verifies that we're recovering from a training with an identical configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params: ``Params``\n",
    "        A parameter object specifying an AllenNLP Experiment.\n",
    "    serialization_dir: ``str``\n",
    "        The directory in which to save results and logs.\n",
    "    recover: ``bool``\n",
    "        If ``True``, we will try to recover from an existing serialization directory, and crash if\n",
    "        the directory doesn't exist, or doesn't match the configuration we're given.\n",
    "    \"\"\"\n",
    "    if os.path.exists(serialization_dir) and os.listdir(serialization_dir):\n",
    "        if not recover:\n",
    "            raise ConfigurationError(f\"Serialization directory ({serialization_dir}) already exists and is \"\n",
    "                                     f\"not empty. Specify --recover to recover training from existing output.\")\n",
    "\n",
    "        logger.info(f\"Recovering from prior training at {serialization_dir}.\")\n",
    "\n",
    "        recovered_config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
    "        if not os.path.exists(recovered_config_file):\n",
    "            raise ConfigurationError(\"The serialization directory already exists but doesn't \"\n",
    "                                     \"contain a config.json. You probably gave the wrong directory.\")\n",
    "        else:\n",
    "            loaded_params = Params.from_file(recovered_config_file)\n",
    "\n",
    "            # Check whether any of the training configuration differs from the configuration we are\n",
    "            # resuming.  If so, warn the user that training may fail.\n",
    "            fail = False\n",
    "            flat_params = params.as_flat_dict()\n",
    "            flat_loaded = loaded_params.as_flat_dict()\n",
    "            for key in flat_params.keys() - flat_loaded.keys():\n",
    "                logger.error(f\"Key '{key}' found in training configuration but not in the serialization \"\n",
    "                             f\"directory we're recovering from.\")\n",
    "                fail = True\n",
    "            for key in flat_loaded.keys() - flat_params.keys():\n",
    "                logger.error(f\"Key '{key}' found in the serialization directory we're recovering from \"\n",
    "                             f\"but not in the training config.\")\n",
    "                fail = True\n",
    "            for key in flat_params.keys():\n",
    "                if flat_params.get(key, None) != flat_loaded.get(key, None):\n",
    "                    logger.error(f\"Value for '{key}' in training configuration does not match that the value in \"\n",
    "                                 f\"the serialization directory we're recovering from: \"\n",
    "                                 f\"{flat_params[key]} != {flat_loaded[key]}\")\n",
    "                    fail = True\n",
    "            if fail:\n",
    "                raise ConfigurationError(\"Training configuration does not match the configuration we're \"\n",
    "                                         \"recovering from.\")\n",
    "    else:\n",
    "        if recover:\n",
    "            raise ConfigurationError(f\"--recover specified but serialization_dir ({serialization_dir}) \"\n",
    "                                     \"does not exist.  There is nothing to recover from.\")\n",
    "        os.makedirs(serialization_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def train_model(params: Params,\n",
    "                serialization_dir: str,\n",
    "                file_friendly_logging: bool = False,\n",
    "                recover: bool = False) -> Model:\n",
    "    \"\"\"\n",
    "    Trains the model specified in the given :class:`Params` object, using the data and training\n",
    "    parameters also specified in that object, and saves the results in ``serialization_dir``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : ``Params``\n",
    "        A parameter object specifying an AllenNLP Experiment.\n",
    "    serialization_dir : ``str``\n",
    "        The directory in which to save results and logs.\n",
    "    file_friendly_logging : ``bool``, optional (default=False)\n",
    "        If ``True``, we add newlines to tqdm output, even on an interactive terminal, and we slow\n",
    "        down tqdm's output to only once every 10 seconds.\n",
    "    recover : ``bool``, optional (default=False)\n",
    "        If ``True``, we will try to recover a training run from an existing serialization\n",
    "        directory.  This is only intended for use when something actually crashed during the middle\n",
    "        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_model: ``Model``\n",
    "        The model with the best epoch weights.\n",
    "    \"\"\"\n",
    "    prepare_environment(params)\n",
    "\n",
    "    create_serialization_dir(params, serialization_dir, recover)\n",
    "    prepare_global_logging(serialization_dir, file_friendly_logging)\n",
    "\n",
    "    check_for_gpu(params.get('trainer').get('cuda_device', -1))\n",
    "\n",
    "    params.to_file(os.path.join(serialization_dir, CONFIG_NAME))\n",
    "\n",
    "    all_datasets, all_datasets_aux, all_datasets_aux2 = datasets_from_params(params)\n",
    "    datasets_for_vocab_creation = set(params.pop(\"datasets_for_vocab_creation\", all_datasets))\n",
    "    datasets_for_vocab_creation_aux = set(params.pop(\"auxiliary_datasets_for_vocab_creation\", all_datasets_aux))\n",
    "    datasets_for_vocab_creation_aux2 = set(params.pop(\"auxiliary_datasets_for_vocab_creation_2\", all_datasets_aux2))\n",
    "\n",
    "\n",
    "    mixing_ratio = params.pop_float(\"mixing_ratio\")\n",
    "    mixing_ratio2 = params.pop_float(\"mixing_ratio2\")\n",
    "\n",
    "    cutoff_epoch = params.pop(\"cutoff_epoch\", -1)\n",
    "\n",
    "    for dataset in datasets_for_vocab_creation:\n",
    "        if dataset not in all_datasets:\n",
    "            raise ConfigurationError(f\"invalid 'dataset_for_vocab_creation' {dataset}\")\n",
    "\n",
    "    logger.info(\"From dataset instances, %s will be considered for vocabulary creation.\",\n",
    "                \", \".join(datasets_for_vocab_creation))\n",
    "    vocab_instances_aux = [\n",
    "        instance for key, dataset in all_datasets_aux.items()\n",
    "        for instance in dataset\n",
    "        if key in datasets_for_vocab_creation_aux\n",
    "    ]\n",
    "    vocab_instances_aux.extend([\n",
    "        instance for key, dataset in all_datasets_aux2.items()\n",
    "        for instance in dataset\n",
    "        if key in datasets_for_vocab_creation_aux2\n",
    "    ])\n",
    "    vocab = VocabularyMultitask.from_params(\n",
    "            params.pop(\"vocabulary\", {}),\n",
    "            (instance for key, dataset in all_datasets.items()\n",
    "             for instance in dataset\n",
    "             if key in datasets_for_vocab_creation),\n",
    "            instances_aux=vocab_instances_aux\n",
    "    )\n",
    "    model = Model.from_params(vocab=vocab, params=params.pop('model'))\n",
    "\n",
    "    # Initializing the model can have side effect of expanding the vocabulary\n",
    "    vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))\n",
    "    \n",
    "    iterator = DataIterator.from_params(params.pop(\"iterator\"))\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    iterator_aux = DataIterator.from_params(params.pop(\"iterator_aux\"))\n",
    "    iterator_aux.index_with(vocab)\n",
    "\n",
    "    iterator_aux2 = DataIterator.from_params(params.pop(\"iterator_aux2\"))\n",
    "    iterator_aux2.index_with(vocab)\n",
    "\n",
    "    validation_iterator_params = params.pop(\"validation_iterator\", None)\n",
    "    if validation_iterator_params:\n",
    "        validation_iterator = DataIterator.from_params(validation_iterator_params)\n",
    "        validation_iterator.index_with(vocab)\n",
    "    else:\n",
    "        validation_iterator = None\n",
    "\n",
    "    # TODO: if validation in multi-task need to add validation iterator as above\n",
    "\n",
    "    train_data = all_datasets.get('train')\n",
    "    validation_data = all_datasets.get('validation')\n",
    "    test_data = all_datasets.get('test')\n",
    "\n",
    "    train_data_aux = all_datasets_aux.get('train_aux')\n",
    "    validation_data_aux = all_datasets_aux.get('validation_aux')\n",
    "    test_data_aux = all_datasets_aux.get('test_aux')\n",
    "\n",
    "    train_data_aux2 = all_datasets_aux2.get('train_aux')\n",
    "    validation_data_aux2 = all_datasets_aux2.get('validation_aux')\n",
    "    test_data_aux2 = all_datasets_aux2.get('test_aux')\n",
    "\n",
    "    trainer_params = params.pop(\"trainer\")\n",
    "    no_grad_regexes = trainer_params.pop(\"no_grad\", ())\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if any(re.search(regex, name) for regex in no_grad_regexes):\n",
    "            parameter.requires_grad_(False)\n",
    "\n",
    "    frozen_parameter_names, tunable_parameter_names = \\\n",
    "                   get_frozen_and_tunable_parameter_names(model)\n",
    "    logger.info(\"Following parameters are Frozen  (without gradient):\")\n",
    "    for name in frozen_parameter_names:\n",
    "        logger.info(name)\n",
    "    logger.info(\"Following parameters are Tunable (with gradient):\")\n",
    "    for name in tunable_parameter_names:\n",
    "        logger.info(name)\n",
    "\n",
    "    trainer = MultiTaskTrainer2.from_params(model=model,\n",
    "                                            serialization_dir=serialization_dir,\n",
    "                                            iterator=iterator,\n",
    "                                            iterator_aux=iterator_aux,\n",
    "                                            iterator_aux2=iterator_aux2,\n",
    "                                            train_data=train_data,\n",
    "                                            train_data_aux=train_data_aux,\n",
    "                                            train_data_aux2=train_data_aux2,\n",
    "                                            mixing_ratio=mixing_ratio,\n",
    "                                            mixing_ratio2=mixing_ratio2,\n",
    "                                            cutoff_epoch=cutoff_epoch,\n",
    "                                            validation_data_aux=validation_data_aux,\n",
    "                                            validation_data_aux2=validation_data_aux2,\n",
    "                                            validation_data=validation_data,\n",
    "                                            params=trainer_params,\n",
    "                                            validation_iterator=validation_iterator)\n",
    "    print(trainer._cuda_devices[0])\n",
    "    evaluate_on_test = params.pop_bool(\"evaluate_on_test\", False)\n",
    "    evaluate_aux_on_test = params.pop_bool(\"evaluate_aux_on_test\", False)\n",
    "    params.assert_empty('base train command')\n",
    "\n",
    "    try:\n",
    "        metrics = trainer.train()\n",
    "    except KeyboardInterrupt:\n",
    "        # if we have completed an epoch, try to create a model archive.\n",
    "        if os.path.exists(os.path.join(serialization_dir, _DEFAULT_WEIGHTS)):\n",
    "            logging.info(\"Training interrupted by the user. Attempting to create \"\n",
    "                         \"a model archive using the current best epoch weights.\")\n",
    "            archive_model(serialization_dir, files_to_archive=params.files_to_archive)\n",
    "        raise\n",
    "\n",
    "    # Now tar up results\n",
    "    archive_model(serialization_dir, files_to_archive=params.files_to_archive)\n",
    "\n",
    "    logger.info(\"Loading the best epoch weights.\")\n",
    "    best_model_state_path = os.path.join(serialization_dir, 'best.th')\n",
    "    best_model_state = torch.load(best_model_state_path)\n",
    "    best_model = model\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "\n",
    "    if test_data and evaluate_on_test:\n",
    "        logger.info(\"The model will be evaluated using the best epoch weights.\")\n",
    "        test_metrics = evaluate(\n",
    "                best_model, test_data, validation_iterator or iterator,\n",
    "                cuda_device=trainer._cuda_devices[0] # pylint: disable=protected-access\n",
    "        )\n",
    "        for key, value in test_metrics.items():\n",
    "            metrics[\"test_\" + key] = value\n",
    "\n",
    "    elif test_data:\n",
    "        logger.info(\"To evaluate on the test set after training, pass the \"\n",
    "                    \"'evaluate_on_test' flag, or use the 'allennlp evaluate' command.\")\n",
    "\n",
    "    if test_data_aux and evaluate_aux_on_test:\n",
    "        # for instance in test_data_aux:\n",
    "        #     instance.index_fields(vocab)\n",
    "        # for instance in test_data_aux2:\n",
    "        #     instance.index_fields(vocab)\n",
    "        test_metrics_aux = evaluate(best_model, test_data_aux, iterator_aux,\n",
    "                                    cuda_device=trainer._cuda_devices[0])  # pylint: disable=protected-access\n",
    "        test_metrics_aux2 = evaluate(best_model, test_data_aux2, iterator_aux2,\n",
    "                                     cuda_device=trainer._cuda_devices[0])  # pylint: disable=protected-access\n",
    "\n",
    "        for key, value in test_metrics_aux.items():\n",
    "            metrics[\"test_aux_\" + key] = value\n",
    "        for key, value in test_metrics_aux2.items():\n",
    "            metrics[\"test_aux2_\" + key] = value\n",
    "\n",
    "    elif test_data_aux:\n",
    "        logger.info(\"To evaluate on the auxiliary test set after training, pass the \"\n",
    "                    \"'evaluate_on_test' flag, or use the 'allennlp evaluate' command.\")\n",
    "\n",
    "    dump_metrics(os.path.join(serialization_dir, \"metrics.json\"), metrics, log=True)\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544d4fc-0fe7-42a9-b3da-b9e4cdcc8fcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MultiTaskTrainer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "495713dd-4a99-410e-b71a-d117b802d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module is an extended trainer based on the allennlp's default trainer to handle multitask training\n",
    "    for two auxiliary tasks\n",
    "\n",
    "A :class:`~allennlp.training.trainer.Trainer` is responsible for training a\n",
    ":class:`~allennlp.models.model.Model`.\n",
    "\n",
    "Typically you might create a configuration file specifying the model and\n",
    "training parameters and then use :mod:`~allennlp.commands.train`\n",
    "rather than instantiating a ``Trainer`` yourself.\n",
    "\"\"\"\n",
    "# pylint: disable=too-many-lines\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import traceback\n",
    "from typing import Dict, Optional, List, Tuple, Union, Iterable, Any, Set\n",
    "\n",
    "import torch\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.nn.parallel import replicate, parallel_apply\n",
    "from torch.nn.parallel.scatter_gather import scatter_kwargs, gather\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import peak_memory_mb, gpu_memory_mb, dump_metrics\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.learning_rate_schedulers import LearningRateScheduler\n",
    "from allennlp.training.optimizers import Optimizer\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "def is_sparse(tensor):\n",
    "    return tensor.is_sparse\n",
    "\n",
    "\n",
    "def sparse_clip_norm(parameters, max_norm, norm_type=2) -> float:\n",
    "    \"\"\"Clips gradient norm of an iterable of parameters.\n",
    "\n",
    "    The norm is computed over all gradients together, as if they were\n",
    "    concatenated into a single vector. Gradients are modified in-place.\n",
    "    Supports sparse gradients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : ``(Iterable[torch.Tensor])``\n",
    "        An iterable of Tensors that will have gradients normalized.\n",
    "    max_norm : ``float``\n",
    "        The max norm of the gradients.\n",
    "    norm_type : ``float``\n",
    "        The type of the used p-norm. Can be ``'inf'`` for infinity norm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Total norm of the parameters (viewed as a single vector).\n",
    "    \"\"\"\n",
    "    # pylint: disable=invalid-name,protected-access\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    max_norm = float(max_norm)\n",
    "    norm_type = float(norm_type)\n",
    "    if norm_type == float('inf'):\n",
    "        total_norm = max(p.grad.data.abs().max() for p in parameters)\n",
    "    else:\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            if is_sparse(p.grad):\n",
    "                # need to coalesce the repeated indices before finding norm\n",
    "                grad = p.grad.data.coalesce()\n",
    "                param_norm = grad._values().norm(norm_type)\n",
    "            else:\n",
    "                param_norm = p.grad.data.norm(norm_type)\n",
    "            total_norm += param_norm ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        for p in parameters:\n",
    "            if is_sparse(p.grad):\n",
    "                p.grad.data._values().mul_(clip_coef)\n",
    "            else:\n",
    "                p.grad.data.mul_(clip_coef)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def move_optimizer_to_cuda(optimizer):\n",
    "    \"\"\"\n",
    "    Move the optimizer state to GPU, if necessary.\n",
    "    After calling, any parameter specific state in the optimizer\n",
    "    will be located on the same device as the parameter.\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        for param in param_group['params']:\n",
    "            if param.is_cuda:\n",
    "                param_state = optimizer.state[param]\n",
    "                for k in param_state.keys():\n",
    "                    if isinstance(param_state[k], torch.Tensor):\n",
    "                        param_state[k] = param_state[k].cuda(device=param.get_device())\n",
    "\n",
    "\n",
    "class TensorboardWriter:\n",
    "    \"\"\"\n",
    "    Wraps a pair of ``SummaryWriter`` instances but is a no-op if they're ``None``.\n",
    "    Allows Tensorboard logging without always checking for Nones first.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_log: SummaryWriter = None, validation_log: SummaryWriter = None) -> None:\n",
    "        self._train_log = train_log\n",
    "        self._validation_log = validation_log\n",
    "\n",
    "    @staticmethod\n",
    "    def _item(value: Any):\n",
    "        if hasattr(value, 'item'):\n",
    "            val = value.item()\n",
    "        else:\n",
    "            val = value\n",
    "        return val\n",
    "\n",
    "    def add_train_scalar(self, name: str, value: float, global_step: int) -> None:\n",
    "        # get the scalar\n",
    "        if self._train_log is not None:\n",
    "            self._train_log.add_scalar(name, self._item(value), global_step)\n",
    "\n",
    "    def add_train_histogram(self, name: str, values: torch.Tensor, global_step: int) -> None:\n",
    "        if self._train_log is not None:\n",
    "            if isinstance(values, torch.Tensor):\n",
    "                values_to_write = values.cpu().data.numpy().flatten()\n",
    "                self._train_log.add_histogram(name, values_to_write, global_step)\n",
    "\n",
    "    def add_validation_scalar(self, name: str, value: float, global_step: int) -> None:\n",
    "\n",
    "        if self._validation_log is not None:\n",
    "            self._validation_log.add_scalar(name, self._item(value), global_step)\n",
    "\n",
    "\n",
    "def time_to_str(timestamp: int) -> str:\n",
    "    \"\"\"\n",
    "    Convert seconds past Epoch to human readable string.\n",
    "    \"\"\"\n",
    "    datetimestamp = datetime.datetime.fromtimestamp(timestamp)\n",
    "    return '{:04d}-{:02d}-{:02d}-{:02d}-{:02d}-{:02d}'.format(\n",
    "            datetimestamp.year, datetimestamp.month, datetimestamp.day,\n",
    "            datetimestamp.hour, datetimestamp.minute, datetimestamp.second\n",
    "    )\n",
    "\n",
    "\n",
    "def str_to_time(time_str: str) -> datetime.datetime:\n",
    "    \"\"\"\n",
    "    Convert human readable string to datetime.datetime.\n",
    "    \"\"\"\n",
    "    pieces: Any = [int(piece) for piece in time_str.split('-')]\n",
    "    return datetime.datetime(*pieces)\n",
    "\n",
    "\n",
    "class MultiTaskTrainer2:\n",
    "    def __init__(self,\n",
    "                 model: Model,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 iterator: DataIterator,\n",
    "                 train_dataset: Iterable[Instance],\n",
    "                 train_dataset_aux: Iterable[Instance],\n",
    "                 train_dataset_aux2: Optional[Iterable[Instance]],\n",
    "                 mixing_ratio: float = 0.17,\n",
    "                 mixing_ratio2: float = 0.17,\n",
    "                 cutoff_epoch: int = -1,\n",
    "                 validation_dataset: Optional[Iterable[Instance]] = None,\n",
    "                 validation_dataset_aux: Optional[Iterable] = None,\n",
    "                 validation_dataset_aux2: Optional[Iterable[Instance]] = None,\n",
    "                 patience: Optional[int] = None,\n",
    "                 validation_metric: str = \"-loss\",\n",
    "                 validation_iterator: DataIterator = None,\n",
    "                 shuffle: bool = True,\n",
    "                 num_epochs: int = 20,\n",
    "                 serialization_dir: Optional[str] = None,\n",
    "                 num_serialized_models_to_keep: int = 20,\n",
    "                 keep_serialized_model_every_num_seconds: int = None,\n",
    "                 model_save_interval: float = None,\n",
    "                 cuda_device: Union[int, List] = -1,\n",
    "                 grad_norm: Optional[float] = None,\n",
    "                 grad_clipping: Optional[float] = None,\n",
    "                 learning_rate_scheduler: Optional[LearningRateScheduler] = None,\n",
    "                 summary_interval: int = 100,\n",
    "                 histogram_interval: int = None,\n",
    "                 should_log_parameter_statistics: bool = True,\n",
    "                 should_log_learning_rate: bool = False,\n",
    "                 iterator_aux: Optional[DataIterator] = None,\n",
    "                 iterator_aux2: Optional[DataIterator] = None) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : ``Model``, required.\n",
    "            An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n",
    "            their ``forward`` method returns a dictionary with a \"loss\" key, containing a\n",
    "            scalar tensor representing the loss function to be optimized.\n",
    "        optimizer : ``torch.nn.Optimizer``, required.\n",
    "            An instance of a Pytorch Optimizer, instantiated with the parameters of the\n",
    "            model to be optimized.\n",
    "        iterator : ``DataIterator``, required.\n",
    "            A method for iterating over a ``Dataset``, yielding padded indexed batches.\n",
    "        train_dataset : ``Dataset``, required.\n",
    "            A ``Dataset`` to train on. The dataset should have already been indexed.\n",
    "        train_dataset_aux : ``Dataset``, required.\n",
    "            A ``Dataset`` for auxiliary task 1 to train on.\n",
    "        train_dataset_aux2 : ``Dataset``, required.\n",
    "            A ``Dataset`` for second auxiliary task to train on. The dataset should have already been indexed.\n",
    "        mixing_ratio: a float specifying the influence of the first auxiliary task on the final loss\n",
    "        mixing_ratio2: a float specifying the influence of the second auxiliary task on the final loss\n",
    "        cutoff_epoch: multitask training starts from the epoch after the epoch specified by cutoff_epoch\n",
    "        validation_dataset : ``Dataset``, optional, (default = None).\n",
    "            A ``Dataset`` to evaluate on. The dataset should have already been indexed.\n",
    "        validation_dataset_aux : a validation dataset for the first auxiliary task\n",
    "        validation_dataset_aux_2 : a validation dataset for the second auxiliary task\n",
    "        patience : Optional[int] > 0, optional (default=None)\n",
    "            Number of epochs to be patient before early stopping: the training is stopped\n",
    "            after ``patience`` epochs with no improvement. If given, it must be ``> 0``.\n",
    "            If None, early stopping is disabled.\n",
    "        validation_metric : str, optional (default=\"loss\")\n",
    "            Validation metric to measure for whether to stop training using patience\n",
    "            and whether to serialize an ``is_best`` model each epoch. The metric name\n",
    "            must be prepended with either \"+\" or \"-\", which specifies whether the metric\n",
    "            is an increasing or decreasing function.\n",
    "        validation_iterator : ``DataIterator``, optional (default=None)\n",
    "            An iterator to use for the validation set.  If ``None``, then\n",
    "            use the training `iterator`.\n",
    "        shuffle: ``bool``, optional (default=True)\n",
    "            Whether to shuffle the instances in the iterator or not.\n",
    "        num_epochs : int, optional (default = 20)\n",
    "            Number of training epochs.\n",
    "        serialization_dir : str, optional (default=None)\n",
    "            Path to directory for saving and loading model files. Models will not be saved if\n",
    "            this parameter is not passed.\n",
    "        num_serialized_models_to_keep : ``int``, optional (default=20)\n",
    "            Number of previous model checkpoints to retain.  Default is to keep 20 checkpoints.\n",
    "            A value of None or -1 means all checkpoints will be kept.\n",
    "        keep_serialized_model_every_num_seconds : ``int``, optional (default=None)\n",
    "            If num_serialized_models_to_keep is not None, then occasionally it's useful to\n",
    "            save models at a given interval in addition to the last num_serialized_models_to_keep.\n",
    "            To do so, specify keep_serialized_model_every_num_seconds as the number of seconds\n",
    "            between permanently saved checkpoints.  Note that this option is only used if\n",
    "            num_serialized_models_to_keep is not None, otherwise all checkpoints are kept.\n",
    "        model_save_interval : ``float``, optional (default=None)\n",
    "            If provided, then serialize models every ``model_save_interval``\n",
    "            seconds within single epochs.  In all cases, models are also saved\n",
    "            at the end of every epoch if ``serialization_dir`` is provided.\n",
    "        cuda_device : ``int``, optional (default = -1)\n",
    "            An integer specifying the CUDA device to use. If -1, the CPU is used.\n",
    "        grad_norm : ``float``, optional, (default = None).\n",
    "            If provided, gradient norms will be rescaled to have a maximum of this value.\n",
    "        grad_clipping : ``float``, optional (default = ``None``).\n",
    "            If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n",
    "            maximum of this value.  If you are getting ``NaNs`` in your gradients during training\n",
    "            that are not solved by using ``grad_norm``, you may need this.\n",
    "        learning_rate_scheduler : ``PytorchLRScheduler``, optional, (default = None)\n",
    "            A Pytorch learning rate scheduler. The learning rate will be decayed with respect to\n",
    "            this schedule at the end of each epoch. If you use\n",
    "            :class:`torch.optim.lr_scheduler.ReduceLROnPlateau`, this will use the ``validation_metric``\n",
    "            provided to determine if learning has plateaued.  To support updating the learning\n",
    "            rate on every batch, this can optionally implement ``step_batch(batch_num_total)`` which\n",
    "            updates the learning rate given the batch number.\n",
    "        summary_interval: ``int``, optional, (default = 100)\n",
    "            Number of batches between logging scalars to tensorboard\n",
    "        histogram_interval : ``int``, optional, (default = ``None``)\n",
    "            If not None, then log histograms to tensorboard every ``histogram_interval`` batches.\n",
    "            When this parameter is specified, the following additional logging is enabled:\n",
    "                * Histograms of model parameters\n",
    "                * The ratio of parameter update norm to parameter norm\n",
    "                * Histogram of layer activations\n",
    "            We log histograms of the parameters returned by\n",
    "            ``model.get_parameters_for_histogram_tensorboard_logging``.\n",
    "            The layer activations are logged for any modules in the ``Model`` that have\n",
    "            the attribute ``should_log_activations`` set to ``True``.  Logging\n",
    "            histograms requires a number of GPU-CPU copies during training and is typically\n",
    "            slow, so we recommend logging histograms relatively infrequently.\n",
    "            Note: only Modules that return tensors, tuples of tensors or dicts\n",
    "            with tensors as values currently support activation logging.\n",
    "        should_log_parameter_statistics : ``bool``, optional, (default = True)\n",
    "            Whether to send parameter statistics (mean and standard deviation\n",
    "            of parameters and gradients) to tensorboard.\n",
    "        should_log_learning_rate : ``bool``, optional, (default = False)\n",
    "            Whether to send parameter specific learning rate to tensorboard.\n",
    "        iterator_aux : ``DataIterator``, required.\n",
    "            A method for iterating over a ``Dataset`` for the first auxiliary task, yielding padded indexed batches.\n",
    "        iterator_aux2 : ``DataIterator``, required.\n",
    "            A method for iterating over a ``Dataset`` for the second auxiliary task, yielding padded indexed batches.\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        self._iterator = iterator\n",
    "        self._validation_iterator = validation_iterator\n",
    "        self._shuffle = shuffle\n",
    "        self._optimizer = optimizer\n",
    "        self._train_data = train_dataset\n",
    "        self._validation_data = validation_dataset\n",
    "        self._train_dataset_aux = train_dataset_aux\n",
    "        self._train_dataset_aux2 = train_dataset_aux2\n",
    "        self._validation_data_aux = validation_dataset_aux\n",
    "        self._validation_data_aux2 = validation_dataset_aux2\n",
    "\n",
    "        self._cutoff_epoch = cutoff_epoch\n",
    "        self._mixing_ratio = mixing_ratio\n",
    "        self._mixing_ratio2 = mixing_ratio2\n",
    "        self._iterator_aux = iterator_aux\n",
    "        self._iterator_aux2 = iterator_aux2\n",
    "\n",
    "        if patience is None:  # no early stopping\n",
    "            if validation_dataset:\n",
    "                logger.warning('You provided a validation dataset but patience was set to None, '\n",
    "                               'meaning that early stopping is disabled')\n",
    "        elif (not isinstance(patience, int)) or patience <= 0:\n",
    "            raise ConfigurationError('{} is an invalid value for \"patience\": it must be a positive integer '\n",
    "                                     'or None (if you want to disable early stopping)'.format(patience))\n",
    "        self._patience = patience\n",
    "        self._num_epochs = num_epochs\n",
    "\n",
    "        self._serialization_dir = serialization_dir\n",
    "        self._num_serialized_models_to_keep = num_serialized_models_to_keep\n",
    "        self._keep_serialized_model_every_num_seconds = keep_serialized_model_every_num_seconds\n",
    "        self._serialized_paths: List[Any] = []\n",
    "        self._last_permanent_saved_checkpoint_time = time.time()\n",
    "        self._model_save_interval = model_save_interval\n",
    "\n",
    "        self._grad_norm = grad_norm\n",
    "        self._grad_clipping = grad_clipping\n",
    "        self._learning_rate_scheduler = learning_rate_scheduler\n",
    "\n",
    "        increase_or_decrease = validation_metric[0]\n",
    "        if increase_or_decrease not in [\"+\", \"-\"]:\n",
    "            raise ConfigurationError(\"Validation metrics must specify whether they should increase \"\n",
    "                                     \"or decrease by pre-pending the metric name with a +/-.\")\n",
    "        self._validation_metric = validation_metric[1:]\n",
    "        self._validation_metric_decreases = increase_or_decrease == \"-\"\n",
    "\n",
    "        if not isinstance(cuda_device, int) and not isinstance(cuda_device, list):\n",
    "            raise ConfigurationError(\"Expected an int or list for cuda_device, got {}\".format(cuda_device))\n",
    "\n",
    "        if isinstance(cuda_device, list):\n",
    "            logger.warning(f\"Multiple GPU support is experimental not recommended for use. \"\n",
    "                           \"In some cases it may lead to incorrect results or undefined behavior.\")\n",
    "            self._multiple_gpu = True\n",
    "            self._cuda_devices = cuda_device\n",
    "        else:\n",
    "            self._multiple_gpu = False\n",
    "            self._cuda_devices = [cuda_device]\n",
    "\n",
    "        if self._cuda_devices[0] != -1:\n",
    "            self._model = self._model.cuda(self._cuda_devices[0])\n",
    "\n",
    "        self._cuda_device = self._cuda_devices[0]\n",
    "\n",
    "        self._log_interval = 10  # seconds\n",
    "        self._summary_interval = summary_interval\n",
    "        self._histogram_interval = histogram_interval\n",
    "        self._log_histograms_this_batch = False\n",
    "        self._should_log_parameter_statistics = should_log_parameter_statistics\n",
    "        self._should_log_learning_rate = should_log_learning_rate\n",
    "\n",
    "        # We keep the total batch number as a class variable because it\n",
    "        # is used inside a closure for the hook which logs activations in\n",
    "        # ``_enable_activation_logging``.\n",
    "        self._batch_num_total = 0\n",
    "\n",
    "        self._last_log = 0.0  # time of last logging\n",
    "\n",
    "        if serialization_dir is not None:\n",
    "            train_log = SummaryWriter(os.path.join(serialization_dir, \"log\", \"train\"))\n",
    "            validation_log = SummaryWriter(os.path.join(serialization_dir, \"log\", \"validation\"))\n",
    "            self._tensorboard = TensorboardWriter(train_log, validation_log)\n",
    "        else:\n",
    "            self._tensorboard = TensorboardWriter()\n",
    "        self._warned_tqdm_ignores_underscores = False\n",
    "\n",
    "    def _enable_gradient_clipping(self) -> None:\n",
    "        if self._grad_clipping is not None:\n",
    "            # Pylint is unable to tell that we're in the case that _grad_clipping is not None...\n",
    "            # pylint: disable=invalid-unary-operand-type\n",
    "            clip_function = lambda grad: grad.clamp(-self._grad_clipping, self._grad_clipping)\n",
    "            for parameter in self._model.parameters():\n",
    "                if parameter.requires_grad:\n",
    "                    parameter.register_hook(clip_function)\n",
    "\n",
    "    def _enable_activation_logging(self) -> None:\n",
    "        \"\"\"\n",
    "        Log activations to tensorboard\n",
    "        \"\"\"\n",
    "        if self._histogram_interval is not None:\n",
    "            # To log activation histograms to the forward pass, we register\n",
    "            # a hook on forward to capture the output tensors.\n",
    "            # This uses a closure on self._log_histograms_this_batch to\n",
    "            # determine whether to send the activations to tensorboard,\n",
    "            # since we don't want them on every call.\n",
    "            for _, module in self._model.named_modules():\n",
    "                if not getattr(module, 'should_log_activations', False):\n",
    "                    # skip it\n",
    "                    continue\n",
    "\n",
    "                def hook(module_, inputs, outputs):\n",
    "                    # pylint: disable=unused-argument,cell-var-from-loop\n",
    "                    log_prefix = 'activation_histogram/{0}'.format(module_.__class__)\n",
    "                    if self._log_histograms_this_batch:\n",
    "                        if isinstance(outputs, torch.Tensor):\n",
    "                            log_name = log_prefix\n",
    "                            self._tensorboard.add_train_histogram(log_name,\n",
    "                                                                  outputs,\n",
    "                                                                  self._batch_num_total)\n",
    "                        elif isinstance(outputs, (list, tuple)):\n",
    "                            for i, output in enumerate(outputs):\n",
    "                                log_name = \"{0}_{1}\".format(log_prefix, i)\n",
    "                                self._tensorboard.add_train_histogram(log_name,\n",
    "                                                                      output,\n",
    "                                                                      self._batch_num_total)\n",
    "                        elif isinstance(outputs, dict):\n",
    "                            for k, tensor in outputs.items():\n",
    "                                log_name = \"{0}_{1}\".format(log_prefix, k)\n",
    "                                self._tensorboard.add_train_histogram(log_name,\n",
    "                                                                      tensor,\n",
    "                                                                      self._batch_num_total)\n",
    "                        else:\n",
    "                            # skip it\n",
    "                            pass\n",
    "\n",
    "                module.register_forward_hook(hook)\n",
    "\n",
    "    def _rescale_gradients(self) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n",
    "        \"\"\"\n",
    "        if self._grad_norm:\n",
    "            parameters_to_clip = [p for p in self._model.parameters()\n",
    "                                  if p.grad is not None]\n",
    "            return sparse_clip_norm(parameters_to_clip, self._grad_norm)\n",
    "        return None\n",
    "\n",
    "    def _data_parallel(self, batch):\n",
    "        \"\"\"\n",
    "        Do the forward pass using multiple GPUs.  This is a simplification\n",
    "        of torch.nn.parallel.data_parallel to support the allennlp model\n",
    "        interface.\n",
    "        \"\"\"\n",
    "        inputs, module_kwargs = scatter_kwargs((), batch, self._cuda_devices, 0)\n",
    "        used_device_ids = self._cuda_devices[:len(inputs)]\n",
    "        replicas = replicate(self._model, used_device_ids)\n",
    "        outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)\n",
    "\n",
    "        # Only the 'loss' is needed.\n",
    "        # a (num_gpu, ) tensor with loss on each GPU\n",
    "        losses = gather([output['loss'].unsqueeze(0) for output in outputs], used_device_ids[0], 0)\n",
    "        return {'loss': losses.mean()}\n",
    "\n",
    "    def _batch_loss(self, batch: torch.Tensor,\n",
    "                    for_training: bool,\n",
    "                    batch_aux: torch.Tensor=None,\n",
    "                    batch_aux2: torch.Tensor=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Does a forward pass on the given batch and auxiliary data batches and returns the ``loss`` value in the result.\n",
    "        If ``for_training`` is `True` also applies regularization penalty.\n",
    "        \"\"\"\n",
    "        if self._multiple_gpu:\n",
    "            output_dict = self._data_parallel(batch)\n",
    "            if batch_aux is not None:\n",
    "                raise ConfigurationError('multi-gpu not supported for multi-task training.')\n",
    "        else:\n",
    "            batch = util.move_to_device(batch, self._cuda_devices[0])\n",
    "            print(batch)\n",
    "            output_dict = self._model(**batch)\n",
    "\n",
    "        try:\n",
    "            loss = output_dict[\"loss\"]\n",
    "            if for_training:\n",
    "                loss += self._model.get_regularization_penalty()\n",
    "        except KeyError:\n",
    "            if for_training:\n",
    "                raise RuntimeError(\"The model you are trying to optimize does not contain a\"\n",
    "                                   \" 'loss' key in the output of model.forward(inputs).\")\n",
    "            loss = None\n",
    "\n",
    "        if batch_aux is not None and batch_aux2 is not None:\n",
    "            batch_aux = util.move_to_device(batch_aux, self._cuda_devices[0])\n",
    "            batch_aux2 = util.move_to_device(batch_aux2, self._cuda_devices[0])\n",
    "            output_dict_aux = self._model(**batch_aux)\n",
    "            output_dict_aux2 = self._model(**batch_aux2)\n",
    "            try:\n",
    "                loss_aux = output_dict_aux[\"loss\"]\n",
    "                loss_aux2 = output_dict_aux2[\"loss\"]\n",
    "                if for_training:\n",
    "                    loss_aux += self._model.get_regularization_penalty()\n",
    "                    loss_aux2 += self._model.get_regularization_penalty()\n",
    "            except KeyError:\n",
    "                raise ConfigurationError(\"The auxiliary model you are trying to optimize does not contain a\"\n",
    "                                         \" 'loss' key in the output of model.forward(inputs).\")\n",
    "\n",
    "            # multi-task loss\n",
    "            loss = loss + self._mixing_ratio * loss_aux + self._mixing_ratio2 * loss_aux2\n",
    "        return loss\n",
    "\n",
    "    def _get_metrics(self, total_loss: float, num_batches: int, reset: bool = False) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Gets the metrics but sets ``\"loss\"`` to\n",
    "        the total loss divided by the ``num_batches`` so that\n",
    "        the ``\"loss\"`` metric is \"average loss per batch\".\n",
    "        \"\"\"\n",
    "        metrics = self._model.get_metrics(reset=reset)\n",
    "        metrics[\"loss\"] = float(total_loss / num_batches) if num_batches > 0 else 0.0\n",
    "        return metrics\n",
    "\n",
    "    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics.\n",
    "        \"\"\"\n",
    "        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_memory_mb()}\")\n",
    "        for gpu, memory in gpu_memory_mb().items():\n",
    "            logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "        # Set the model to \"train\" mode.\n",
    "        self._model.train()\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self._iterator(self._train_data,\n",
    "                                         num_epochs=1,\n",
    "                                         shuffle=self._shuffle)\n",
    "        train_generator_aux = self._iterator_aux(self._train_dataset_aux,\n",
    "                                                 num_epochs=1,\n",
    "                                                 shuffle=self._shuffle)\n",
    "        train_generator_aux2 = self._iterator_aux2(self._train_dataset_aux2,\n",
    "                                                  num_epochs=1,\n",
    "                                                  shuffle=self._shuffle)\n",
    "\n",
    "        multitask_training = False\n",
    "        if epoch > self._cutoff_epoch:\n",
    "            multitask_training = True\n",
    "            logger.info(\"Multitask Training\")\n",
    "        else:\n",
    "            logger.info(\"Training\")\n",
    "\n",
    "        num_training_batches = self._iterator.get_num_batches(self._train_data)\n",
    "        num_training_batches_aux = self._iterator_aux.get_num_batches(self._train_dataset_aux)\n",
    "        num_training_batches_aux2 = self._iterator_aux2.get_num_batches(self._train_dataset_aux2)\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        if self._histogram_interval is not None:\n",
    "            histogram_parameters = set(self._model.get_parameters_for_histogram_tensorboard_logging())\n",
    "\n",
    "        logger.info(\"Training\")\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches)\n",
    "        # train_aux_generator_tqdm = Tqdm.tqdm(train_generator_aux,\n",
    "        #                                      total=num_training_batches_aux)\n",
    "        for batch, batch_aux, batch_aux2 in zip(train_generator_tqdm, train_generator_aux, train_generator_aux2):\n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "\n",
    "            self._log_histograms_this_batch = self._histogram_interval is not None and (\n",
    "                    batch_num_total % self._histogram_interval == 0)\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "\n",
    "            if multitask_training:\n",
    "                loss = self._batch_loss(batch,\n",
    "                                        for_training=True,\n",
    "                                        batch_aux=batch_aux,\n",
    "                                        batch_aux2=batch_aux2)\n",
    "            else:\n",
    "                loss = self._batch_loss(batch, for_training=True)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            batch_grad_norm = self._rescale_gradients()\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                # get the magnitude of parameter updates for logging\n",
    "                # We need a copy of current parameters to compute magnitude of updates,\n",
    "                # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "                param_updates = {name: param.detach().cpu().clone()\n",
    "                                 for name, param in self._model.named_parameters()}\n",
    "                self._optimizer.step()\n",
    "                for name, param in self._model.named_parameters():\n",
    "                    param_updates[name].sub_(param.detach().cpu())\n",
    "                    update_norm = torch.norm(param_updates[name].view(-1, ))\n",
    "                    param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                    self._tensorboard.add_train_scalar(\"gradient_update/\" + name,\n",
    "                                                       update_norm / (param_norm + 1e-7),\n",
    "                                                       batch_num_total)\n",
    "            else:\n",
    "                self._optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "            # Log parameter values to Tensorboard\n",
    "            if batch_num_total % self._summary_interval == 0:\n",
    "                if self._should_log_parameter_statistics:\n",
    "                    self._parameter_and_gradient_statistics_to_tensorboard(batch_num_total, batch_grad_norm)\n",
    "                if self._should_log_learning_rate:\n",
    "                    self._learning_rates_to_tensorboard(batch_num_total)\n",
    "                self._tensorboard.add_train_scalar(\"loss/loss_train\", metrics[\"loss\"], batch_num_total)\n",
    "                self._metrics_to_tensorboard(batch_num_total,\n",
    "                                             {\"epoch_metrics/\" + k: v for k, v in metrics.items()})\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                self._histograms_to_tensorboard(batch_num_total, histogram_parameters)\n",
    "\n",
    "            # Save model if needed.\n",
    "            if self._model_save_interval is not None and (\n",
    "                    time.time() - last_save_time > self._model_save_interval\n",
    "            ):\n",
    "                last_save_time = time.time()\n",
    "                self._save_checkpoint(\n",
    "                        '{0}.{1}'.format(epoch, time_to_str(int(last_save_time))), [], is_best=False\n",
    "                )\n",
    "\n",
    "        return self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "\n",
    "    def _should_stop_early(self, metric_history: List[float]) -> bool:\n",
    "        \"\"\"\n",
    "        uses patience and the validation metric to determine if training should stop early\n",
    "        \"\"\"\n",
    "        if self._patience and self._patience < len(metric_history):\n",
    "            # Pylint can't figure out that in this branch `self._patience` is an int.\n",
    "            # pylint: disable=invalid-unary-operand-type\n",
    "\n",
    "            # Is the best score in the past N epochs worse than or equal the best score overall?\n",
    "            if self._validation_metric_decreases:\n",
    "                return min(metric_history[-self._patience:]) >= min(metric_history[:-self._patience])\n",
    "            else:\n",
    "                return max(metric_history[-self._patience:]) <= max(metric_history[:-self._patience])\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _parameter_and_gradient_statistics_to_tensorboard(self, # pylint: disable=invalid-name\n",
    "                                                          epoch: int,\n",
    "                                                          batch_grad_norm: float) -> None:\n",
    "        \"\"\"\n",
    "        Send the mean and std of all parameters and gradients to tensorboard, as well\n",
    "        as logging the average gradient norm.\n",
    "        \"\"\"\n",
    "        # Log parameter values to Tensorboard\n",
    "        for name, param in self._model.named_parameters():\n",
    "            self._tensorboard.add_train_scalar(\"parameter_mean/\" + name,\n",
    "                                               param.data.mean(),\n",
    "                                               epoch)\n",
    "            self._tensorboard.add_train_scalar(\"parameter_std/\" + name, param.data.std(), epoch)\n",
    "            if param.grad is not None:\n",
    "                if is_sparse(param.grad):\n",
    "                    # pylint: disable=protected-access\n",
    "                    grad_data = param.grad.data._values()\n",
    "                else:\n",
    "                    grad_data = param.grad.data\n",
    "\n",
    "                # skip empty gradients\n",
    "                if torch.prod(torch.tensor(grad_data.shape)).item() > 0: # pylint: disable=not-callable\n",
    "                    self._tensorboard.add_train_scalar(\"gradient_mean/\" + name,\n",
    "                                                       grad_data.mean(),\n",
    "                                                       epoch)\n",
    "                    self._tensorboard.add_train_scalar(\"gradient_std/\" + name,\n",
    "                                                       grad_data.std(),\n",
    "                                                       epoch)\n",
    "                else:\n",
    "                    # no gradient for a parameter with sparse gradients\n",
    "                    logger.info(\"No gradient for %s, skipping tensorboard logging.\", name)\n",
    "        # norm of gradients\n",
    "        if batch_grad_norm is not None:\n",
    "            self._tensorboard.add_train_scalar(\"gradient_norm\",\n",
    "                                               batch_grad_norm,\n",
    "                                               epoch)\n",
    "\n",
    "    def _learning_rates_to_tensorboard(self, batch_num_total: int):\n",
    "        \"\"\"\n",
    "        Send current parameter specific learning rates to tensorboard\n",
    "        \"\"\"\n",
    "        # optimizer stores lr info keyed by parameter tensor\n",
    "        # we want to log with parameter name\n",
    "        names = {param: name for name, param in self._model.named_parameters()}\n",
    "        for group in self._optimizer.param_groups:\n",
    "            if 'lr' not in group:\n",
    "                continue\n",
    "            rate = group['lr']\n",
    "            for param in group['params']:\n",
    "                # check whether params has requires grad or not\n",
    "                effective_rate = rate * float(param.requires_grad)\n",
    "                self._tensorboard.add_train_scalar(\n",
    "                        \"learning_rate/\" + names[param],\n",
    "                        effective_rate,\n",
    "                        batch_num_total\n",
    "                )\n",
    "\n",
    "    def _histograms_to_tensorboard(self, epoch: int, histogram_parameters: Set[str]) -> None:\n",
    "        \"\"\"\n",
    "        Send histograms of parameters to tensorboard.\n",
    "        \"\"\"\n",
    "        for name, param in self._model.named_parameters():\n",
    "            if name in histogram_parameters:\n",
    "                self._tensorboard.add_train_histogram(\"parameter_histogram/\" + name,\n",
    "                                                      param,\n",
    "                                                      epoch)\n",
    "\n",
    "    def _metrics_to_tensorboard(self,\n",
    "                                epoch: int,\n",
    "                                train_metrics: dict,\n",
    "                                val_metrics: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Sends all of the train metrics (and validation metrics, if provided) to tensorboard.\n",
    "        \"\"\"\n",
    "        metric_names = set(train_metrics.keys())\n",
    "        if val_metrics is not None:\n",
    "            metric_names.update(val_metrics.keys())\n",
    "        val_metrics = val_metrics or {}\n",
    "\n",
    "        for name in metric_names:\n",
    "            train_metric = train_metrics.get(name)\n",
    "            if train_metric is not None:\n",
    "                self._tensorboard.add_train_scalar(name, train_metric, epoch)\n",
    "            val_metric = val_metrics.get(name)\n",
    "            if val_metric is not None:\n",
    "                self._tensorboard.add_validation_scalar(name, val_metric, epoch)\n",
    "\n",
    "    def _metrics_to_console(self,  # pylint: disable=no-self-use\n",
    "                            train_metrics: dict,\n",
    "                            val_metrics: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Logs all of the train metrics (and validation metrics, if provided) to the console.\n",
    "        \"\"\"\n",
    "        val_metrics = val_metrics or {}\n",
    "        dual_message_template = \"%s |  %8.3f  |  %8.3f\"\n",
    "        no_val_message_template = \"%s |  %8.3f  |  %8s\"\n",
    "        no_train_message_template = \"%s |  %8s  |  %8.3f\"\n",
    "        header_template = \"%s |  %-10s\"\n",
    "\n",
    "        metric_names = set(train_metrics.keys())\n",
    "        if val_metrics:\n",
    "            metric_names.update(val_metrics.keys())\n",
    "\n",
    "        name_length = max([len(x) for x in metric_names])\n",
    "\n",
    "        logger.info(header_template, \"Training\".rjust(name_length + 13), \"Validation\")\n",
    "        for name in metric_names:\n",
    "            train_metric = train_metrics.get(name)\n",
    "            val_metric = val_metrics.get(name)\n",
    "\n",
    "            if val_metric is not None and train_metric is not None:\n",
    "                logger.info(dual_message_template, name.ljust(name_length), train_metric, val_metric)\n",
    "            elif val_metric is not None:\n",
    "                logger.info(no_train_message_template, name.ljust(name_length), \"N/A\", val_metric)\n",
    "            elif train_metric is not None:\n",
    "                logger.info(no_val_message_template, name.ljust(name_length), train_metric, \"N/A\")\n",
    "\n",
    "    def _validation_loss(self) -> Tuple[float, int]:\n",
    "        \"\"\"\n",
    "        Computes the validation loss. Returns it and the number of batches.\n",
    "        \"\"\"\n",
    "        logger.info(\"Validating\")\n",
    "\n",
    "        self._model.eval()\n",
    "\n",
    "        if self._validation_iterator is not None:\n",
    "            val_iterator = self._validation_iterator\n",
    "        else:\n",
    "            val_iterator = self._iterator\n",
    "\n",
    "        val_generator = val_iterator(self._validation_data,\n",
    "                                     num_epochs=1,\n",
    "                                     shuffle=False)\n",
    "        num_validation_batches = val_iterator.get_num_batches(self._validation_data)\n",
    "        val_generator_tqdm = Tqdm.tqdm(val_generator,\n",
    "                                       total=num_validation_batches)\n",
    "        batches_this_epoch = 0\n",
    "        val_loss = 0\n",
    "        for batch in val_generator_tqdm:\n",
    "\n",
    "            loss = self._batch_loss(batch, for_training=False)\n",
    "            if loss is not None:\n",
    "                # You shouldn't necessarily have to compute a loss for validation, so we allow for\n",
    "                # `loss` to be None.  We need to be careful, though - `batches_this_epoch` is\n",
    "                # currently only used as the divisor for the loss function, so we can safely only\n",
    "                # count those batches for which we actually have a loss.  If this variable ever\n",
    "                # gets used for something else, we might need to change things around a bit.\n",
    "                batches_this_epoch += 1\n",
    "                val_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            val_metrics = self._get_metrics(val_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(val_metrics)\n",
    "            val_generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "        return val_loss, batches_this_epoch\n",
    "\n",
    "    def train(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Trains the supplied model with the supplied parameters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            epoch_counter, validation_metric_per_epoch = self._restore_checkpoint()\n",
    "        except RuntimeError:\n",
    "            traceback.print_exc()\n",
    "            raise ConfigurationError(\"Could not recover training from the checkpoint.  Did you mean to output to \"\n",
    "                                     \"a different serialization directory or delete the existing serialization \"\n",
    "                                     \"directory?\")\n",
    "\n",
    "        self._enable_gradient_clipping()\n",
    "        self._enable_activation_logging()\n",
    "\n",
    "        logger.info(\"Beginning training.\")\n",
    "\n",
    "        train_metrics: Dict[str, float] = {}\n",
    "        val_metrics: Dict[str, float] = {}\n",
    "        metrics: Dict[str, Any] = {}\n",
    "        epochs_trained = 0\n",
    "        training_start_time = time.time()\n",
    "\n",
    "        for epoch in range(epoch_counter, self._num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            train_metrics = self._train_epoch(epoch)\n",
    "\n",
    "            if self._validation_data is not None:\n",
    "                with torch.no_grad():\n",
    "                    # We have a validation set, so compute all the metrics on it.\n",
    "                    val_loss, num_batches = self._validation_loss()\n",
    "                    val_metrics = self._get_metrics(val_loss, num_batches, reset=True)\n",
    "\n",
    "                    # Check validation metric for early stopping\n",
    "                    this_epoch_val_metric = val_metrics[self._validation_metric]\n",
    "\n",
    "                    # Check validation metric to see if it's the best so far\n",
    "                    is_best_so_far = self._is_best_so_far(this_epoch_val_metric, validation_metric_per_epoch)\n",
    "                    validation_metric_per_epoch.append(this_epoch_val_metric)\n",
    "                    if self._should_stop_early(validation_metric_per_epoch):\n",
    "                        logger.info(\"Ran out of patience.  Stopping training.\")\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                # No validation set, so just assume it's the best so far.\n",
    "                is_best_so_far = True\n",
    "                val_metrics = {}\n",
    "                this_epoch_val_metric = None\n",
    "\n",
    "            self._metrics_to_tensorboard(epoch, train_metrics, val_metrics=val_metrics)\n",
    "            self._metrics_to_console(train_metrics, val_metrics)\n",
    "\n",
    "            # Create overall metrics dict\n",
    "            training_elapsed_time = time.time() - training_start_time\n",
    "            metrics[\"training_duration\"] = time.strftime(\"%H:%M:%S\", time.gmtime(training_elapsed_time))\n",
    "            metrics[\"training_start_epoch\"] = epoch_counter\n",
    "            metrics[\"training_epochs\"] = epochs_trained\n",
    "            metrics[\"epoch\"] = epoch\n",
    "\n",
    "            for key, value in train_metrics.items():\n",
    "                metrics[\"training_\" + key] = value\n",
    "            for key, value in val_metrics.items():\n",
    "                metrics[\"validation_\" + key] = value\n",
    "\n",
    "            if is_best_so_far:\n",
    "                # Update all the best_ metrics.\n",
    "                # (Otherwise they just stay the same as they were.)\n",
    "                metrics['best_epoch'] = epoch\n",
    "                for key, value in val_metrics.items():\n",
    "                    metrics[\"best_validation_\" + key] = value\n",
    "\n",
    "            if self._serialization_dir:\n",
    "                dump_metrics(os.path.join(self._serialization_dir, f'metrics_epoch_{epoch}.json'), metrics)\n",
    "\n",
    "            if self._learning_rate_scheduler:\n",
    "                # The LRScheduler API is agnostic to whether your schedule requires a validation metric -\n",
    "                # if it doesn't, the validation metric passed here is ignored.\n",
    "                self._learning_rate_scheduler.step(this_epoch_val_metric, epoch)\n",
    "\n",
    "            self._save_checkpoint(epoch, validation_metric_per_epoch, is_best=is_best_so_far)\n",
    "\n",
    "            epoch_elapsed_time = time.time() - epoch_start_time\n",
    "            logger.info(\"Epoch duration: %s\", time.strftime(\"%H:%M:%S\", time.gmtime(epoch_elapsed_time)))\n",
    "\n",
    "            if epoch < self._num_epochs - 1:\n",
    "                training_elapsed_time = time.time() - training_start_time\n",
    "                estimated_time_remaining = training_elapsed_time * \\\n",
    "                    ((self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1)\n",
    "                formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n",
    "                logger.info(\"Estimated training time remaining: %s\", formatted_time)\n",
    "\n",
    "            epochs_trained += 1\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _is_best_so_far(self,\n",
    "                        this_epoch_val_metric: float,\n",
    "                        validation_metric_per_epoch: List[float]):\n",
    "        if not validation_metric_per_epoch:\n",
    "            return True\n",
    "        elif self._validation_metric_decreases:\n",
    "            return this_epoch_val_metric < min(validation_metric_per_epoch)\n",
    "        else:\n",
    "            return this_epoch_val_metric > max(validation_metric_per_epoch)\n",
    "\n",
    "    def _description_from_metrics(self, metrics: Dict[str, float]) -> str:\n",
    "        if (not self._warned_tqdm_ignores_underscores and\n",
    "                    any(metric_name.startswith(\"_\") for metric_name in metrics)):\n",
    "            logger.warning(\"Metrics with names beginning with \\\"_\\\" will \"\n",
    "                           \"not be logged to the tqdm progress bar.\")\n",
    "            self._warned_tqdm_ignores_underscores = True\n",
    "        return ', '.join([\"%s: %.4f\" % (name, value) for name, value in\n",
    "                          metrics.items() if not name.startswith(\"_\")]) + \" ||\"\n",
    "\n",
    "    def _save_checkpoint(self,\n",
    "                         epoch: Union[int, str],\n",
    "                         val_metric_per_epoch: List[float],\n",
    "                         is_best: Optional[bool] = None) -> None:\n",
    "        \"\"\"\n",
    "        Saves a checkpoint of the model to self._serialization_dir.\n",
    "        Is a no-op if self._serialization_dir is None.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : Union[int, str], required.\n",
    "            The epoch of training.  If the checkpoint is saved in the middle\n",
    "            of an epoch, the parameter is a string with the epoch and timestamp.\n",
    "        is_best: bool, optional (default = None)\n",
    "            A flag which causes the model weights at the given epoch to\n",
    "            be copied to a \"best.th\" file. The value of this flag should\n",
    "            be based on some validation metric computed by your model.\n",
    "        \"\"\"\n",
    "        if self._serialization_dir is not None:\n",
    "            model_path = os.path.join(self._serialization_dir, \"model_state_epoch_{}.th\".format(epoch))\n",
    "            model_state = self._model.state_dict()\n",
    "            torch.save(model_state, model_path)\n",
    "\n",
    "            training_state = {'epoch': epoch,\n",
    "                              'val_metric_per_epoch': val_metric_per_epoch,\n",
    "                              'optimizer': self._optimizer.state_dict(),\n",
    "                              'batch_num_total': self._batch_num_total}\n",
    "            if self._learning_rate_scheduler is not None:\n",
    "                training_state[\"learning_rate_scheduler\"] = \\\n",
    "                    self._learning_rate_scheduler.lr_scheduler.state_dict()\n",
    "            training_path = os.path.join(self._serialization_dir,\n",
    "                                         \"training_state_epoch_{}.th\".format(epoch))\n",
    "            torch.save(training_state, training_path)\n",
    "            if is_best:\n",
    "                logger.info(\"Best validation performance so far. \"\n",
    "                            \"Copying weights to '%s/best.th'.\", self._serialization_dir)\n",
    "                shutil.copyfile(model_path, os.path.join(self._serialization_dir, \"best.th\"))\n",
    "\n",
    "            if self._num_serialized_models_to_keep and self._num_serialized_models_to_keep >= 0:\n",
    "                self._serialized_paths.append([time.time(), model_path, training_path])\n",
    "                if len(self._serialized_paths) > self._num_serialized_models_to_keep:\n",
    "                    paths_to_remove = self._serialized_paths.pop(0)\n",
    "                    # Check to see if we should keep this checkpoint, if it has been longer\n",
    "                    # then self._keep_serialized_model_every_num_seconds since the last\n",
    "                    # kept checkpoint.\n",
    "                    remove_path = True\n",
    "                    if self._keep_serialized_model_every_num_seconds is not None:\n",
    "                        save_time = paths_to_remove[0]\n",
    "                        time_since_checkpoint_kept = save_time - self._last_permanent_saved_checkpoint_time\n",
    "                        if time_since_checkpoint_kept > self._keep_serialized_model_every_num_seconds:\n",
    "                            # We want to keep this checkpoint.\n",
    "                            remove_path = False\n",
    "                            self._last_permanent_saved_checkpoint_time = save_time\n",
    "                    if remove_path:\n",
    "                        for fname in paths_to_remove[1:]:\n",
    "                            os.remove(fname)\n",
    "\n",
    "    def find_latest_checkpoint(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Return the location of the latest model and training state files.\n",
    "        If there isn't a valid checkpoint then return None.\n",
    "        \"\"\"\n",
    "        have_checkpoint = (self._serialization_dir is not None and\n",
    "                           any(\"model_state_epoch_\" in x for x in os.listdir(self._serialization_dir)))\n",
    "\n",
    "        if not have_checkpoint:\n",
    "            return None\n",
    "\n",
    "        serialization_files = os.listdir(self._serialization_dir)\n",
    "        model_checkpoints = [x for x in serialization_files if \"model_state_epoch\" in x]\n",
    "        # Get the last checkpoint file.  Epochs are specified as either an\n",
    "        # int (for end of epoch files) or with epoch and timestamp for\n",
    "        # within epoch checkpoints, e.g. 5.2018-02-02-15-33-42\n",
    "        found_epochs = [\n",
    "                # pylint: disable=anomalous-backslash-in-string\n",
    "                re.search(\"model_state_epoch_([0-9\\.\\-]+)\\.th\", x).group(1)\n",
    "                for x in model_checkpoints\n",
    "        ]\n",
    "        int_epochs: Any = []\n",
    "        for epoch in found_epochs:\n",
    "            pieces = epoch.split('.')\n",
    "            if len(pieces) == 1:\n",
    "                # Just a single epoch without timestamp\n",
    "                int_epochs.append([int(pieces[0]), 0])\n",
    "            else:\n",
    "                # has a timestamp\n",
    "                int_epochs.append([int(pieces[0]), pieces[1]])\n",
    "        last_epoch = sorted(int_epochs, reverse=True)[0]\n",
    "        if last_epoch[1] == 0:\n",
    "            epoch_to_load = str(last_epoch[0])\n",
    "        else:\n",
    "            epoch_to_load = '{0}.{1}'.format(last_epoch[0], last_epoch[1])\n",
    "\n",
    "        model_path = os.path.join(self._serialization_dir,\n",
    "                                  \"model_state_epoch_{}.th\".format(epoch_to_load))\n",
    "        training_state_path = os.path.join(self._serialization_dir,\n",
    "                                           \"training_state_epoch_{}.th\".format(epoch_to_load))\n",
    "\n",
    "        return (model_path, training_state_path)\n",
    "\n",
    "    def _restore_checkpoint(self) -> Tuple[int, List[float]]:\n",
    "        \"\"\"\n",
    "        Restores a model from a serialization_dir to the last saved checkpoint.\n",
    "        This includes an epoch count and optimizer state, which is serialized separately\n",
    "        from  model parameters. This function should only be used to continue training -\n",
    "        if you wish to load a model for inference/load parts of a model into a new\n",
    "        computation graph, you should use the native Pytorch functions:\n",
    "        `` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))``\n",
    "\n",
    "        If ``self._serialization_dir`` does not exist or does not contain any checkpointed weights,\n",
    "        this function will do nothing and return 0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        epoch: int\n",
    "            The epoch at which to resume training, which should be one after the epoch\n",
    "            in the saved training state.\n",
    "        \"\"\"\n",
    "        latest_checkpoint = self.find_latest_checkpoint()\n",
    "\n",
    "        if latest_checkpoint is None:\n",
    "            # No checkpoint to restore, start at 0\n",
    "            return 0, []\n",
    "\n",
    "        model_path, training_state_path = latest_checkpoint\n",
    "\n",
    "        # Load the parameters onto CPU, then transfer to GPU.\n",
    "        # This avoids potential OOM on GPU for large models that\n",
    "        # load parameters onto GPU then make a new GPU copy into the parameter\n",
    "        # buffer. The GPU transfer happens implicitly in load_state_dict.\n",
    "        model_state = torch.load(model_path, map_location=util.device_mapping(-1))\n",
    "        training_state = torch.load(training_state_path, map_location=util.device_mapping(-1))\n",
    "        self._model.load_state_dict(model_state)\n",
    "        self._optimizer.load_state_dict(training_state[\"optimizer\"])\n",
    "        if self._learning_rate_scheduler is not None and \"learning_rate_scheduler\" in training_state:\n",
    "            self._learning_rate_scheduler.lr_scheduler.load_state_dict(\n",
    "                    training_state[\"learning_rate_scheduler\"])\n",
    "        move_optimizer_to_cuda(self._optimizer)\n",
    "\n",
    "        # We didn't used to save `validation_metric_per_epoch`, so we can't assume\n",
    "        # that it's part of the trainer state. If it's not there, an empty list is all\n",
    "        # we can do.\n",
    "        if \"val_metric_per_epoch\" not in training_state:\n",
    "            logger.warning(\"trainer state `val_metric_per_epoch` not found, using empty list\")\n",
    "            val_metric_per_epoch: List[float] = []\n",
    "        else:\n",
    "            val_metric_per_epoch = training_state[\"val_metric_per_epoch\"]\n",
    "\n",
    "        if isinstance(training_state[\"epoch\"], int):\n",
    "            epoch_to_return = training_state[\"epoch\"] + 1\n",
    "        else:\n",
    "            epoch_to_return = int(training_state[\"epoch\"].split('.')[0]) + 1\n",
    "\n",
    "        # For older checkpoints with batch_num_total missing, default to old behavior where\n",
    "        # it is unchanged.\n",
    "        batch_num_total = training_state.get('batch_num_total')\n",
    "        if batch_num_total is not None:\n",
    "            self._batch_num_total = batch_num_total\n",
    "\n",
    "        return epoch_to_return, val_metric_per_epoch\n",
    "\n",
    "    # Requires custom from_params.\n",
    "    @classmethod\n",
    "    def from_params(cls,\n",
    "                    model: Model,\n",
    "                    serialization_dir: str,\n",
    "                    iterator: DataIterator,\n",
    "                    iterator_aux: DataIterator,\n",
    "                    iterator_aux2: DataIterator,\n",
    "                    train_data: Iterable[Instance],\n",
    "                    train_data_aux: Iterable[Instance],\n",
    "                    train_data_aux2: Iterable[Instance],\n",
    "                    mixing_ratio: float,\n",
    "                    mixing_ratio2: float,\n",
    "                    cutoff_epoch: int,\n",
    "                    validation_data: Optional[Iterable[Instance]],\n",
    "                    validation_data_aux: Optional[Iterable[Instance]],\n",
    "                    validation_data_aux2: Optional[Iterable[Instance]],\n",
    "                    params: Params,\n",
    "                    validation_iterator: DataIterator = None) -> 'MultiTaskTrainer2':\n",
    "\n",
    "        patience = params.pop_int(\"patience\", None)\n",
    "        validation_metric = params.pop(\"validation_metric\", \"-loss\")\n",
    "        shuffle = params.pop_bool(\"shuffle\", True)\n",
    "        num_epochs = params.pop_int(\"num_epochs\", 20)\n",
    "        cuda_device = params.pop_int(\"cuda_device\", -1)\n",
    "        grad_norm = params.pop_float(\"grad_norm\", None)\n",
    "        grad_clipping = params.pop_float(\"grad_clipping\", None)\n",
    "        lr_scheduler_params = params.pop(\"learning_rate_scheduler\", None)\n",
    "\n",
    "        if cuda_device >= 0:\n",
    "            model = model.cuda(cuda_device)\n",
    "        parameters = [[n, p] for n, p in model.named_parameters() if p.requires_grad]\n",
    "        optimizer = Optimizer.from_params(parameters, params.pop(\"optimizer\"))\n",
    "\n",
    "        if lr_scheduler_params:\n",
    "            scheduler = LearningRateScheduler.from_params(optimizer, lr_scheduler_params)\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "        num_serialized_models_to_keep = params.pop_int(\"num_serialized_models_to_keep\", 20)\n",
    "        keep_serialized_model_every_num_seconds = params.pop_int(\n",
    "                \"keep_serialized_model_every_num_seconds\", None)\n",
    "        model_save_interval = params.pop_float(\"model_save_interval\", None)\n",
    "        summary_interval = params.pop_int(\"summary_interval\", 100)\n",
    "        histogram_interval = params.pop_int(\"histogram_interval\", None)\n",
    "        should_log_parameter_statistics = params.pop_bool(\"should_log_parameter_statistics\", True)\n",
    "        should_log_learning_rate = params.pop_bool(\"should_log_learning_rate\", False)\n",
    "\n",
    "        params.assert_empty(cls.__name__)\n",
    "        return MultiTaskTrainer2(model, optimizer, iterator,\n",
    "                                    train_data,\n",
    "                                    train_data_aux,\n",
    "                                    train_data_aux2,\n",
    "                                    mixing_ratio,\n",
    "                                    mixing_ratio2,\n",
    "                                    cutoff_epoch,\n",
    "                                    validation_data,\n",
    "                                    validation_data_aux,\n",
    "                                    validation_data_aux2,\n",
    "                                    patience=patience,\n",
    "                                    validation_metric=validation_metric,\n",
    "                                    validation_iterator=validation_iterator,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_epochs=num_epochs,\n",
    "                                    serialization_dir=serialization_dir,\n",
    "                                    cuda_device=cuda_device,\n",
    "                                    grad_norm=grad_norm,\n",
    "                                    grad_clipping=grad_clipping,\n",
    "                                    learning_rate_scheduler=scheduler,\n",
    "                                    num_serialized_models_to_keep=num_serialized_models_to_keep,\n",
    "                                    keep_serialized_model_every_num_seconds=keep_serialized_model_every_num_seconds,\n",
    "                                    model_save_interval=model_save_interval,\n",
    "                                    summary_interval=summary_interval,\n",
    "                                    histogram_interval=histogram_interval,\n",
    "                                    should_log_parameter_statistics=should_log_parameter_statistics,\n",
    "                                    should_log_learning_rate=should_log_learning_rate,\n",
    "                                    iterator_aux=iterator_aux,\n",
    "                                    iterator_aux2=iterator_aux2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d971a-b82c-4aca-9a3f-7f8bf3bc2f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48226276-8712-4d73-a276-f024629d4d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_model_from_file('experiment_configs/custom_config.json', './runs/test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41a148-5521-4eda-b649-35a0fc1ab9cc",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1258675-d32b-4fb6-9515-9a97ad5c0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params.from_file('experiment_configs/custom_config.json', \"\")\n",
    "serialization_dir = './runs/test5'\n",
    "file_friendly_logging = False\n",
    "recover = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb79d800-07d0-4b55-abe3-63a92dab064f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   random_seed = 21016\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   numpy_seed = 5000\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   pytorch_seed = 8000\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.checks -   Pytorch version: 1.10.2\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'multilabel': 'false', 'type': 'scicite_datasetreader', 'use_sparse_lexicon_features': 'false', 'with_elmo': 'true'} and extras {}\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.type = scicite_datasetreader\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {} and extras {}\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.tokenizer.type = word\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras {}\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.tokenizer.start_tokens = None\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.tokenizer.end_tokens = None\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.use_lexicon_features = False\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.use_sparse_lexicon_features = false\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.multilabel = false\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.with_elmo = true\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   dataset_reader.reader_format = flat\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   validation_dataset_reader = None\n",
      "04/10/2024 22:23:08 - INFO - allennlp.common.params -   train_data_path = scicite_data/train.jsonl\n",
      "04/10/2024 22:23:08 - INFO - __main__ -   Reading training data from scicite_data/train.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "8243it [00:02, 3269.70it/s]\n",
      "\n",
      "04/10/2024 22:23:11 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'scicite_section_title_data_reader', 'with_elmo': 'true'} and extras {}\n",
      "04/10/2024 22:23:11 - INFO - allennlp.common.params -   dataset_reader_aux.type = scicite_section_title_data_reader\n",
      "04/10/2024 22:23:11 - INFO - allennlp.common.params -   dataset_reader_aux.with_elmo = true\n",
      "04/10/2024 22:23:11 - INFO - allennlp.common.params -   train_data_path_aux = scicite_data/scaffolds/sections-scaffold-train.jsonl\n",
      "04/10/2024 22:23:11 - INFO - __main__ -   Reading auxiliary training data from scicite_data/scaffolds/sections-scaffold-train.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "42033it [00:10, 4203.28it/s]\n",
      "87800it [00:20, 4422.88it/s]\n",
      "91412it [00:20, 4408.03it/s]\n",
      "\n",
      "04/10/2024 22:23:31 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'scicite_cite_worthiness_data_reader', 'with_elmo': 'true'} and extras {}\n",
      "04/10/2024 22:23:31 - INFO - allennlp.common.params -   dataset_reader_aux2.type = scicite_cite_worthiness_data_reader\n",
      "04/10/2024 22:23:31 - INFO - allennlp.common.params -   dataset_reader_aux2.with_elmo = true\n",
      "04/10/2024 22:23:31 - INFO - allennlp.common.params -   train_data_path_aux2 = scicite_data/scaffolds/cite-worthiness-scaffold-train.jsonl\n",
      "04/10/2024 22:23:31 - INFO - __main__ -   Reading second auxiliary training data for from scicite_data/scaffolds/cite-worthiness-scaffold-train.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "60644it [00:10, 6064.37it/s]\n",
      "73484it [00:11, 6228.48it/s]\n",
      "\n",
      "04/10/2024 22:23:43 - INFO - allennlp.common.params -   aux_sample_fraction = 1.0\n",
      "04/10/2024 22:23:43 - INFO - __main__ -   Inflating train data from 8243 to 91412 samples\n",
      "04/10/2024 22:23:43 - INFO - allennlp.common.params -   validation_data_path = scicite_data/dev.jsonl\n",
      "04/10/2024 22:23:43 - INFO - __main__ -   Reading validation data from scicite_data/dev.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "916it [00:00, 3564.38it/s]\n",
      "\n",
      "04/10/2024 22:23:43 - INFO - allennlp.common.params -   validation_data_path_aux = None\n",
      "04/10/2024 22:23:43 - INFO - allennlp.common.params -   validation_data_path_aux2 = None\n",
      "04/10/2024 22:23:43 - INFO - allennlp.common.params -   test_data_path = scicite_data/test.jsonl\n",
      "04/10/2024 22:23:43 - INFO - __main__ -   Reading test data from scicite_data/test.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "1861it [00:00, 3557.98it/s]\n",
      "\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   test_data_path_aux = None\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   test_data_path_aux2 = None\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   mixing_ratio = 0.05\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   mixing_ratio2 = 0.05\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   cutoff_epoch = -1\n",
      "04/10/2024 22:23:44 - INFO - __main__ -   From dataset instances, validation, test, train will be considered for vocabulary creation.\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.type = None\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.extend = False\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.directory_path = None\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.min_count = None\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.max_vocab_size = None\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.min_pretrained_embeddings = None\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.only_include_pretrained_words = False\n",
      "04/10/2024 22:23:44 - INFO - allennlp.common.params -   vocabulary.tokens_to_add = None\n",
      "04/10/2024 22:23:44 - INFO - scicite.training.vocabulary_multitask -   Fitting token dictionary from dataset.\n",
      "0it [00:00, ?it/s]\n",
      "94189it [00:04, 23022.34it/s]\n",
      "\n",
      "04/10/2024 22:23:48 - INFO - scicite.training.vocabulary_multitask -   Fitting token dictionary from auxillary dataset.\n",
      "  0%|          | 0/146968 [00:00<?, ?it/s]\n",
      "100%|##########| 146968/146968 [00:04<00:00, 29976.50it/s]\n",
      "\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'citation_text_encoder': {'bidirectional': True, 'dropout': 0.3, 'hidden_size': 100, 'input_size': 1324, 'num_layers': 2, 'type': 'gru'}, 'classifier_feedforward': {'activations': ['linear', 'linear'], 'dropout': [0, 0], 'hidden_dims': [20, 3], 'input_dim': 200, 'num_layers': 2}, 'classifier_feedforward_2': {'activations': ['linear', 'linear'], 'dropout': [0, 0], 'hidden_dims': [20, 5], 'input_dim': 200, 'num_layers': 2}, 'classifier_feedforward_3': {'activations': ['linear', 'linear'], 'dropout': [0, 0], 'hidden_dims': [20, 2], 'input_dim': 200, 'num_layers': 2}, 'data_format': 'scicite_flat_jsonlines', 'elmo_text_field_embedder': {'elmo': {'do_layer_norm': 'true', 'dropout': 0.5, 'options_file': 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json', 'type': 'elmo_token_embedder', 'weight_file': 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': '/home/kanhon/Desktop/kanhon/NUS Computing/CS4248/github_dir/cs4248/scaffold/Project/sci-cite/scicite/pretrained_weights/GoogleNews-vectors-negative300.txt', 'trainable': 'false', 'type': 'embedding'}}, 'lexicon_embedder': {'embedding_dim': 100, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz', 'trainable': 'false', 'vocab_namespace': 'lexicon_ids'}, 'multilabel': 'false', 'report_auxiliary_metrics': 'true', 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'pretrained_file': '/home/kanhon/Desktop/kanhon/NUS Computing/CS4248/github_dir/cs4248/scaffold/Project/sci-cite/scicite/pretrained_weights/GoogleNews-vectors-negative300.txt', 'trainable': 'false', 'type': 'embedding'}}, 'type': 'scaffold_bilstm_attention_classifier', 'use_lexicon_features': 'false', 'use_sparse_lexicon_features': 'false', 'with_elmo': 'true'} and extras {'vocab': <scicite.training.vocabulary_multitask.VocabularyMultitask object at 0x7f14378eb208>}\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.params -   model.type = scaffold_bilstm_attention_classifier\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.params -   model.with_elmo = true\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'elmo': {'do_layer_norm': 'true', 'dropout': 0.5, 'options_file': 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json', 'type': 'elmo_token_embedder', 'weight_file': 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': '/home/kanhon/Desktop/kanhon/NUS Computing/CS4248/github_dir/cs4248/scaffold/Project/sci-cite/scicite/pretrained_weights/GoogleNews-vectors-negative300.txt', 'trainable': 'false', 'type': 'embedding'}} and extras {'vocab': <scicite.training.vocabulary_multitask.VocabularyMultitask object at 0x7f14378eb208>}\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.type = basic\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.embedder_to_indexer_map = None\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.allow_unmatched_keys = False\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.token_embedders = None\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'do_layer_norm': 'true', 'dropout': 0.5, 'options_file': 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json', 'type': 'elmo_token_embedder', 'weight_file': 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'} and extras {'vocab': <scicite.training.vocabulary_multitask.VocabularyMultitask object at 0x7f14378eb208>}\n",
      "04/10/2024 22:23:53 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.type = elmo_token_embedder\n",
      "04/10/2024 22:23:55 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.options_file = https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\n",
      "04/10/2024 22:23:55 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.weight_file = https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\n",
      "04/10/2024 22:23:55 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.requires_grad = False\n",
      "04/10/2024 22:23:55 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.do_layer_norm = true\n",
      "04/10/2024 22:23:55 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.dropout = 0.5\n",
      "04/10/2024 22:23:55 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.namespace_to_cache = None\n",
      "04/10/2024 22:23:55 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.projection_dim = None\n",
      "04/10/2024 22:23:55 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "04/10/2024 22:23:55 - INFO - allennlp.modules.elmo -   Initializing ELMo\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'pretrained_file': '/home/kanhon/Desktop/kanhon/NUS Computing/CS4248/github_dir/cs4248/scaffold/Project/sci-cite/scicite/pretrained_weights/GoogleNews-vectors-negative300.txt', 'trainable': 'false', 'type': 'embedding'} and extras {'vocab': <scicite.training.vocabulary_multitask.VocabularyMultitask object at 0x7f14378eb208>}\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.type = embedding\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.num_embeddings = None\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.vocab_namespace = tokens\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.embedding_dim = 300\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.pretrained_file = /home/kanhon/Desktop/kanhon/NUS Computing/CS4248/github_dir/cs4248/scaffold/Project/sci-cite/scicite/pretrained_weights/GoogleNews-vectors-negative300.txt\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.projection_dim = None\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.trainable = false\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.padding_index = None\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.max_norm = None\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.norm_type = 2.0\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "04/10/2024 22:24:14 - INFO - allennlp.common.params -   model.elmo_text_field_embedder.tokens.sparse = False\n",
      "04/10/2024 22:24:14 - INFO - allennlp.modules.token_embedders.embedding -   Reading pretrained embeddings from file\n",
      "04/10/2024 22:24:14 - INFO - allennlp.modules.token_embedders.embedding -   Recognized a header line in the embedding file with number of tokens: 3000000\n",
      "  0%|          | 0/3000000 [00:00<?, ?it/s]\n",
      " 44%|####4     | 1325286/3000000 [00:10<00:12, 132528.55it/s]\n",
      " 94%|#########4| 2824338/3000000 [00:20<00:01, 142750.07it/s]\n",
      "100%|##########| 3000000/3000000 [00:21<00:00, 140997.15it/s]\n",
      "\n",
      "04/10/2024 22:24:36 - INFO - allennlp.modules.token_embedders.embedding -   Initializing pre-trained embedding layer\n",
      "04/10/2024 22:24:36 - INFO - allennlp.modules.token_embedders.embedding -   Pretrained embeddings were found for 78939 out of 142085 tokens\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.3, 'hidden_size': 100, 'input_size': 1324, 'num_layers': 2, 'type': 'gru'} and extras {}\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.type = gru\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.batch_first = True\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.stateful = False\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.bidirectional = True\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.dropout = 0.3\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.hidden_size = 100\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.input_size = 1324\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.num_layers = 2\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.citation_text_encoder.batch_first = True\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward.input_dim = 200\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward.num_layers = 2\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward.hidden_dims = [20, 3]\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward.activations = ['linear', 'linear']\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward.dropout = [0, 0]\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_2.input_dim = 200\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_2.num_layers = 2\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_2.hidden_dims = [20, 5]\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_2.activations = ['linear', 'linear']\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_2.dropout = [0, 0]\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_3.input_dim = 200\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_3.num_layers = 2\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_3.hidden_dims = [20, 2]\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_3.activations = ['linear', 'linear']\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.classifier_feedforward_3.dropout = [0, 0]\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.initializer = []\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.regularizer = []\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.use_lexicon_features = false\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.use_sparse_lexicon_features = false\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.data_format = scicite_flat_jsonlines\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.report_auxiliary_metrics = true\n",
      "04/10/2024 22:24:36 - INFO - allennlp.common.params -   model.predict_mode = False\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      attention_seq2seq.attention\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.bias_hh_l0\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.bias_hh_l0_reverse\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.bias_hh_l1\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.bias_hh_l1_reverse\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.bias_ih_l0\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.bias_ih_l0_reverse\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.bias_ih_l1\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.bias_ih_l1_reverse\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.weight_hh_l0\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.weight_hh_l0_reverse\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.weight_hh_l1\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.weight_hh_l1_reverse\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.weight_ih_l0\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.weight_ih_l0_reverse\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.weight_ih_l1\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      citation_text_encoder._module.weight_ih_l1_reverse\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward._linear_layers.0.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward._linear_layers.0.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward._linear_layers.1.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward._linear_layers.1.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward_2._linear_layers.0.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward_2._linear_layers.0.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward_2._linear_layers.1.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward_2._linear_layers.1.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward_3._linear_layers.0.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward_3._linear_layers.0.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward_3._linear_layers.1.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      classifier_feedforward_3._linear_layers.1.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "04/10/2024 22:24:36 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred mode: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2024 22:24:37 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']], 'type': 'bucket'} and extras {}\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.type = bucket\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']]} and extras {}\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.sorting_keys = [['citation_text', 'num_tokens']]\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.padding_noise = 0.1\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.biggest_batch_first = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.batch_size = 16\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.instances_per_epoch = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.max_instances_in_memory = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.cache_instances = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.track_epoch = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator.maximum_samples_per_batch = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']], 'type': 'bucket'} and extras {}\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.type = bucket\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']]} and extras {}\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.sorting_keys = [['citation_text', 'num_tokens']]\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.padding_noise = 0.1\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.biggest_batch_first = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.batch_size = 16\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.instances_per_epoch = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.max_instances_in_memory = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.cache_instances = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.track_epoch = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux.maximum_samples_per_batch = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']], 'type': 'bucket'} and extras {}\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.type = bucket\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']]} and extras {}\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.sorting_keys = [['citation_text', 'num_tokens']]\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.padding_noise = 0.1\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.biggest_batch_first = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.batch_size = 16\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.instances_per_epoch = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.max_instances_in_memory = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.cache_instances = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.track_epoch = False\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   iterator_aux2.maximum_samples_per_batch = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   validation_iterator = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.no_grad = ()\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   Following parameters are Frozen  (without gradient):\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_tokens.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   Following parameters are Tunable (with gradient):\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.weight_ih_l0\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.weight_hh_l0\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.bias_ih_l0\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.bias_hh_l0\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.weight_ih_l0_reverse\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.weight_hh_l0_reverse\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.bias_ih_l0_reverse\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.bias_hh_l0_reverse\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.weight_ih_l1\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.weight_hh_l1\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.bias_ih_l1\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.bias_hh_l1\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.weight_ih_l1_reverse\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.weight_hh_l1_reverse\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.bias_ih_l1_reverse\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   citation_text_encoder._module.bias_hh_l1_reverse\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward._linear_layers.0.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward._linear_layers.0.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward._linear_layers.1.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward._linear_layers.1.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward_2._linear_layers.0.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward_2._linear_layers.0.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward_2._linear_layers.1.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward_2._linear_layers.1.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward_3._linear_layers.0.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward_3._linear_layers.0.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward_3._linear_layers.1.weight\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   classifier_feedforward_3._linear_layers.1.bias\n",
      "04/10/2024 22:24:37 - INFO - __main__ -   attention_seq2seq.attention\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.patience = 4\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.validation_metric = +average_F1\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.shuffle = True\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.num_epochs = 10\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.cuda_device = 0\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.grad_norm = None\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.grad_clipping = 5\n",
      "04/10/2024 22:24:37 - INFO - allennlp.common.params -   trainer.learning_rate_scheduler = None\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.optimizer.type = adadelta\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.optimizer.parameter_groups = None\n",
      "04/10/2024 22:24:41 - INFO - allennlp.training.optimizers -   Number of trainable parameters: 1049274\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.optimizer.rho = 0.95\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.num_serialized_models_to_keep = 20\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.keep_serialized_model_every_num_seconds = None\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.model_save_interval = None\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.summary_interval = 100\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.histogram_interval = None\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.should_log_parameter_statistics = True\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   trainer.should_log_learning_rate = False\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   evaluate_on_test = true\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   evaluate_aux_on_test = true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    metrics = trainer.train()\\nexcept KeyboardInterrupt:\\n    # if we have completed an epoch, try to create a model archive.\\n    if os.path.exists(os.path.join(serialization_dir, _DEFAULT_WEIGHTS)):\\n        logging.info(\"Training interrupted by the user. Attempting to create \"\\n                     \"a model archive using the current best epoch weights.\")\\n        archive_model(serialization_dir, files_to_archive=params.files_to_archive)\\n    raise\\n\\n# Now tar up results\\narchive_model(serialization_dir, files_to_archive=params.files_to_archive)\\n\\nlogger.info(\"Loading the best epoch weights.\")\\nbest_model_state_path = os.path.join(serialization_dir, \\'best.th\\')\\nbest_model_state = torch.load(best_model_state_path)\\nbest_model = model\\nbest_model.load_state_dict(best_model_state)\\n\\nif test_data and evaluate_on_test:\\n    logger.info(\"The model will be evaluated using the best epoch weights.\")\\n    test_metrics = evaluate(\\n            best_model, test_data, validation_iterator or iterator,\\n            cuda_device=trainer._cuda_devices[0] # pylint: disable=protected-access\\n    )\\n    for key, value in test_metrics.items():\\n        metrics[\"test_\" + key] = value\\n\\nelif test_data:\\n    logger.info(\"To evaluate on the test set after training, pass the \"\\n                \"\\'evaluate_on_test\\' flag, or use the \\'allennlp evaluate\\' command.\")\\n\\nif test_data_aux and evaluate_aux_on_test:\\n    # for instance in test_data_aux:\\n    #     instance.index_fields(vocab)\\n    # for instance in test_data_aux2:\\n    #     instance.index_fields(vocab)\\n    test_metrics_aux = evaluate(best_model, test_data_aux, iterator_aux,\\n                                cuda_device=trainer._cuda_devices[0])  # pylint: disable=protected-access\\n    test_metrics_aux2 = evaluate(best_model, test_data_aux2, iterator_aux2,\\n                                 cuda_device=trainer._cuda_devices[0])  # pylint: disable=protected-access\\n\\n    for key, value in test_metrics_aux.items():\\n        metrics[\"test_aux_\" + key] = value\\n    for key, value in test_metrics_aux2.items():\\n        metrics[\"test_aux2_\" + key] = value\\n\\nelif test_data_aux:\\n    logger.info(\"To evaluate on the auxiliary test set after training, pass the \"\\n                \"\\'evaluate_on_test\\' flag, or use the \\'allennlp evaluate\\' command.\")\\n\\ndump_metrics(os.path.join(serialization_dir, \"metrics.json\"), metrics, log=True)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2024 22:24:41 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'multilabel': 'false', 'type': 'scicite_datasetreader', 'use_sparse_lexicon_features': 'false', 'with_elmo': 'true'} and extras {}\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.type = scicite_datasetreader\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {} and extras {}\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.tokenizer.type = word\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras {}\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.tokenizer.start_tokens = None\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.tokenizer.end_tokens = None\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.use_lexicon_features = False\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.use_sparse_lexicon_features = false\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.multilabel = false\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.with_elmo = true\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   dataset_reader.reader_format = flat\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   validation_dataset_reader = None\n",
      "04/10/2024 22:24:41 - INFO - allennlp.common.params -   train_data_path = scicite_data/train.jsonl\n",
      "04/10/2024 22:24:41 - INFO - __main__ -   Reading training data from scicite_data/train.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "8243it [00:02, 3671.09it/s]\n",
      "\n",
      "04/10/2024 22:24:43 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'scicite_section_title_data_reader', 'with_elmo': 'true'} and extras {}\n",
      "04/10/2024 22:24:43 - INFO - allennlp.common.params -   dataset_reader_aux.type = scicite_section_title_data_reader\n",
      "04/10/2024 22:24:43 - INFO - allennlp.common.params -   dataset_reader_aux.with_elmo = true\n",
      "04/10/2024 22:24:43 - INFO - allennlp.common.params -   train_data_path_aux = scicite_data/scaffolds/sections-scaffold-train.jsonl\n",
      "04/10/2024 22:24:43 - INFO - __main__ -   Reading auxiliary training data from scicite_data/scaffolds/sections-scaffold-train.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "46174it [00:10, 4617.22it/s]\n",
      "91412it [00:20, 4561.83it/s]\n",
      "\n",
      "04/10/2024 22:25:03 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'scicite_cite_worthiness_data_reader', 'with_elmo': 'true'} and extras {}\n",
      "04/10/2024 22:25:03 - INFO - allennlp.common.params -   dataset_reader_aux2.type = scicite_cite_worthiness_data_reader\n",
      "04/10/2024 22:25:03 - INFO - allennlp.common.params -   dataset_reader_aux2.with_elmo = true\n",
      "04/10/2024 22:25:03 - INFO - allennlp.common.params -   train_data_path_aux2 = scicite_data/scaffolds/cite-worthiness-scaffold-train.jsonl\n",
      "04/10/2024 22:25:03 - INFO - __main__ -   Reading second auxiliary training data for from scicite_data/scaffolds/cite-worthiness-scaffold-train.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "61996it [00:10, 6199.59it/s]\n",
      "73484it [00:11, 6374.72it/s]\n",
      "\n",
      "04/10/2024 22:25:15 - INFO - allennlp.common.params -   aux_sample_fraction = 1.0\n",
      "04/10/2024 22:25:15 - INFO - __main__ -   Inflating train data from 8243 to 91412 samples\n",
      "04/10/2024 22:25:15 - INFO - allennlp.common.params -   validation_data_path = scicite_data/dev.jsonl\n",
      "04/10/2024 22:25:15 - INFO - __main__ -   Reading validation data from scicite_data/dev.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "916it [00:00, 3640.38it/s]\n",
      "\n",
      "04/10/2024 22:25:15 - INFO - allennlp.common.params -   validation_data_path_aux = None\n",
      "04/10/2024 22:25:15 - INFO - allennlp.common.params -   validation_data_path_aux2 = None\n",
      "04/10/2024 22:25:15 - INFO - allennlp.common.params -   test_data_path = scicite_data/test.jsonl\n",
      "04/10/2024 22:25:15 - INFO - __main__ -   Reading test data from scicite_data/test.jsonl\n",
      "0it [00:00, ?it/s]\n",
      "1861it [00:00, 3734.14it/s]\n",
      "\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   test_data_path_aux = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   test_data_path_aux2 = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']], 'type': 'bucket'} and extras {}\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.type = bucket\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']]} and extras {}\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.sorting_keys = [['citation_text', 'num_tokens']]\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.padding_noise = 0.1\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.biggest_batch_first = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.batch_size = 16\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.instances_per_epoch = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.max_instances_in_memory = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.cache_instances = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.track_epoch = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator.maximum_samples_per_batch = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']], 'type': 'bucket'} and extras {}\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.type = bucket\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']]} and extras {}\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.sorting_keys = [['citation_text', 'num_tokens']]\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.padding_noise = 0.1\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.biggest_batch_first = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.batch_size = 16\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.instances_per_epoch = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.max_instances_in_memory = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.cache_instances = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.track_epoch = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux.maximum_samples_per_batch = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']], 'type': 'bucket'} and extras {}\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.type = bucket\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'sorting_keys': [['citation_text', 'num_tokens']]} and extras {}\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.sorting_keys = [['citation_text', 'num_tokens']]\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.padding_noise = 0.1\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.biggest_batch_first = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.batch_size = 16\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.instances_per_epoch = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.max_instances_in_memory = None\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.cache_instances = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.track_epoch = False\n",
      "04/10/2024 22:25:16 - INFO - allennlp.common.params -   iterator_aux2.maximum_samples_per_batch = None\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [09:45<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'citation_text': {'elmo': tensor([[[259,  84, 109,  ..., 261, 261, 261],\n",
      "         [259, 100, 112,  ..., 261, 261, 261],\n",
      "         [259,  79,  80,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  85, 105,  ..., 261, 261, 261],\n",
      "         [259, 102, 121,  ..., 261, 261, 261],\n",
      "         [259, 106, 116,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [259, 105, 122,  ..., 261, 261, 261],\n",
      "         [259,  47, 260,  ..., 261, 261, 261],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  84, 102,  ..., 261, 261, 261],\n",
      "         [259,  45, 260,  ..., 261, 261, 261],\n",
      "         [259,  73,  66,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[259,  85, 105,  ..., 261, 261, 261],\n",
      "         [259, 115, 102,  ..., 261, 261, 261],\n",
      "         [259, 120, 102,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [259,  94, 260,  ..., 261, 261, 261],\n",
      "         [259,  47, 260,  ..., 261, 261, 261],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  73, 112,  ..., 261, 261, 261],\n",
      "         [259,  45, 260,  ..., 261, 261, 261],\n",
      "         [259, 105, 112,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  73, 112,  ..., 261, 261, 261],\n",
      "         [259,  78, 112,  ..., 261, 261, 261],\n",
      "         [259,  66, 116,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]]]), 'tokens': tensor([[30792,  3174,  9531,    17,  5112,    49,    16,  8243,   449,   490,\n",
      "          5159,  1395,     9,  9532,     7, 30793,     2,     5,    14, 30794,\n",
      "            12,    15,     3,     2,     5,     8,    23,    10,     4,  3212,\n",
      "          8243, 10764,     9,  9164,    12,    15,     3,     2, 30795, 11106,\n",
      "            12,    15,     3,     2,     5,     8,     3,     0,     0,     0,\n",
      "             0],\n",
      "        [   62,  2098,    17,   196,    20,     4,  1267,    18,  7478,   380,\n",
      "            38,  3226,    11,  2975,    11,  1329,  1987,     6,    91,     6,\n",
      "            60,  7607,   407,    24,  3936,     6,  3322,   362,    10,     4,\n",
      "          1817,   407,     9, 10443,    12,    15,     3,     2,     5,     8,\n",
      "            33,    17,    47,   196,    20,     4,  2324,  2468,   920,     3,\n",
      "             0],\n",
      "        [ 1853,     2, 29991,    75,    10,   915,     2,   101,   166,    58,\n",
      "             5,     7,     5,   876,   142, 13812,     2,    38,    43,  3455,\n",
      "          4331,    30,  3429,    10,  1047,     2,    23,    92,    25,    45,\n",
      "             4,   407,   132,     9,     5,     8,    14,   977,     2,    92,\n",
      "            38, 26572,  4331,     9,   428,     3,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [40072,    47,  3320,    56, 10997,    13,     4,    13,   732,   954,\n",
      "            51,    23,     4,  9803,  5436,  5671,     9, 18899,     8,    22,\n",
      "             5,    21,     2, 37708, 15200,    19,  7937,     9, 16320,     8,\n",
      "            22,     5,    21,    30,     4,  3187,    13,  5667,  9303, 37709,\n",
      "             9, 23611,     8,    22,     5,    21,     3,     0,     0,     0,\n",
      "             0],\n",
      "        [12134,   401,    17,    47,    16,   543,   545,     6,   256,    10,\n",
      "           857,    96,     6,  3984,     9, 27460,    12,    15,     3,     2,\n",
      "             5,    14,  2737,     7,  2112,     2,     5,    14,  5022,    12,\n",
      "            15,     3,     2,     5,     8,     7,  2060,    10,   137,  3984,\n",
      "             9, 27461,    12,    15,     3,     2,     5,     8,     3,     0,\n",
      "             0],\n",
      "        [    5,   568,    30,    57,    10,    91, 28628,     8,    27,  1698,\n",
      "            19,   669,   617,    19, 26944, 28629, 16342,    17,   252,   246,\n",
      "             2,   230,   243,  2488,     6,  7393,     2, 20969,     2,    30,\n",
      "          2428,     6,    63, 16342,    41,   164,    10,   207,  5624,    29,\n",
      "             4,  2509,  8910,   807,    23,    86,    23,  4924,   408, 20234,\n",
      "             3],\n",
      "        [ 1849,     7,     9,  8291,    12,    15,     3,     2,     5,    14,\n",
      "          8209,    12,    15,     3,     2,     5,     8,     8,     2,  1437,\n",
      "          1137,    11,  1305,     4,  1812,     6,   374,     7,   674,  1565,\n",
      "          1700,  1128,    11,  9294,   197, 39657,    29,     4,  2545,  1843,\n",
      "           134,   463,    80,   368,     3,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [ 9383,     2,   237,  1686,    25, 25382,    24,    16,   224,  8837,\n",
      "             2,    33,    17,    35,   112,  2368,  8055,   111,     9, 36877,\n",
      "             2,     5,     2,  1201,     5,     8,    18,  4722,    23,   112,\n",
      "            16,   585,    11,    35,   519,   111,     9,  6994,    12,    15,\n",
      "             3,     2,     5,     2,  1201,     5,     8,     3,     0,     0,\n",
      "             0],\n",
      "        [   42, 19683,     9,     5,     8,   121,    38, 37803,   185,   139,\n",
      "             6,   976, 37804,    10,   263,    65,    10,  1218,    14,    10,\n",
      "          2253,  2690, 31094,     9,     5,     8,   263,   201,   356,   259,\n",
      "           139,    65,  1218,     7,    10, 14920,     9,     5,     8,   263,\n",
      "             7,  1218,   201,   117,   139,     3,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [14458,  1256, 30308,   139,    25,   391,    10,     4,  4210,     6,\n",
      "           553,   100,    13,   756,  5927,  1432,    51,    23,  1754, 15133,\n",
      "         17218,     9,  3769,     8,    22,     5,    21,     2,   237,  6566,\n",
      "             9,  1517,     8,    22,     5,    21,     2,  1754,  6566,     9,\n",
      "          7220,     8,    22,     5,    21,     7,  2957,    22,     5,    21,\n",
      "             3],\n",
      "        [  653,     4,    56,   448,     2,  9723,   327,    24, 12450,     9,\n",
      "             5,     2,     5,     8,    39,    34,   309,    87,    11,  1539,\n",
      "          1195,     6,     4, 41748,  1693,     6, 39306,     2,    16,   440,\n",
      "            13,   189,  7587,    18,    41, 32191,  9723,   327,    24, 41749,\n",
      "            89,  2097, 41750,  1195,   545,     9,     5,     8,     3,     0,\n",
      "             0],\n",
      "        [    5,     8,     7,    17,   391,    10,     4,  5194,     6,  6407,\n",
      "            23,    86,    23,  1565,  1598,  1360,     2,    24,  4220,    20,\n",
      "          2713,    13,  3684,   326,   240,  4036,     2,  4746,     2, 16572,\n",
      "             2, 26443,     2, 18657,     2, 28030,     2, 28031,     7,   290,\n",
      "             9, 10862,    13, 13186,    12,    15,     3,     5,     8,     3,\n",
      "             0],\n",
      "        [  129,    17,   555,    46,    19,     4,   203,     6,  2958,     2,\n",
      "          3705,     2,  9078,     2,  1152,     7,  2801,  2498,     9,  1101,\n",
      "            12,    15,     3,     2,     5,    14,  4878,    12,    15,     3,\n",
      "             2,     5,    14, 27368,    12,    15,     3,     2,     5,    14,\n",
      "         27369,     7, 27370,     2,     5,     8,     3,     0,     0,     0,\n",
      "             0],\n",
      "        [  120,    53,    38,  3652,   101,  1475,   139,    10,  2649,  5445,\n",
      "           414,    20,   176,    13,  2649,    22,     5,    13,     5,    21,\n",
      "             2,   176,    13,  5445,    22,     5,    21,     2,    30, 18689,\n",
      "           414,    22,     5,    21,    30,   252,    20,    60,  1117,   139,\n",
      "           433,   498,     7,   142,  7051,  1334,    22,     5,    21,     3,\n",
      "             0],\n",
      "        [  128,     2,   286,  8203,  3096,    20,     4,  3048,    13,   713,\n",
      "          1246,  3590,   459,  1311,  2712,    23,   597,     5,    13,    11,\n",
      "            13,     5,   767,    20,    90,   688,    38,    71,     9, 11781,\n",
      "            22,     5,    21,     2, 13761,    70, 28004,    22,     5,    21,\n",
      "            30, 28005,    22,     5,    21,     8,     3,     0,     0,     0,\n",
      "             0],\n",
      "        [40178,  1280,  3671,   171, 10259,  9978,     9, 25931,    13,  2984,\n",
      "             8,    32,   643,    23,  1284,    48, 25931,    13,  2984,   131,\n",
      "          2649,  1197,   218,     9,  1002,    70, 23664,     8,  2676,  2649,\n",
      "          1022,   218,     9, 42374,    70, 42375,     9,  7233,    12,    15,\n",
      "             3,     5,     8,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0]])}, 'labels': tensor([1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1]), 'year_diff': tensor([[-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.]]), 'citing_paper_id': ['5f9e27fd212e0eac30dd8b2e9a53d422f5ca0bd4', '062a50f458b97b833ba4543900a3e5da6bc5c956', '7940cf29d1389531a416d415b4fb34c3371b956b', 'dd8c456a60b0ae3dc19f2d1eee6fbd3c3921c400', '18c97ea2ff60c110cc2a523e0fdf729608cbb083', '83b4ab34de05f70eb49d6529deb4b2e5e7ecfc6a', '45701ff05d3ed583a658f18aa998944c1d4eee7a', 'f104a3a2693ee78c2933f67f611a344bc6e1d987', '25b442e8fcc52b495b776f208a1904e854c239e8', '3cd81a85fd7cbfd006384dfbff1c5d317fd32adb', 'aab326884c65b09274891f94f71d3cb620346f87', '9d10103f5056b6b650245395b3a1ee49df9d5199', '952660f4a305ebc8bc46ea23dd1ea41464d6681f', '5ba35b32fbd5fadaa10df97b87df75eaa2203923', '1872080baa7d30ec8fb87be9a65358cd3a7fb649', '439fb02e7457da470a31961847ee8a215b96bab8'], 'cited_paper_id': ['9e4ab179a114ed860aebd5ee4699406aa7088069', '5fe3d50dc8e23da35efe84f2436d56900dbb6f65', 'ce4a7038224fe2cba9fcbdbeacaf3957c259fcb8', '1cc4208afda03257c461e8b767c007b5570104bd', 'fc13b9c3dfcc121013edaa12fa8ce7842aaed21a', 'df29d6d2602ef4c0188d5d74fb2898ab7570f101', 'f89d87e5f4d58e9bb7d576ec6ae34a84240b9c5e', '11877ad1b3e5ccebbdf02cbe6c6015c82661e550', '7ef70c0f59587e8183581c34e32991d8b3b3d77b', '18367eed9e76446a1e4e3563d4826fdac584bc68', '76c45f67dcb8a9299c0a191cf611d551a8148921', '2ae69afebf2f5f68f646c31ef8d7028bf0193ed5', '6b1223f442606ef04c8c04b36f1cdc0efb14c560', 'fd4e86ba764f50dcefd4d57bb489a7f2c169e137', '894be9b4ea46a5c422e81ef3c241072d4c73fdc0', '43781b850f0b718afe13e1cd5ebd5ab5b61ea086'], 'citation_excerpt_index': [2, 0, 0, 9, 8, 0, 7, 11, 2, 1, 15, 1, 11, 7, 11, 0], 'citation_id': ['5f9e27fd212e0eac30dd8b2e9a53d422f5ca0bd4>9e4ab179a114ed860aebd5ee4699406aa7088069', '062a50f458b97b833ba4543900a3e5da6bc5c956>5fe3d50dc8e23da35efe84f2436d56900dbb6f65', '7940cf29d1389531a416d415b4fb34c3371b956b>ce4a7038224fe2cba9fcbdbeacaf3957c259fcb8', 'dd8c456a60b0ae3dc19f2d1eee6fbd3c3921c400>1cc4208afda03257c461e8b767c007b5570104bd', '18c97ea2ff60c110cc2a523e0fdf729608cbb083>fc13b9c3dfcc121013edaa12fa8ce7842aaed21a', '83b4ab34de05f70eb49d6529deb4b2e5e7ecfc6a>df29d6d2602ef4c0188d5d74fb2898ab7570f101', '45701ff05d3ed583a658f18aa998944c1d4eee7a>f89d87e5f4d58e9bb7d576ec6ae34a84240b9c5e', 'f104a3a2693ee78c2933f67f611a344bc6e1d987>11877ad1b3e5ccebbdf02cbe6c6015c82661e550', '25b442e8fcc52b495b776f208a1904e854c239e8>7ef70c0f59587e8183581c34e32991d8b3b3d77b', '3cd81a85fd7cbfd006384dfbff1c5d317fd32adb>18367eed9e76446a1e4e3563d4826fdac584bc68', 'aab326884c65b09274891f94f71d3cb620346f87>76c45f67dcb8a9299c0a191cf611d551a8148921', '9d10103f5056b6b650245395b3a1ee49df9d5199>2ae69afebf2f5f68f646c31ef8d7028bf0193ed5', '952660f4a305ebc8bc46ea23dd1ea41464d6681f>6b1223f442606ef04c8c04b36f1cdc0efb14c560', '5ba35b32fbd5fadaa10df97b87df75eaa2203923>fd4e86ba764f50dcefd4d57bb489a7f2c169e137', '1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be9b4ea46a5c422e81ef3c241072d4c73fdc0', '439fb02e7457da470a31961847ee8a215b96bab8>43781b850f0b718afe13e1cd5ebd5ab5b61ea086']} {'citation_text': {'elmo': tensor([[[259,  74, 111,  ..., 261, 261, 261],\n",
      "         [259, 113,  98,  ..., 261, 261, 261],\n",
      "         [259,  45, 260,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  74, 117,  ..., 261, 261, 261],\n",
      "         [259, 120,  98,  ..., 261, 261, 261],\n",
      "         [259, 102, 121,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  88, 106,  ..., 261, 261, 261],\n",
      "         [259, 102,  98,  ..., 261, 261, 261],\n",
      "         [259, 113, 102,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [259, 116, 102,  ..., 261, 261, 261],\n",
      "         [259,  47, 260,  ..., 261, 261, 261],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[259,  88, 105,  ..., 261, 261, 261],\n",
      "         [259, 117, 105,  ..., 261, 261, 261],\n",
      "         [259, 111, 102,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [259,  47, 260,  ..., 261, 261, 261],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  66, 116,  ..., 261, 261, 261],\n",
      "         [259, 105, 122,  ..., 261, 261, 261],\n",
      "         [259, 103, 112,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  80, 111,  ..., 261, 261, 261],\n",
      "         [259, 120,  98,  ..., 261, 261, 261],\n",
      "         [259, 117, 112,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]]]), 'tokens': tensor([[   42,   301,     2,    10,    40,   395,     2,     4,   538,    17,\n",
      "            26,     4,    69, 77895, 77896, 23011,     2,    33,   354,    11,\n",
      "            41,    28,   344,    23, 56251,  3562,  1481,    18, 77897,     4,\n",
      "           108,   457,  2707,    16,  1504,   512,    13,    11,    13,   512,\n",
      "          1386,    20,     4,  2183,     3,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  129,    32,   701,    18,     4, 10561,     6, 38925,    20,  6658,\n",
      "           244,    28,   185,    10,     4, 45832,  2658,   773,   999,     9,\n",
      "          8130,     2,   307,    14,  4685,    37, 41106,     2,   235,    14,\n",
      "         24874,     2,   278,     8,     2,  1855, 31809,     4,   637,    58,\n",
      "          4333,  4891,     7,  5623,    10,    40,   999,     3,     0,     0,\n",
      "             0],\n",
      "        [ 5240,    90, 13565,  5612, 20502,   801,  5212,   811,     7,    90,\n",
      "           110,    41,    28,   257,    11,  1295,  3110,   276,    10,   336,\n",
      "           588,     9,  1696,     8,     2,   691,     7, 11570,     9,  2439,\n",
      "             8,     2,     4,   177,  5090,     9,  2214,     8,     2,   828,\n",
      "          1092,     9,  2919,     8,     2,     7,  4541,     7,  1222,     3,\n",
      "             0],\n",
      "        [ 3437,  1956,  2361,     9,  1646,     8,     7,    56,   381,   192,\n",
      "            31,  4234, 12632, 14329,    10,    16,   548,     6,   596,  1150,\n",
      "           313,     2,    29,   163,   285,    11,   427,   268,     2,   850,\n",
      "           687,     2,   163,  6826,     2,     7,    57,   309,     2,   408,\n",
      "           837,  3416,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [   36,  8101,     2,    30, 13171,  1002,    70,  1720, 57227, 11763,\n",
      "           390,   166,    58,  5421,  4540,   139,     7,  2843,    45,    95,\n",
      "          2692,     6, 31793, 13428, 42771,  7593, 16860,  1196,  1007,  5013,\n",
      "          1002,    70,  1720,  8809,    19,  3371,   788, 47273,   139,     6,\n",
      "          1025,  1774,  1336,     9, 64389,     8,    23,    86,    23,  5553,\n",
      "            36],\n",
      "        [ 1634,    55,    26,  1180,    13,   103,   478,    57,  1062,   932,\n",
      "             6,     4,   489,  3212,  1230,     7,   225,    18,     4,   132,\n",
      "             6,   423,  1278,     6, 27528,    32,   586,  1780,   243,     7,\n",
      "           372,    10,   552,    28,  2028,     9,  2949,     2,   572,    14,\n",
      "          2949,    12,    15,     3,     2,   438,     8,     3,     0,     0,\n",
      "             0],\n",
      "        [11175,     2,   133,   210,  9053,   881,    19,   594,    13,  1682,\n",
      "           108,   313,    31,    34,   119,     2,    51,    23,   594,    13,\n",
      "          1682,   479,  3397,   274,     2,   594,    13,  1682,   618,  1493,\n",
      "             2,   594,    13,  1682,  1871,    97,     7,   594,    13,  1682,\n",
      "           242,   192,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [ 1153,  1030,     2,    44,  2446,    18,    16,   640,   345,     6,\n",
      "            90,  4341,    81, 11107,   249,    24,    17,     4,   327,     6,\n",
      "            16,  1121,  1908,     2,    33,    48,     9,    16,     8,    17,\n",
      "          5678,    24,    16,   301,   424,     6,   762,   828,  4840,    30,\n",
      "          3075,     2,     9,   222,     8, 70071,   762,  1363,   806,     2,\n",
      "            36],\n",
      "        [  128,     2,   420,    50,    31,   697,   286,    11, 42716,     4,\n",
      "          1179,     6, 27615,   401, 70054,     8,    42,    40,    61,     2,\n",
      "            79,    38,  2094,    11,   208,    60, 14718,  2654,   498,     4,\n",
      "         27615,  2149,  5920,    11, 60549,     9,    23,   893,    10,   423,\n",
      "            13,    72,    24,  2977,     7,  1796,     8,     3,     0,     0,\n",
      "             0],\n",
      "        [  129,    17,    47,   284,    11,   933,   185,   502,    24, 38506,\n",
      "            20,     4,  2782,     6,  2394,    16,  1424,     2,     7,  2452,\n",
      "         32916,     7, 18686,     9,   235,     8,    31,    87,    18,   874,\n",
      "         22486,   502,    41,    28,   239,    19,     4,  5117,    59,   491,\n",
      "            16,   185,    13,   211,  4479,   464,   184,    23,  3819,  2399,\n",
      "             3],\n",
      "        [  736,   687,   210,    41,    28,  1545,    23,   262,   331,    16,\n",
      "          5748,     9, 10690,     8,   687,   210,     9, 22169,  2080,    14,\n",
      "         22894,   214,     8,    30,    16, 38487,     9,  1242,  5082,     2,\n",
      "            54,    13,  1438,     8,   687,   210,     9, 22169,  2080,    14,\n",
      "         22894,   214,    14,  6913,     7, 61064,  1710,     8,     3,     0,\n",
      "             0],\n",
      "        [14209,    11,   523,   170,  1511,    31,    34,   205,    29,  5371,\n",
      "            96,     2,     7,   115,    18,  2507,   237,   459,    31,  1056,\n",
      "           482,   926,    10,  2297,   288,     9,   145,     2,    19,   136,\n",
      "             2,  6398,    37,  1170,     2,   534,    14, 12073,     2,   227,\n",
      "            14, 49061,     2,   236,     2,    19,  1279,     8,     3,     0,\n",
      "             0],\n",
      "        [ 8707,  6444,  1515,  5270,  1251,    31,    71,    11,  1275,   493,\n",
      "           926,    20,    16,  2955,  1179,     6,  3831,  3573,     2,  5847,\n",
      "          2214,   856,    10,   104,    61,     2,    66,    20,    16,   207,\n",
      "          3635,    68,   216,     6,  1803,  3831,  3573,     7,  1800,    68,\n",
      "          3831,   670,   216,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  704,    18,   409,    17, 21233,    24,    16,  7418, 10113,   390,\n",
      "             2,  7366,   641,   350,   336,     2,  6482,    29,  7031,    57,\n",
      "          2257,     2,     7,  3814,  1422,     9, 35759,     2, 55000,     2,\n",
      "          7233,     2,    37, 60873,     2,   236,    14,  9987,    13, 55001,\n",
      "             2,   943,     2,    37, 60874,     2,   258,     8,     3,     0,\n",
      "             0],\n",
      "        [  178, 41360,  2696,    25,    47,   305,    11,   880,    16,   190,\n",
      "             2,    23,   346,    24,   590,  2932,  2285,     7,     4,   740,\n",
      "           994,   463,  2499, 11146,     9,   549,     2,  1775,     2,  1696,\n",
      "             8,     2,     4,  4210,     6, 35898,    41,  2920,    28,  1699,\n",
      "            24,  1436,  3626,     7, 41360,  4681,     3,     0,     0,     0,\n",
      "             0],\n",
      "        [  595,    32,    11,   933,  3562,  1232,   434, 13339,     7, 19250,\n",
      "            24,  3835,    35,  1935,  1585,   108,   186,     2,     7,     4,\n",
      "            56,    32,    11,   773,   638,     4,   336,    13,   193,     7,\n",
      "          8020,    24,  4740,    82,  2052,  5675,     7,   841,  2157,   192,\n",
      "             9, 62015,    36,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]])}, 'section_label': tensor([0, 2, 0, 0, 1, 3, 0, 0, 1, 1, 2, 2, 1, 0, 1, 3]), 'citing_paper_id': ['8382df07bc060b87c2310d05123461f2c3afa5ac', '28cc78cf8a07fc3febd3ce534a7a658ff388aac2', 'd7bc4e43056cbb3beee37e0a4246bd5a4a384191', '5582bebed97947a41e3ddd9bd1f284b73f1648c2', '8f58f03bcd0b4bb245fd81c18575a4291aafa634', '8f5cb88eed2c3e9bd134b46b14b6103ebf41c93e', '572b7f31aee095262057fe17b181ac262eb94995', 'f78c7bcf0c6c8534593db74a0b65bd4be60b999b', 'a7d70d79e4ad78534ca11cd279c385c679154e8d', '1e56ed3d2c855f848ffd91baa90f661772a279e1', 'b237877845cca53ea085be3b279284d64a572d0e', '1de4966482722d1f7da8bbef16d108639d9a1c38', '4384a5d742b869c7c2060d6442ddfd35f95d8512', '112505a1d86fb960b571cd6fd843a50e68830ad6', '91c183816c2f8aee9138cf211576ef16cb95ae99', 'f26c8f6754627b8d599e56480f49ab39e9b64dd7'], 'cited_paper_id': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]} {'citation_text': {'elmo': tensor([[[259,  80, 119,  ..., 261, 261, 261],\n",
      "         [259,  45, 260,  ..., 261, 261, 261],\n",
      "         [259, 117, 105,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  74, 111,  ..., 261, 261, 261],\n",
      "         [259, 100, 112,  ..., 261, 261, 261],\n",
      "         [259,  45, 260,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  66,  47,  ..., 261, 261, 261],\n",
      "         [259,  69, 112,  ..., 261, 261, 261],\n",
      "         [259,  45, 260,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [259,  45, 260,  ..., 261, 261, 261],\n",
      "         [259,  81, 115,  ..., 261, 261, 261],\n",
      "         [259,  47, 260,  ..., 261, 261, 261]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[259,  74, 117,  ..., 261, 261, 261],\n",
      "         [259, 106, 116,  ..., 261, 261, 261],\n",
      "         [259, 117, 105,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  84, 112,  ..., 261, 261, 261],\n",
      "         [259, 103,  98,  ..., 261, 261, 261],\n",
      "         [259,  46, 260,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [259, 103,  98,  ..., 261, 261, 261],\n",
      "         [259,  47, 260,  ..., 261, 261, 261],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  66, 109,  ..., 261, 261, 261],\n",
      "         [259, 117, 105,  ..., 261, 261, 261],\n",
      "         [259, 106, 116,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]]]), 'tokens': tensor([[  3509,      2,    370,   1736,     25,    808,     29,    607,    479,\n",
      "            112,   3756, 134393,  11995, 134394,    111,     49,     40,    757,\n",
      "              2,   1484,   8870,  40961,      2,  11995,      2,      7,    193,\n",
      "             10,    428,      3,      0,      0,      0,      0,      0,      0],\n",
      "        [    42,    412,      2,    101,   2006,    469,   4123,   3914,     11,\n",
      "           1728,   8487,   6474,     69,      2,     57,    413,    233,     28,\n",
      "           4565,     11,   3227,    414,     23,     92,     25,     84,    884,\n",
      "             11,    255,     10,      4,    457,      3,      0,      0,      0],\n",
      "        [  1557,  18585,      2,   1801,   7035,      2,   2013,   1801,   3014,\n",
      "              2,   2710, 118217,      7,   1948,   1533,  69914,      2, 118218,\n",
      "           3985,     24,    681,   3925,    626,  17362,     24,     16,  80222,\n",
      "          67446,   5288,     10,      4,    328,   2826,      2,  36463,      3],\n",
      "        [ 53354,    264,     48,    394,    329,   1384,     14,   2525,    329,\n",
      "           1384,      2,     33,     17,    394,    329,   1384,   1915,     24,\n",
      "            394,    870,     14,   7611,  11660,     14,      7,   7611,     13,\n",
      "             11,     13,   2432,    952,      3,      0,      0,      0,      0],\n",
      "        [   120,     53,   1355,  68581,  68615,     23,      4,  10218,    529,\n",
      "             13,   3549,     75,      7,   1552,      4,  68581,   2327,    983,\n",
      "             23,      4,   1921,    688,      6,      4,  49732,   4710,      9,\n",
      "           1499,      8, 141745,      3,      0,      0,      0,      0,      0],\n",
      "        [117834,  82436,     30,    112, 117835,    111,   2517,  26067,  82436,\n",
      "            137,   2169,     17,   2169,     18,     39,     43,     34,  15464,\n",
      "           1072,     24,    203,     20, 117836,      2,   7277,   4068,      2,\n",
      "         117837,      2,     30,   2169,  12049,      3,      0,      0,      0],\n",
      "        [    27,    146,    191,     18,     44,    409,     11,    728,     25,\n",
      "              4,    649,     58,      4,   6975,      7,      4,   3350,      2,\n",
      "              4,    985,      6,   6712,      2,    474,   4110,      2,   5419,\n",
      "            912,      2,      7,  10646,   8733,      3,      0,      0,      0],\n",
      "        [ 84041,      7,  58507,  84041,      7,  13020,     41,     28,    987,\n",
      "             24,   1445,    219,  33490,    101,   1427,    439,     17,   4555,\n",
      "             93,     16,    243,    399,      2,     19,     16,    548,      6,\n",
      "           1641,      9,    272,    229,      8,      3,      0,      0,      0],\n",
      "        [  5987,     31, 113035,    717,     13,   1523,    518,     26,   2658,\n",
      "              6,     35,    765,     72,    559,     10,    882,     72,    559,\n",
      "             54,      2,   3910,      7,    945,     54,      2,      7,    163,\n",
      "             54,      7,    239,   1694,     53,      3,      0,      0,      0],\n",
      "        [  1533, 132324,   5625,    689,      4,  36527,     17,     16,   5184,\n",
      "           5190,      2,     16,  41524,    233,     28,     46,    183,     52,\n",
      "             17,   2496,     49,     16,  38908,     30,     56,  18629,    990,\n",
      "            343,    189,      3,      0,      0,      0,      0,      0,      0],\n",
      "        [ 26293,  16155,    442,  24901,  80751,  53479,      9,  80752,      8,\n",
      "              2,   1782,  66783,   4307,  42671,   3844,   2645,  22401,  79642,\n",
      "           4320,  82880,  54312,     12,  16966, 119326,  29409, 119327, 119328,\n",
      "              9,    428,      3,      0,      0,      0,      0,      0,      0],\n",
      "        [   680,    346,     18,      4,    220,      6,   1851,  14557,     10,\n",
      "              4,    298,    558,    372,    622,     16,   1104,   1760,     29,\n",
      "           7828,     11,      4,  61891,   5925,   1145,      2,   1855,    590,\n",
      "           6547,    139,      3,      0,      0,      0,      0,      0,      0],\n",
      "        [ 40706,  26292,    304,     19,   5771,   1279,     74,   3299,      6,\n",
      "              4,    818,     26,     90,      6,      4,   1447,    471,     41,\n",
      "             28,    379,     10,      4,    306,      6,     16,  11931,    304,\n",
      "              2,      4,   2730,      6,   1447,      3,      0,      0,      0],\n",
      "        [   129,     17,     18,   2231,      6,    479,    285,     18,   1437,\n",
      "           1137,     11,   2232,     97,     19,    181,   1713,     18,   8563,\n",
      "             69,      4,   1343,    377,      7,    322,   3363,     19,      4,\n",
      "            324,    181,     54,      3,      0,      0,      0,      0,      0],\n",
      "        [ 23869,  62061,     13,   1456,      6,    662,    719,   1193,     13,\n",
      "             39,     16,   1466,   1463,     10,    441,      6,   1359,     35,\n",
      "           2927,     19,   6433,      2,     33,     41,    320,   1719,      4,\n",
      "            939,      6,  11192, 113751,   2105,    413,   3179,      3,      0],\n",
      "        [   293,     40,     17,   1231,    104,      6,      4,  59539, 127418,\n",
      "             11,   2923,      4,   1745,   1268,      2,     52,   1354,      4,\n",
      "           1788,     19,  84692,   2199,   3885,     18,     25,    701,     11,\n",
      "          26390,   2923,   1745,   1268,      3,      0,      0,      0,      0]])}, 'is_citation': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]), 'citing_paper_id': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'd297445dc6b372fb502d26a6f29828b3c160a60e', None], 'cited_paper_id': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.type = None\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.extend = False\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.directory_path = None\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.min_count = None\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.max_vocab_size = None\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.min_pretrained_embeddings = None\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.only_include_pretrained_words = False\n",
      "04/10/2024 22:35:58 - INFO - allennlp.common.params -   vocabulary.tokens_to_add = None\n",
      "04/10/2024 22:35:58 - INFO - scicite.training.vocabulary_multitask -   Fitting token dictionary from dataset.\n",
      "0it [00:00, ?it/s]\n",
      "94189it [00:04, 22152.85it/s]\n",
      "\n",
      "04/10/2024 22:36:03 - INFO - scicite.training.vocabulary_multitask -   Fitting token dictionary from auxillary dataset.\n",
      "  0%|          | 0/146968 [00:00<?, ?it/s]\n",
      "100%|##########| 146968/146968 [00:05<00:00, 28684.91it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_model(params, serialization_dir, file_friendly_logging, recover)\n",
    "\"\"\"\n",
    "Trains the model specified in the given :class:`Params` object, using the data and training\n",
    "parameters also specified in that object, and saves the results in ``serialization_dir``.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "params : ``Params``\n",
    "    A parameter object specifying an AllenNLP Experiment.\n",
    "serialization_dir : ``str``\n",
    "    The directory in which to save results and logs.\n",
    "file_friendly_logging : ``bool``, optional (default=False)\n",
    "    If ``True``, we add newlines to tqdm output, even on an interactive terminal, and we slow\n",
    "    down tqdm's output to only once every 10 seconds.\n",
    "recover : ``bool``, optional (default=False)\n",
    "    If ``True``, we will try to recover a training run from an existing serialization\n",
    "    directory.  This is only intended for use when something actually crashed during the middle\n",
    "    of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "best_model: ``Model``\n",
    "    The model with the best epoch weights.\n",
    "\"\"\"\n",
    "prepare_environment(params)\n",
    "\n",
    "create_serialization_dir(params, serialization_dir, recover)\n",
    "prepare_global_logging(serialization_dir, file_friendly_logging)\n",
    "\n",
    "check_for_gpu(params.get('trainer').get('cuda_device', -1))\n",
    "\n",
    "params.to_file(os.path.join(serialization_dir, CONFIG_NAME))\n",
    "\n",
    "all_datasets, all_datasets_aux, all_datasets_aux2 = datasets_from_params(params)\n",
    "# print(all_datasets)\n",
    "datasets_for_vocab_creation = set(params.pop(\"datasets_for_vocab_creation\", all_datasets))\n",
    "datasets_for_vocab_creation_aux = set(params.pop(\"auxiliary_datasets_for_vocab_creation\", all_datasets_aux))\n",
    "datasets_for_vocab_creation_aux2 = set(params.pop(\"auxiliary_datasets_for_vocab_creation_2\", all_datasets_aux2))\n",
    "\n",
    "\n",
    "mixing_ratio = params.pop_float(\"mixing_ratio\")\n",
    "mixing_ratio2 = params.pop_float(\"mixing_ratio2\")\n",
    "\n",
    "cutoff_epoch = params.pop(\"cutoff_epoch\", -1)\n",
    "\n",
    "for dataset in datasets_for_vocab_creation:\n",
    "    if dataset not in all_datasets:\n",
    "        raise ConfigurationError(f\"invalid 'dataset_for_vocab_creation' {dataset}\")\n",
    "\n",
    "logger.info(\"From dataset instances, %s will be considered for vocabulary creation.\",\n",
    "            \", \".join(datasets_for_vocab_creation))\n",
    "vocab_instances_aux = [\n",
    "    instance for key, dataset in all_datasets_aux.items()\n",
    "    for instance in dataset\n",
    "    if key in datasets_for_vocab_creation_aux\n",
    "]\n",
    "vocab_instances_aux.extend([\n",
    "    instance for key, dataset in all_datasets_aux2.items()\n",
    "    for instance in dataset\n",
    "    if key in datasets_for_vocab_creation_aux2\n",
    "])\n",
    "vocab = VocabularyMultitask.from_params(\n",
    "        params.pop(\"vocabulary\", {}),\n",
    "        (instance for key, dataset in all_datasets.items()\n",
    "         for instance in dataset\n",
    "         if key in datasets_for_vocab_creation),\n",
    "        instances_aux=vocab_instances_aux\n",
    ")\n",
    "model = Model.from_params(vocab=vocab, params=params.pop('model'))\n",
    "\n",
    "# Initializing the model can have side effect of expanding the vocabulary\n",
    "vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))\n",
    "\n",
    "iterator = DataIterator.from_params(params.pop(\"iterator\"))\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "iterator_aux = DataIterator.from_params(params.pop(\"iterator_aux\"))\n",
    "iterator_aux.index_with(vocab)\n",
    "\n",
    "iterator_aux2 = DataIterator.from_params(params.pop(\"iterator_aux2\"))\n",
    "iterator_aux2.index_with(vocab)\n",
    "\n",
    "validation_iterator_params = params.pop(\"validation_iterator\", None)\n",
    "if validation_iterator_params:\n",
    "    validation_iterator = DataIterator.from_params(validation_iterator_params)\n",
    "    validation_iterator.index_with(vocab)\n",
    "else:\n",
    "    validation_iterator = None\n",
    "\n",
    "# TODO: if validation in multi-task need to add validation iterator as above\n",
    "\n",
    "train_data = all_datasets.get('train')\n",
    "validation_data = all_datasets.get('validation')\n",
    "test_data = all_datasets.get('test')\n",
    "\n",
    "train_data_aux = all_datasets_aux.get('train_aux')\n",
    "validation_data_aux = all_datasets_aux.get('validation_aux')\n",
    "test_data_aux = all_datasets_aux.get('test_aux')\n",
    "\n",
    "train_data_aux2 = all_datasets_aux2.get('train_aux')\n",
    "validation_data_aux2 = all_datasets_aux2.get('validation_aux')\n",
    "test_data_aux2 = all_datasets_aux2.get('test_aux')\n",
    "\n",
    "trainer_params = params.pop(\"trainer\")\n",
    "no_grad_regexes = trainer_params.pop(\"no_grad\", ())\n",
    "for name, parameter in model.named_parameters():\n",
    "    if any(re.search(regex, name) for regex in no_grad_regexes):\n",
    "        parameter.requires_grad_(False)\n",
    "\n",
    "frozen_parameter_names, tunable_parameter_names = \\\n",
    "               get_frozen_and_tunable_parameter_names(model)\n",
    "logger.info(\"Following parameters are Frozen  (without gradient):\")\n",
    "for name in frozen_parameter_names:\n",
    "    logger.info(name)\n",
    "logger.info(\"Following parameters are Tunable (with gradient):\")\n",
    "for name in tunable_parameter_names:\n",
    "    logger.info(name)\n",
    "\n",
    "trainer = MultiTaskTrainer2.from_params(model=model,\n",
    "                                        serialization_dir=serialization_dir,\n",
    "                                        iterator=iterator,\n",
    "                                        iterator_aux=iterator_aux,\n",
    "                                        iterator_aux2=iterator_aux2,\n",
    "                                        train_data=train_data,\n",
    "                                        train_data_aux=train_data_aux,\n",
    "                                        train_data_aux2=train_data_aux2,\n",
    "                                        mixing_ratio=mixing_ratio,\n",
    "                                        mixing_ratio2=mixing_ratio2,\n",
    "                                        cutoff_epoch=cutoff_epoch,\n",
    "                                        validation_data_aux=validation_data_aux,\n",
    "                                        validation_data_aux2=validation_data_aux2,\n",
    "                                        validation_data=validation_data,\n",
    "                                        params=trainer_params,\n",
    "                                        validation_iterator=validation_iterator)\n",
    "# print(trainer._cuda_devices[0])\n",
    "evaluate_on_test = params.pop_bool(\"evaluate_on_test\", False)\n",
    "evaluate_aux_on_test = params.pop_bool(\"evaluate_aux_on_test\", False)\n",
    "params.assert_empty('base train command')\n",
    "'''\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    # if we have completed an epoch, try to create a model archive.\n",
    "    if os.path.exists(os.path.join(serialization_dir, _DEFAULT_WEIGHTS)):\n",
    "        logging.info(\"Training interrupted by the user. Attempting to create \"\n",
    "                     \"a model archive using the current best epoch weights.\")\n",
    "        archive_model(serialization_dir, files_to_archive=params.files_to_archive)\n",
    "    raise\n",
    "\n",
    "# Now tar up results\n",
    "archive_model(serialization_dir, files_to_archive=params.files_to_archive)\n",
    "\n",
    "logger.info(\"Loading the best epoch weights.\")\n",
    "best_model_state_path = os.path.join(serialization_dir, 'best.th')\n",
    "best_model_state = torch.load(best_model_state_path)\n",
    "best_model = model\n",
    "best_model.load_state_dict(best_model_state)\n",
    "\n",
    "if test_data and evaluate_on_test:\n",
    "    logger.info(\"The model will be evaluated using the best epoch weights.\")\n",
    "    test_metrics = evaluate(\n",
    "            best_model, test_data, validation_iterator or iterator,\n",
    "            cuda_device=trainer._cuda_devices[0] # pylint: disable=protected-access\n",
    "    )\n",
    "    for key, value in test_metrics.items():\n",
    "        metrics[\"test_\" + key] = value\n",
    "\n",
    "elif test_data:\n",
    "    logger.info(\"To evaluate on the test set after training, pass the \"\n",
    "                \"'evaluate_on_test' flag, or use the 'allennlp evaluate' command.\")\n",
    "\n",
    "if test_data_aux and evaluate_aux_on_test:\n",
    "    # for instance in test_data_aux:\n",
    "    #     instance.index_fields(vocab)\n",
    "    # for instance in test_data_aux2:\n",
    "    #     instance.index_fields(vocab)\n",
    "    test_metrics_aux = evaluate(best_model, test_data_aux, iterator_aux,\n",
    "                                cuda_device=trainer._cuda_devices[0])  # pylint: disable=protected-access\n",
    "    test_metrics_aux2 = evaluate(best_model, test_data_aux2, iterator_aux2,\n",
    "                                 cuda_device=trainer._cuda_devices[0])  # pylint: disable=protected-access\n",
    "\n",
    "    for key, value in test_metrics_aux.items():\n",
    "        metrics[\"test_aux_\" + key] = value\n",
    "    for key, value in test_metrics_aux2.items():\n",
    "        metrics[\"test_aux2_\" + key] = value\n",
    "\n",
    "elif test_data_aux:\n",
    "    logger.info(\"To evaluate on the auxiliary test set after training, pass the \"\n",
    "                \"'evaluate_on_test' flag, or use the 'allennlp evaluate' command.\")\n",
    "\n",
    "dump_metrics(os.path.join(serialization_dir, \"metrics.json\"), metrics, log=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac296ebc-f37b-46dd-a12e-789913357946",
   "metadata": {},
   "source": [
    "### Datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9dacd45-6634-4603-9376-8ff8544be1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params.from_file('experiment_configs/custom_config.json', \"\")\n",
    "data = datasets_from_params(params)\n",
    "# from scicite_datasetreader text_to_instance() -> Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eea471f-b389-4ace-93c1-f7b9a27bc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0]['train'][0].fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2715604f-5a13-4b62-96f1-dcd80ff71f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data[0]['train'][1].fields['citation_text'].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "231a6337-7afe-4271-90fc-ccc6197bbf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'background'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['train'][0].fields['labels'].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15ffded2-6a0f-41e4-a272-f5cb86535262",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Therefore, ,, in, our, case]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_instances_aux[0].fields['citation_text'].tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a9a93e5-d6b5-4f3a-ae2a-cb257c36ce10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab._token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4013d645-b4b8-4a22-b2fd-91d461a5d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = DataIterator.from_params(params.pop(\"iterator\"))\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "iterator_aux = DataIterator.from_params(params.pop(\"iterator_aux\"))\n",
    "iterator_aux.index_with(vocab)\n",
    "\n",
    "iterator_aux2 = DataIterator.from_params(params.pop(\"iterator_aux2\"))\n",
    "iterator_aux2.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b7772b9-532a-4827-9165-d66b60a46d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.iterators.bucket_iterator.BucketIterator at 0x7f13636ff1d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1048237f-4ef0-447d-98cb-2ddc8f7274d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data[0].fields['citation_text'].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e19caa3-dc96-4f74-b4db-84ee7c053ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_shuffle = True\n",
    "train_generator = iterator(train_data,\n",
    "                                 num_epochs=1,\n",
    "                                 shuffle=_shuffle)\n",
    "\n",
    "train_generator_aux = iterator_aux(train_data_aux,\n",
    "                                         num_epochs=1,\n",
    "                                         shuffle=_shuffle)\n",
    "train_generator_aux2 = iterator_aux2(train_data_aux2,\n",
    "                                      num_epochs=1,\n",
    "                                          shuffle=_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dc401b9-3d89-4a7c-8c53-629cdaeb7034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataIterator.__call__ at 0x7f1370749db0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8882ab83-dcd0-4a70-ae82-f5de1e680045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_training_batches=1\n",
    "train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                 total=num_training_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25dc9b3b-ce7c-4467-be85-163fbf399485",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch_aux, batch_aux2 in zip(train_generator_tqdm, train_generator_aux, train_generator_aux2):\n",
    "    print(batch, batch_aux, batch_aux2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c4acc90-7bb8-4977-bb07-73b7ad605d6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elmo': tensor([[[259,  87, 106,  ..., 261, 261, 261],\n",
       "          [259,  66, 260,  ..., 261, 261, 261],\n",
       "          [259, 116, 118,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  71, 112,  ..., 261, 261, 261],\n",
       "          [259,  72, 118,  ..., 261, 261, 261],\n",
       "          [259,  98, 111,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  72, 106,  ..., 261, 261, 261],\n",
       "          [259, 117, 105,  ..., 261, 261, 261],\n",
       "          [259, 108, 111,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [259, 117, 105,  ..., 261, 261, 261],\n",
       "          [259, 101, 106,  ..., 261, 261, 261],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[259,  85, 105,  ..., 261, 261, 261],\n",
       "          [259, 115, 102,  ..., 261, 261, 261],\n",
       "          [259, 112, 103,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  66, 116,  ..., 261, 261, 261],\n",
       "          [259,  98, 260,  ..., 261, 261, 261],\n",
       "          [259, 113,  98,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  72,  71,  ..., 261, 261, 261],\n",
       "          [259, 103, 109,  ..., 261, 261, 261],\n",
       "          [259, 112, 103,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]]]),\n",
       " 'tokens': tensor([[22229,    88,  3834,     9, 23939,   369,  1717,     4,  1425,     5,\n",
       "          31596,   283,     8,   129,     7,     2, 26269,  4447,     8,   128,\n",
       "              7,     2, 21872,     8,   169,     2,   191,     7,     2,     6,\n",
       "           6433,    12,    10,    12,  1434,  1029,     5,  2059,     8,  2069,\n",
       "              3,     0,     0,     0,     0,     0],\n",
       "         [ 1524, 29006,     6, 29007,     8,   118,     7,     2,    15,  2006,\n",
       "             12,  2083,   351,  2954,   794,    31,    45,    10,  1182,   475,\n",
       "           3799,  2006,    12,  4127,    25,     4, 20456,  1028,  2633,     2,\n",
       "             22,   882,     9,     4,  1277, 29008,   174,     8, 29009,   316,\n",
       "              7,     3,     0,     0,     0,     0],\n",
       "         [ 1243,     4,   213, 20214,     9,  1272,   352,     5, 10668,     9,\n",
       "            369,    19,  2799,     8, 12535,    11,    14,     3,   274,    13,\n",
       "            541,    11,    14,     3,    89,    13,  9560,    11,    14,     3,\n",
       "             61,    13, 11528,    11,    14,     3,    70,     7,     2,    77,\n",
       "             66,  6344,    17,     4, 30384,     0],\n",
       "         [    8,   662,     7,   259,    17,  7205,   465,    19,   937,   359,\n",
       "          22865,    31,   139,    10,    17,   465,    19, 38690,   310,   493,\n",
       "            287,     9, 23465,    12,  7393,   412,     2,    32,   765,    17,\n",
       "              4, 20683,   727,    16,   270,   310,    50,   287,     3,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  153,    16,   713,  3700,    17, 16713,   917,    15,  1865,   219,\n",
       "              9,     4,  2282,     5,  5170,   879,     2,    50,    22,     4,\n",
       "           3134,   586,  1432,     9,   340,  3064,  2644,     6, 14925,     5,\n",
       "          40130,     9,  1201,    91,     8,   240,     2,   224,     7,     3,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [    8,   316,     7,   238,    17,  4795, 13180,  1239,   527,    19,\n",
       "             15,  3049,   325,  2256,     2, 13181,    12, 13182,    21, 13183,\n",
       "              8, 13184,    12, 12909,     7,     2,    38,   975,    18,  2449,\n",
       "              6,  1063,  1901,     6,  4400,     9,   412,     8,  3271,    11,\n",
       "             14,     3,     2,   316,     7,     3],\n",
       "         [   93,  1851,    71,  2037,    28,    15,   184,    62,    17,    82,\n",
       "            387,   381,  1809,     5,  3917,     5,  4906,   789,     9,  1065,\n",
       "           1156,   121,  1638,   119,    31,   228,    19,   612,   348,    67,\n",
       "            121,    51,    31,  3153,     8, 17065,     2,    61,     7,     3,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [   93,   338,    92,    17,  1100,     8,  7204,    11,    14,     3,\n",
       "              2,   113,     7,    22,   103,    22,   161,     8,  7204,    11,\n",
       "             14,     3,    85,     7,  3016,    24,  1546,     9,  6647,  1372,\n",
       "              6,   180,     2,  1977,    15,   628,   219,    18,     4,  6647,\n",
       "            472,     9,  2451,  6249,     3,     0],\n",
       "         [   41, 22882,   928,     2,  2516,     8,  3101,     7, 20530,   551,\n",
       "          22883,    24,  4954,    28,     4,  3104,     2,     6,   917,    34,\n",
       "           1122,   219,     9,  9384, 15738,  3455,  3101,     9,     4, 15362,\n",
       "              8, 12569,    11,    14,     3,    97,    13, 10360,    11,    14,\n",
       "              3,    97,     7,     3,     0,     0],\n",
       "         [  144,    52,    37,  8701,    19, 15584,    11,    14,     3,     2,\n",
       "              8,    83,     7,   199,   192,  6080,  1541,     5, 12807,   434,\n",
       "             31,   214,   231,    10,   737,   434,     6,    82,   176,   241,\n",
       "            186,     5,  1149,    25,  6080,  1541,     9, 10243,     3,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  770,     5,    65,   356,   922,   377,     9,  1112,  6055,     6,\n",
       "           3341,  8509,     2,    50,    22,  4218,   752,    12, 27146,     8,\n",
       "          12109,     7,     2, 12593, 12594,  1633,     8, 32964,     7,     6,\n",
       "          14936,    12,  6010,  8578,    21,  1026,     2,  1898,     2,  1455,\n",
       "             20,     3,     0,     0,     0,     0],\n",
       "         [ 1484,     2,   184,   126,    23, 23790,     2, 19207,     6, 16138,\n",
       "             21,    72,    20,   520,    34,  1470,  1468,    25,  1722,    12,\n",
       "           1036,     5, 23791,  1962,    57,     7,    18,   127,  1744,    12,\n",
       "           2542,   310,     4,  3171,     2,     9,    15,   700, 15737,  2559,\n",
       "              2,   525,    34, 11251,   215,     3],\n",
       "         [   26,   561, 12851,  3174,     9,     4, 32876,    38,   275,    33,\n",
       "            292,    10,    27,     4, 15263,    21,   129,    20,     2,     6,\n",
       "              4,   195, 21902,  8081,     6,  3691,    24,  1228,    10,    27,\n",
       "              4,   561,  4499,     5,  2486,     9,     4, 35181,    21,    57,\n",
       "              2,    72,    20,     3,     0,     0],\n",
       "         [   26,    52,     5,  5971,    11,    14,     3,     8,    76,     7,\n",
       "             46,   362,    17,   145,    16,    15,   250,   114,   675,    58,\n",
       "              4,  1429,  2920,     6,     4,  2524,  1129,     5,     4,  1472,\n",
       "              6,  1103,   751,    18,    96,     4,  5326,     5,     4,  5522,\n",
       "           9105,     3,     0,     0,     0,     0],\n",
       "         [  205,    15,   377,     5,  1841,  1168,   505,    18,     4,    62,\n",
       "           2770,  3367,  3330,   817,    48,     4, 30583,    21,   169,    20,\n",
       "              6,  4881,  1180,    48,     4,  2625, 12151,    10,  1589,  1858,\n",
       "          20275,  6334,   166, 15617, 30584, 18365,     8, 18366,     7,    21,\n",
       "            191,    20,     3,     0,     0,     0],\n",
       "         [ 2663,  3123,     5, 24845,    91,  2680,  2663,    12, 24846,     6,\n",
       "              4,  3044,  2622, 24847,    12, 12018,     7,     2,  3170,   989,\n",
       "           1259,    57,    12,   143,     2,     6,  2663,    12, 24848,    12,\n",
       "          12018,     7,     2,  3170,   989,  1259,    57,    12,   726,     3,\n",
       "              0,     0,     0,     0,     0,     0]])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch['citation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ec435d7-1a15-414d-bff4-bea8852dd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict['prediction'] = labels\n",
    "output_dict = {}\n",
    "citation_text = []\n",
    "for batch_text in batch['citation_text']['tokens']:\n",
    "    citation_text.append([vocab.get_token_from_index(token_id.item()) for token_id in batch_text])\n",
    "# output_dict['citation_text'] = citation_text\n",
    "# output_dict['all_labels'] = [vocab.get_index_to_token_vocabulary(namespace=\"labels\")\n",
    "#                              for _ in range(output_dict['logits'].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ed782dd-7a5c-4a41-a8c5-cbe0343c5bf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Slant',\n",
       "  'column',\n",
       "  'NO2',\n",
       "  'is',\n",
       "  'retrieved',\n",
       "  'using',\n",
       "  'a',\n",
       "  'DOAS',\n",
       "  'linear',\n",
       "  'least',\n",
       "  'squares',\n",
       "  'fit',\n",
       "  '(',\n",
       "  'Platt',\n",
       "  'and',\n",
       "  'Stutz',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ';',\n",
       "  'Wenig',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  'as',\n",
       "  'in',\n",
       "  'the',\n",
       "  'operational',\n",
       "  'DOAS',\n",
       "  'retrievals',\n",
       "  '(',\n",
       "  'Boersma',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '2007;10',\n",
       "  'Bucsela',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['This',\n",
       "  'explanation',\n",
       "  'is',\n",
       "  'consistent',\n",
       "  'with',\n",
       "  'the',\n",
       "  'observation',\n",
       "  'that',\n",
       "  'Swiss',\n",
       "  'mice',\n",
       "  'were',\n",
       "  'unable',\n",
       "  'to',\n",
       "  'respond',\n",
       "  'to',\n",
       "  'surgical',\n",
       "  'removal',\n",
       "  'of',\n",
       "  'some',\n",
       "  'of',\n",
       "  'their',\n",
       "  'mammary',\n",
       "  'tissue',\n",
       "  'by',\n",
       "  'elevation',\n",
       "  'of',\n",
       "  'milk',\n",
       "  'production',\n",
       "  'in',\n",
       "  'the',\n",
       "  'remaining',\n",
       "  'tissue',\n",
       "  '(',\n",
       "  'Hammond',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  'which',\n",
       "  'is',\n",
       "  'also',\n",
       "  'consistent',\n",
       "  'with',\n",
       "  'the',\n",
       "  'peripheral',\n",
       "  'limitation',\n",
       "  'hypothesis',\n",
       "  '.',\n",
       "  '@@PADDING@@'],\n",
       " ['Second',\n",
       "  ',',\n",
       "  'HASM',\n",
       "  'cells',\n",
       "  'in',\n",
       "  'culture',\n",
       "  ',',\n",
       "  'when',\n",
       "  'observed',\n",
       "  'between',\n",
       "  '@@NUM@@',\n",
       "  'and',\n",
       "  '@@NUM@@',\n",
       "  'h',\n",
       "  'after',\n",
       "  'plating',\n",
       "  ',',\n",
       "  'were',\n",
       "  'not',\n",
       "  'spindle',\n",
       "  'shaped',\n",
       "  'or',\n",
       "  'aligned',\n",
       "  'in',\n",
       "  'parallel',\n",
       "  ',',\n",
       "  'as',\n",
       "  'they',\n",
       "  'are',\n",
       "  'at',\n",
       "  'the',\n",
       "  'tissue',\n",
       "  'level',\n",
       "  '(',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  ';',\n",
       "  'instead',\n",
       "  ',',\n",
       "  'they',\n",
       "  'were',\n",
       "  'irregularly',\n",
       "  'shaped',\n",
       "  '(',\n",
       "  'Fig',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['ORPL',\n",
       "  'also',\n",
       "  'outperforms',\n",
       "  'other',\n",
       "  'stateof',\n",
       "  '-',\n",
       "  'the',\n",
       "  '-',\n",
       "  'art',\n",
       "  'solutions',\n",
       "  'such',\n",
       "  'as',\n",
       "  'the',\n",
       "  'Collection',\n",
       "  'Tree',\n",
       "  'Protocol',\n",
       "  '(',\n",
       "  'CTP',\n",
       "  ')',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  ',',\n",
       "  'Opportunistic',\n",
       "  'Routing',\n",
       "  'for',\n",
       "  'WSN',\n",
       "  '(',\n",
       "  'ORW',\n",
       "  ')',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  'or',\n",
       "  'the',\n",
       "  'Low',\n",
       "  '-',\n",
       "  'Power',\n",
       "  'Wireless',\n",
       "  'Bus',\n",
       "  '(',\n",
       "  'LWB',\n",
       "  ')',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['Skeletal',\n",
       "  'muscle',\n",
       "  'is',\n",
       "  'also',\n",
       "  'a',\n",
       "  'primary',\n",
       "  'site',\n",
       "  'of',\n",
       "  'disease',\n",
       "  'in',\n",
       "  'mouse',\n",
       "  'models',\n",
       "  'of',\n",
       "  'ALS',\n",
       "  '(',\n",
       "  'Dobrowolny',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ';',\n",
       "  'Wong',\n",
       "  'and',\n",
       "  'Martin',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ';',\n",
       "  'Luo',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  'and',\n",
       "  'possibly',\n",
       "  'in',\n",
       "  'human',\n",
       "  'ALS',\n",
       "  '(',\n",
       "  'Vielhaber',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['@@NUM@@',\n",
       "  'D',\n",
       "  'or',\n",
       "  'more',\n",
       "  'in',\n",
       "  'some',\n",
       "  'situations.(21,22',\n",
       "  ')',\n",
       "  'The',\n",
       "  'tolerance',\n",
       "  'for',\n",
       "  'position',\n",
       "  'error',\n",
       "  'for',\n",
       "  'toric',\n",
       "  'andmultifocal',\n",
       "  'IOLs',\n",
       "  'is',\n",
       "  'even',\n",
       "  'less',\n",
       "  ',',\n",
       "  'so',\n",
       "  'small',\n",
       "  'amounts',\n",
       "  'of',\n",
       "  'tilt',\n",
       "  ',',\n",
       "  'decentration',\n",
       "  ',',\n",
       "  'or',\n",
       "  'rotation',\n",
       "  'of',\n",
       "  'these',\n",
       "  'IOLs',\n",
       "  'can',\n",
       "  'result',\n",
       "  'in',\n",
       "  'significant',\n",
       "  'deviations',\n",
       "  'from',\n",
       "  'the',\n",
       "  'desired',\n",
       "  'refractive',\n",
       "  'outcome',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'symptomatic',\n",
       "  'visual',\n",
       "  'aberrations',\n",
       "  '.'],\n",
       " ['2D',\n",
       "  'and',\n",
       "  '(',\n",
       "  'Klingner',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ';',\n",
       "  'Shmuel',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  ')',\n",
       "  ',',\n",
       "  'led',\n",
       "  'us',\n",
       "  'to',\n",
       "  'investigate',\n",
       "  'the',\n",
       "  'contribution',\n",
       "  'of',\n",
       "  'local',\n",
       "  'and',\n",
       "  'global',\n",
       "  'intrinsic',\n",
       "  'vascular',\n",
       "  'signals',\n",
       "  'to',\n",
       "  'BOLD',\n",
       "  'response',\n",
       "  'variabilitymeasured',\n",
       "  'from',\n",
       "  'the',\n",
       "  'bilateral',\n",
       "  'sensory',\n",
       "  'network',\n",
       "  'across',\n",
       "  'all',\n",
       "  'experiments',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['Often',\n",
       "  ',',\n",
       "  'multiple',\n",
       "  'interventions',\n",
       "  'are',\n",
       "  'bundled',\n",
       "  'by',\n",
       "  'a',\n",
       "  'single',\n",
       "  'artifact',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  'an',\n",
       "  '',\n",
       "  'artificial',\n",
       "  'thing',\n",
       "  '',\n",
       "  '(',\n",
       "  'Romme',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ',',\n",
       "  'p.',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  'that',\n",
       "  'serves',\n",
       "  'as',\n",
       "  '',\n",
       "  'a',\n",
       "  'means',\n",
       "  'to',\n",
       "  'an',\n",
       "  'end',\n",
       "  '',\n",
       "  '(',\n",
       "  'Holmstrm',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ',',\n",
       "  'p.',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['In',\n",
       "  'Wales',\n",
       "  '(',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  'there',\n",
       "  'were',\n",
       "  'nonsignificantly',\n",
       "  'higher',\n",
       "  'levels',\n",
       "  'of',\n",
       "  'plasma',\n",
       "  'androstenedione',\n",
       "  'in',\n",
       "  'cases',\n",
       "  'than',\n",
       "  'in',\n",
       "  'controls',\n",
       "  ';',\n",
       "  'in',\n",
       "  'South',\n",
       "  'African',\n",
       "  'blacks',\n",
       "  '(',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  'cases',\n",
       "  'had',\n",
       "  'significantly',\n",
       "  'lower',\n",
       "  'levels',\n",
       "  'than',\n",
       "  'controls',\n",
       "  'and',\n",
       "  'in',\n",
       "  'Finland',\n",
       "  '(',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  'cases',\n",
       "  'and',\n",
       "  'controls',\n",
       "  'had',\n",
       "  'similar',\n",
       "  'levels',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['Elevated',\n",
       "  'serum',\n",
       "  'BAFF',\n",
       "  'levels',\n",
       "  'are',\n",
       "  'involved',\n",
       "  'in',\n",
       "  'the',\n",
       "  'pathogenesis',\n",
       "  'of',\n",
       "  'B',\n",
       "  'cell',\n",
       "  '-',\n",
       "  'mediated',\n",
       "  'autoimmune',\n",
       "  'diseases',\n",
       "  'such',\n",
       "  'as',\n",
       "  'systemic',\n",
       "  'lupus',\n",
       "  'erythematosus',\n",
       "  '(',\n",
       "  'SLE',\n",
       "  ')',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  ',',\n",
       "  'multiple',\n",
       "  'sclerosis',\n",
       "  '(',\n",
       "  'MS',\n",
       "  ')',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  ',',\n",
       "  'systemic',\n",
       "  'sclerosis',\n",
       "  '(',\n",
       "  'SS',\n",
       "  ')',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  'and',\n",
       "  'RA',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  '.'],\n",
       " ['On',\n",
       "  'the',\n",
       "  'other',\n",
       "  'hand',\n",
       "  ',',\n",
       "  'SRC',\n",
       "  'activation',\n",
       "  'by',\n",
       "  'BCAR3',\n",
       "  '(',\n",
       "  '@@NUM@@',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  'has',\n",
       "  'been',\n",
       "  'recently',\n",
       "  'shown',\n",
       "  'to',\n",
       "  'promote',\n",
       "  'phosphorylation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Tyr-789',\n",
       "  'motif',\n",
       "  'of',\n",
       "  'PTP',\n",
       "  ',',\n",
       "  'a',\n",
       "  'receptor',\n",
       "  '-',\n",
       "  'type',\n",
       "  'phosphatase',\n",
       "  'that',\n",
       "  'can',\n",
       "  'potentiate',\n",
       "  'SRC',\n",
       "  'activation',\n",
       "  'by',\n",
       "  'dephosphorylating',\n",
       "  'its',\n",
       "  'inhibitory',\n",
       "  'Tyr-527',\n",
       "  'phosphorylation',\n",
       "  'site',\n",
       "  '(',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['@@NUM@@',\n",
       "  ')',\n",
       "  'and',\n",
       "  'is',\n",
       "  'involved',\n",
       "  'in',\n",
       "  'the',\n",
       "  'inactivation',\n",
       "  'of',\n",
       "  'extrinsic',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'intrinsic',\n",
       "  'apoptosis',\n",
       "  'pathways',\n",
       "  ',',\n",
       "  'by',\n",
       "  'interacting',\n",
       "  'with',\n",
       "  'pro',\n",
       "  '-',\n",
       "  'apoptotic',\n",
       "  'proteins',\n",
       "  'like',\n",
       "  'p53',\n",
       "  ',',\n",
       "  'Bcl-2',\n",
       "  ',',\n",
       "  'Bax',\n",
       "  ',',\n",
       "  'Bad',\n",
       "  ',',\n",
       "  'PUMA',\n",
       "  ',',\n",
       "  'MSH2',\n",
       "  ',',\n",
       "  'MSH6',\n",
       "  'and',\n",
       "  'others',\n",
       "  '(',\n",
       "  'Ludwig',\n",
       "  '-',\n",
       "  'Galezowska',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  '.',\n",
       "  '@@PADDING@@'],\n",
       " ['It',\n",
       "  'is',\n",
       "  'particularly',\n",
       "  'used',\n",
       "  'for',\n",
       "  'the',\n",
       "  'treatment',\n",
       "  'of',\n",
       "  'ovarian',\n",
       "  ',',\n",
       "  'testicular',\n",
       "  ',',\n",
       "  'esophageal',\n",
       "  ',',\n",
       "  'head',\n",
       "  'and',\n",
       "  'neck',\n",
       "  'cancers',\n",
       "  '(',\n",
       "  'Liu',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ';',\n",
       "  'Mansour',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ';',\n",
       "  'Zicca',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ';',\n",
       "  'Hanigan',\n",
       "  'and',\n",
       "  'Devarajan',\n",
       "  ',',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['These',\n",
       "  'results',\n",
       "  'were',\n",
       "  'evident',\n",
       "  'when',\n",
       "  'comparing',\n",
       "  'levels',\n",
       "  'in',\n",
       "  'fasting',\n",
       "  'pregnant',\n",
       "  'women',\n",
       "  'with',\n",
       "  'non',\n",
       "  '-',\n",
       "  'fasting',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  '-',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  ',',\n",
       "  'non',\n",
       "  '-',\n",
       "  'pregnant',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  ',',\n",
       "  'or',\n",
       "  'lactating',\n",
       "  'women',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  'or',\n",
       "  'even',\n",
       "  'with',\n",
       "  'their',\n",
       "  'own',\n",
       "  'levels',\n",
       "  'measured',\n",
       "  'before',\n",
       "  'and',\n",
       "  'after',\n",
       "  'breaking',\n",
       "  'fast',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  '.',\n",
       "  '@@PADDING@@'],\n",
       " ['However',\n",
       "  ',',\n",
       "  'how',\n",
       "  'frataxin',\n",
       "  'interacts',\n",
       "  'with',\n",
       "  'the',\n",
       "  'Fe',\n",
       "  '-',\n",
       "  'S',\n",
       "  'cluster',\n",
       "  'biosynthesis',\n",
       "  'components',\n",
       "  'remains',\n",
       "  'unclear',\n",
       "  'as',\n",
       "  'direct',\n",
       "  '@@NUM@@',\n",
       "  '-',\n",
       "  'to',\n",
       "  '-',\n",
       "  '@@NUM@@',\n",
       "  'interactions',\n",
       "  'with',\n",
       "  'each',\n",
       "  'component',\n",
       "  'were',\n",
       "  'reported',\n",
       "  '(',\n",
       "  'IscS',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  ',',\n",
       "  'IscU',\n",
       "  '/',\n",
       "  'Isu1',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  'or',\n",
       "  'ISD11/Isd11',\n",
       "  '[',\n",
       "  '@@NUM@@',\n",
       "  ']',\n",
       "  ')',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@'],\n",
       " ['Homeostatic',\n",
       "  'Model',\n",
       "  'Assessment',\n",
       "  '',\n",
       "  'Insulin',\n",
       "  'Resistance',\n",
       "  '(',\n",
       "  'HOMA',\n",
       "  '-',\n",
       "  'IR',\n",
       "  ')',\n",
       "  'was',\n",
       "  'calculated',\n",
       "  'as',\n",
       "  'follows',\n",
       "  ':',\n",
       "  'HOMA',\n",
       "  '-',\n",
       "  'IR',\n",
       "  '=',\n",
       "  'fasting',\n",
       "  'glucose',\n",
       "  'value',\n",
       "  '(',\n",
       "  'mg',\n",
       "  '/',\n",
       "  'dl',\n",
       "  ')',\n",
       "  '',\n",
       "  'fasting',\n",
       "  'insulin',\n",
       "  'value',\n",
       "  '(',\n",
       "  'U',\n",
       "  '/',\n",
       "  'ml)/405',\n",
       "  '(',\n",
       "  'Matthews',\n",
       "  'et',\n",
       "  'al',\n",
       "  '.',\n",
       "  '@@NUM@@',\n",
       "  ')',\n",
       "  '.',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@',\n",
       "  '@@PADDING@@']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b9a2e-67a7-4bdb-bda8-36a629fccd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ea13a6-fdf2-4cac-afd3-64660e7ccb22",
   "metadata": {},
   "source": [
    "## Datareader convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e101f4a2-e56c-4ecd-aa7d-21860bec4ea6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PretrainedTransformerIndexer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-fa342579c1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_indexers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleIdTokenIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mELMoTokenCharactersIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedTransformerIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscicite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_ACTION_LEXICONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALL_CONCEPT_LEXICONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PretrainedTransformerIndexer'"
     ]
    }
   ],
   "source": [
    "\"\"\" Data reader for AllenNLP \"\"\"\n",
    "\n",
    "\n",
    "from typing import Dict, List\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from allennlp.data import Field\n",
    "from overrides import overrides\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, MultiLabelField, ListField, ArrayField, MetadataField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, ELMoTokenCharactersIndexer, PretrainedTransformerIndexer\n",
    "\n",
    "from scicite.resources.lexicons import ALL_ACTION_LEXICONS, ALL_CONCEPT_LEXICONS\n",
    "from scicite.data import DataReaderJurgens\n",
    "from scicite.data import DataReaderS2, DataReaderS2ExcerptJL\n",
    "from scicite.compute_features import is_in_lexicon\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "from scicite.constants import S2_CATEGORIES, NONE_LABEL_NAME\n",
    "\n",
    "\n",
    "# @DatasetReader.register(\"scicite_datasetreader\")\n",
    "class SciciteDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads a JSON-lines file containing citations from the Semantic Scholar database, and creates a\n",
    "    dataset suitable for document classification using these papers.\n",
    "\n",
    "    The output of ``read`` is a list of ``Instance`` s with the fields:\n",
    "        citation_text: ``TextField``\n",
    "        label: ``LabelField``\n",
    "\n",
    "    where the ``label`` is derived from the methodology/comparison labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lazy : ``bool`` (optional, default=False)\n",
    "        Passed to ``DatasetReader``.  If this is ``True``, training will start sooner, but will\n",
    "        take longer per batch.  This also allows training with datasets that are too large to fit\n",
    "        in memory.\n",
    "    tokenizer : ``Tokenizer``, optional\n",
    "        Tokenizer to use to split the title and abstrct into words or other kinds of tokens.\n",
    "        Defaults to ``WordTokenizer()``.\n",
    "    token_indexers : ``Dict[str, TokenIndexer]``, optional\n",
    "        Indexers used to define input token representations. Defaults to ``{\"tokens\":\n",
    "        SingleIdTokenIndexer()}``.\n",
    "    reader_format : can be `flat` or `nested`. `flat` for flat json format and nested for\n",
    "        Json format where the each object contains multiple excerpts\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 use_lexicon_features: bool=False,\n",
    "                 use_sparse_lexicon_features: bool = False,\n",
    "                 multilabel: bool = False,\n",
    "                 with_elmo: bool = False,\n",
    "                 reader_format: str = 'flat') -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer() # using WordTokenizer() because config['tokenizer'] not specified \n",
    "        if with_elmo:\n",
    "            # self._token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "            self._token_indexers = {\"elmo\": ELMoTokenCharactersIndexer(),\n",
    "                                    \"tokens\": SingleIdTokenIndexer()}\n",
    "        else:\n",
    "            self._token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "        self.use_lexicon_features = use_lexicon_features\n",
    "        self.use_sparse_lexicon_features = use_sparse_lexicon_features\n",
    "        if self.use_lexicon_features or self.use_sparse_lexicon_features:\n",
    "            self.lexicons = {**ALL_ACTION_LEXICONS, **ALL_CONCEPT_LEXICONS}\n",
    "        self.multilabel = multilabel\n",
    "        self.reader_format = reader_format\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, jsonl_file: str):\n",
    "        if self.reader_format == 'flat':\n",
    "            reader_s2 = DataReaderS2ExcerptJL(jsonl_file)\n",
    "        elif self.reader_format == 'nested':\n",
    "            reader_s2 = DataReaderS2(jsonl_file)\n",
    "        for citation in reader_s2.read():\n",
    "            yield self.text_to_instance(\n",
    "                citation_text=citation.text,\n",
    "                intent=citation.intent,\n",
    "                citing_paper_id=citation.citing_paper_id,\n",
    "                cited_paper_id=citation.cited_paper_id,\n",
    "                citation_excerpt_index=citation.citation_excerpt_index\n",
    "            )\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self,\n",
    "                         citation_text: str,\n",
    "                         citing_paper_id: str,\n",
    "                         cited_paper_id: str,\n",
    "                         intent: List[str] = None,\n",
    "                         citing_paper_title: str = None,\n",
    "                         cited_paper_title: str = None,\n",
    "                         citing_paper_year: int = None,\n",
    "                         cited_paper_year: int = None,\n",
    "                         citing_author_ids: List[str] = None,\n",
    "                         cited_author_ids: List[str] = None,\n",
    "                         extended_context: str = None,\n",
    "                         section_number: int = None,\n",
    "                         section_title: str = None,\n",
    "                         cite_marker_begin: int = None,\n",
    "                         cite_marker_end: int = None,\n",
    "                         sents_before: List[str] = None,\n",
    "                         sents_after: List[str] = None,\n",
    "                         cleaned_cite_text: str = None,\n",
    "                         citation_excerpt_index: str = None,\n",
    "                         venue: str = None) -> Instance:  # type: ignore\n",
    "\n",
    "        citation_tokens = self._tokenizer.tokenize(citation_text)\n",
    "\n",
    "        fields = {\n",
    "            'citation_text': TextField(citation_tokens, self._token_indexers),\n",
    "        }\n",
    "\n",
    "        if self.use_sparse_lexicon_features:\n",
    "            # convert to regular string\n",
    "            sent = [token.text.lower() for token in citation_tokens]\n",
    "            lexicon_features, _ = is_in_lexicon(self.lexicons, sent)\n",
    "            fields[\"lexicon_features\"] = ListField([LabelField(feature, skip_indexing=True)\n",
    "                                                    for feature in lexicon_features])\n",
    "\n",
    "        if intent:\n",
    "            if self.multilabel:\n",
    "                fields['labels'] = MultiLabelField([S2_CATEGORIES[e] for e in intent], skip_indexing=True,\n",
    "                                                   num_labels=len(S2_CATEGORIES))\n",
    "            else:\n",
    "                if not isinstance(intent, str):\n",
    "                    raise TypeError(f\"Undefined label format. Should be a string. Got: f'{intent}'\")\n",
    "                fields['labels'] = LabelField(intent)\n",
    "\n",
    "        if citing_paper_year and cited_paper_year and \\\n",
    "                citing_paper_year > -1 and cited_paper_year > -1:\n",
    "            year_diff = citing_paper_year - cited_paper_year\n",
    "        else:\n",
    "            year_diff = -1\n",
    "        fields['year_diff'] = ArrayField(torch.Tensor([year_diff]))\n",
    "        fields['citing_paper_id'] = MetadataField(citing_paper_id)\n",
    "        fields['cited_paper_id'] = MetadataField(cited_paper_id)\n",
    "        fields['citation_excerpt_index'] = MetadataField(citation_excerpt_index)\n",
    "        fields['citation_id'] = MetadataField(f\"{citing_paper_id}>{cited_paper_id}\")\n",
    "        return Instance(fields)\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, params: Params) -> 'SciciteDatasetReader':\n",
    "        lazy = params.pop('lazy', False)\n",
    "        tokenizer = Tokenizer.from_params(params.pop('tokenizer', {}))\n",
    "        use_lexicon_features = params.pop_bool(\"use_lexicon_features\", False)\n",
    "        use_sparse_lexicon_features = params.pop_bool(\"use_sparse_lexicon_features\", False)\n",
    "        multilabel = params.pop_bool(\"multilabel\")\n",
    "        with_elmo = params.pop_bool(\"with_elmo\", False)\n",
    "        reader_format = params.pop(\"reader_format\", 'flat')\n",
    "        params.assert_empty(cls.__name__)\n",
    "        return cls(lazy=lazy, tokenizer=tokenizer,\n",
    "                   use_lexicon_features=use_lexicon_features,\n",
    "                   use_sparse_lexicon_features=use_sparse_lexicon_features,\n",
    "                   multilabel=multilabel,\n",
    "                   with_elmo=with_elmo,\n",
    "                   reader_format=reader_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02176e8b-bb4e-40df-abb4-d9161130a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_text = \"this is a happy pancake (123)!\"\n",
    "citation_tokens = WordTokenizer().tokenize(citation_text)\n",
    "special_token = \"<NUM>\"\n",
    "citation_tokens_preprocessed = [special_token if x.like_num else x for x in citation_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7c65496-2631-42d9-ba2e-2f32e67994ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokenize() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-31c6e0c9feeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcitation_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcitation_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitation_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tokenize() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "# if self.convert_num:\n",
    "# citation_tokens = [self.NUM_TOKEN if x.like_num else x for x in citation_tokens]\n",
    "processed_text = \"\"\n",
    "for word in citation_text.split():\n",
    "    processed_text += special_token if word.isdigit() else word\n",
    "    processed_text += \" \"\n",
    "\n",
    "citation_text = processed_text\n",
    "citation_tokens = WordTokenizer.tokenize(citation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd62ae36-9b2d-41d5-99f5-56b9a29aa371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'happy', 'pancake', '(123)!']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4c9c4dc-f370-422b-be4d-43554d4e883d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1c90adeedf07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"@@NUM@@\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# token = doc[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "token = nlp(\"@@NUM@@\")[0]\n",
    "# token = doc[0]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62aa8e3c-274b-4bbc-80af-927c6b15f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SingleIdTokenIndexer --> one embedding for each word\n",
    "_token_indexers = {\"elmo\": ELMoTokenCharactersIndexer(),\n",
    "                                    \"tokens\": SingleIdTokenIndexer(), \"bert\": PretrainedTransformerIndexer()}\n",
    "fields = {\n",
    "    'citation_text': TextField(citation_tokens, _token_indexers),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd24d825-1dc5-4a67-acf6-922ffa827e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scicite.training.vocabulary_multitask import VocabularyMultitask\n",
    "\n",
    "params = Params.from_file('experiment_configs/custom_config.json', \"\")\n",
    "serialization_dir = './runs/test'\n",
    "file_friendly_logging = False\n",
    "recover = False\n",
    "vocab = VocabularyMultitask.from_params(\n",
    "        params.pop(\"vocabulary\", {}),\n",
    "        (instance for key, dataset in all_datasets.items()\n",
    "         for instance in dataset\n",
    "         if key in datasets_for_vocab_creation),\n",
    "        instances_aux=vocab_instances_aux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284b10d-a352-4671-8131-7c6b98ac44fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da5fac0f-7ee8-4cac-9eeb-ca1249204739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TextField.as_tensor of <allennlp.data.fields.text_field.TextField object at 0x7f12eb487eb8>>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields['citation_text'].as_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1eb31c70-47f8-41b3-9569-0e8f85b0cd59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "'You must call .index(vocabulary) on a field before determining padding lengths.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-020a6941af0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'citation_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'citation_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_padding_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtoken_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/allennlp/data/fields/text_field.py\u001b[0m in \u001b[0;36mget_padding_lengths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indexed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             raise ConfigurationError(\"You must call .index(vocabulary) on a \"\n\u001b[0m\u001b[1;32m     87\u001b[0m                                      \"field before determining padding lengths.\")\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigurationError\u001b[0m: 'You must call .index(vocabulary) on a field before determining padding lengths.'"
     ]
    }
   ],
   "source": [
    "token_indices = fields['citation_text'].as_tensor(fields['citation_text'].get_padding_lengths()).get(\"tokens\").detach().cpu().numpy()\n",
    "token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6836c67d-2124-4fb1-8323-2d65d61e62d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'undesirable'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab._index_to_token['tokens'][9526]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05df02-6dc8-4e08-ad61-4d84fde5328d",
   "metadata": {},
   "source": [
    "## TextFieldEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216bb8f7-7ecd-4069-be0e-ce14b7606041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.common.params.Params at 0x7fc90067c160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.modules import FeedForward, Seq2VecEncoder, Seq2SeqEncoder, TextFieldEmbedder, Embedding, TimeDistributed\n",
    "params = Params.from_file('experiment_configs/custom_config.json', \"\")\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ed4af8-3f43-4d82-b3de-7cbf001e7f59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-97346bcc7245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#       }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtext_field_embedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFieldEmbedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_field_embedder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# elmo = {\n",
    "#       \"tokens\": {\n",
    "#         \"type\": \"embedding\",\n",
    "#         \"pretrained_file\": \"/home/kanhon/Desktop/kanhon/NUS Computing/CS4248/github_dir/cs4248/scaffold/Project/sci-cite/scicite/pretrained_weights/GoogleNews-vectors-negative300.txt\",\n",
    "#         \"embedding_dim\": 300,\n",
    "#         \"trainable\": \"false\"\n",
    "#       },\n",
    "#       \"elmo\": {\n",
    "#         \"type\": \"elmo_token_embedder\",\n",
    "#         \"options_file\": \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\",\n",
    "#         \"weight_file\": \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\",\n",
    "#         \"do_layer_norm\": \"true\",\n",
    "#         \"dropout\": 0.5\n",
    "#       }\n",
    "# }\n",
    "text_field_embedder = TextFieldEmbedder.from_params(params.pop('model').pop(\"text_field_embedder\"), vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f43452f5-e135-448d-9388-2df11cd2bc36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_field_embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-30b96961d3a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_field_embedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_field_embedder' is not defined"
     ]
    }
   ],
   "source": [
    "text_field_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736434d-f669-454e-ac96-3dd519e577f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from allennlp.models import *\n",
    "# print(pretrained.get_pretrained_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564d33e-06dc-4501-8f76-52772fc82291",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8fbc626-5f57-4f0d-b0cf-b5ff3e990332",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3d0cc953722d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/tokenizer/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d49fac-e219-41cd-bf2a-1a563fa29781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a2b28d5-5ac1-46f4-8e0d-5ff749b702ac",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a28718c-d352-46c2-95e4-83c91989f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from copy import deepcopy\n",
    "from distutils.version import StrictVersion\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from allennlp.common import Params\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import FeedForward, Seq2VecEncoder, Seq2SeqEncoder, TextFieldEmbedder, Embedding, TimeDistributed\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from overrides import overrides\n",
    "from torch.nn import Parameter, Linear\n",
    "\n",
    "from scicite.constants import  Scicite_Format_Nested_Jsonlines\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8b7f2d2-dcbb-45f0-a6bc-d67f545141b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @Model.register(\"scaffold_bilstm_attention_classifier\")\n",
    "class ScaffoldBilstmAttentionClassifier1(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` performs text classification for citation intents.  We assume we're given a\n",
    "    citation text, and we predict some output label.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 citation_text_encoder: Seq2SeqEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 classifier_feedforward_2: FeedForward,\n",
    "                 classifier_feedforward_3: FeedForward,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None,\n",
    "                 report_auxiliary_metrics: bool = False,\n",
    "                 predict_mode: bool = False,\n",
    "                 ) -> None:\n",
    "        \"\"\"\n",
    "        Additional Args:\n",
    "            lexicon_embedder_params: parameters for the lexicon attention model\n",
    "            use_sparse_lexicon_features: whether to use sparse (onehot) lexicon features\n",
    "            multilabel: whether the classification is multi-label\n",
    "            data_format: s2 or jurgens\n",
    "            report_auxiliary_metrics: report metrics for aux tasks\n",
    "            predict_mode: predict unlabeled examples\n",
    "        \"\"\"\n",
    "        super(ScaffoldBilstmAttentionClassifier1, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
    "        self.num_classes_sections = self.vocab.get_vocab_size(\"section_labels\")\n",
    "        self.num_classes_cite_worthiness = self.vocab.get_vocab_size(\"cite_worthiness_labels\")\n",
    "        self.citation_text_encoder = citation_text_encoder\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "        self.classifier_feedforward_2 = classifier_feedforward_2\n",
    "        self.classifier_feedforward_3 = classifier_feedforward_3\n",
    "\n",
    "        self.label_accuracy = CategoricalAccuracy()\n",
    "        self.label_f1_metrics = {}\n",
    "        self.label_f1_metrics_sections = {}\n",
    "        self.label_f1_metrics_cite_worthiness = {}\n",
    "        # for i in range(self.num_classes):\n",
    "        #     self.label_f1_metrics[vocab.get_token_from_index(index=i, namespace=\"labels\")] =\\\n",
    "        #         F1Measure(positive_label=i)\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            self.label_f1_metrics[vocab.get_token_from_index(index=i, namespace=\"labels\")] =\\\n",
    "                F1Measure(positive_label=i)\n",
    "        for i in range(self.num_classes_sections):\n",
    "            self.label_f1_metrics_sections[vocab.get_token_from_index(index=i, namespace=\"section_labels\")] =\\\n",
    "                F1Measure(positive_label=i)\n",
    "        for i in range(self.num_classes_cite_worthiness):\n",
    "            self.label_f1_metrics_cite_worthiness[vocab.get_token_from_index(index=i, namespace=\"cite_worthiness_labels\")] =\\\n",
    "                F1Measure(positive_label=i)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.attention_seq2seq = Attention(citation_text_encoder.get_output_dim())\n",
    "\n",
    "        self.report_auxiliary_metrics = report_auxiliary_metrics\n",
    "        self.predict_mode = predict_mode\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,\n",
    "                citation_text: Dict[str, torch.LongTensor],\n",
    "                labels: torch.LongTensor = None,\n",
    "                lexicon_features: Optional[torch.IntTensor] = None,\n",
    "                year_diff: Optional[torch.Tensor] = None,\n",
    "                citing_paper_id: Optional[str] = None,\n",
    "                cited_paper_id: Optional[str] = None,\n",
    "                citation_excerpt_index: Optional[str] = None,\n",
    "                citation_id: Optional[str] = None,\n",
    "                section_label: Optional[torch.Tensor] = None,\n",
    "                is_citation: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        Args:\n",
    "            citation_text: citation text of shape (batch, sent_len, embedding_dim)\n",
    "            labels: labels\n",
    "            lexicon_features: lexicon sparse features (batch, lexicon_feature_len)\n",
    "            year_diff: difference between cited and citing years\n",
    "            citing_paper_id: id of the citing paper\n",
    "            cited_paper_id: id of the cited paper\n",
    "            citation_excerpt_index: index of the excerpt\n",
    "            citation_id: unique id of the citation\n",
    "            section_label: label of the section\n",
    "            is_citation: citation worthiness label\n",
    "        \"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "        citation_text_embedding = self.text_field_embedder(citation_text)\n",
    "        print(\"citation_text_embedding\", citation_text_embedding.shape)\n",
    "        # print(\"citation_text\", citation_text) # {'elmo': tensor, 'tokens': tensor}\n",
    "        # tokens are converted into numbers using vocab\n",
    "        # print(\"citation_text_embedding\", citation_text_embedding) # tensor\n",
    "        citation_text_mask = util.get_text_field_mask(citation_text)\n",
    "\n",
    "        # shape: [batch, sent, output_dim]\n",
    "        encoded_citation_text = self.citation_text_encoder(citation_text_embedding, citation_text_mask)\n",
    "\n",
    "        # shape: [batch, output_dim]\n",
    "        attn_dist, encoded_citation_text = self.attention_seq2seq(encoded_citation_text, return_attn_distribution=True)\n",
    "\n",
    "        # In training mode, labels are the citation intents\n",
    "        # If in predict_mode, predict the citation intents\n",
    "        if labels is not None:\n",
    "            logits = self.classifier_feedforward(encoded_citation_text)\n",
    "            class_probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            output_dict = {\"logits\": logits}\n",
    "\n",
    "            loss = self.loss(logits, labels)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "            # compute F1 per label\n",
    "            for i in range(self.num_classes):\n",
    "                metric = self.label_f1_metrics[self.vocab.get_token_from_index(index=i, namespace=\"labels\")]\n",
    "                metric(class_probs, labels)\n",
    "            output_dict['labels'] = labels\n",
    "\n",
    "        if section_label is not None:  # this is the first scaffold task\n",
    "            logits = self.classifier_feedforward_2(encoded_citation_text)\n",
    "            class_probs = F.softmax(logits, dim=1)\n",
    "            output_dict = {\"logits\": logits}\n",
    "            loss = self.loss(logits, section_label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "            for i in range(self.num_classes_sections):\n",
    "                metric = self.label_f1_metrics_sections[self.vocab.get_token_from_index(index=i, namespace=\"section_labels\")]\n",
    "                metric(logits, section_label)\n",
    "\n",
    "        if is_citation is not None:  # second scaffold task\n",
    "            logits = self.classifier_feedforward_3(encoded_citation_text)\n",
    "            class_probs = F.softmax(logits, dim=1)\n",
    "            output_dict = {\"logits\": logits}\n",
    "            loss = self.loss(logits, is_citation)\n",
    "            output_dict[\"loss\"] = loss\n",
    "            for i in range(self.num_classes_cite_worthiness):\n",
    "                metric = self.label_f1_metrics_cite_worthiness[\n",
    "                    self.vocab.get_token_from_index(index=i, namespace=\"cite_worthiness_labels\")]\n",
    "                metric(logits, is_citation)\n",
    "\n",
    "        if self.predict_mode:\n",
    "            logits = self.classifier_feedforward(encoded_citation_text)\n",
    "            class_probs = F.softmax(logits, dim=1)\n",
    "            output_dict = {\"logits\": logits}\n",
    "\n",
    "        output_dict['citing_paper_id'] = citing_paper_id\n",
    "        output_dict['cited_paper_id'] = cited_paper_id\n",
    "        output_dict['citation_excerpt_index'] = citation_excerpt_index\n",
    "        output_dict['citation_id'] = citation_id\n",
    "        output_dict['attn_dist'] = attn_dist  # also return attention distribution for analysis\n",
    "        output_dict['citation_text'] = citation_text['tokens']\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        class_probabilities = F.softmax(output_dict['logits'], dim=-1)\n",
    "        predictions = class_probabilities.cpu().data.numpy()\n",
    "        argmax_indices = np.argmax(predictions, axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
    "                 for x in argmax_indices]\n",
    "        output_dict['probabilities'] = class_probabilities\n",
    "        output_dict['positive_labels'] = labels\n",
    "        output_dict['prediction'] = labels\n",
    "        citation_text = []\n",
    "        for batch_text in output_dict['citation_text']:\n",
    "            citation_text.append([self.vocab.get_token_from_index(token_id.item()) for token_id in batch_text])\n",
    "        output_dict['citation_text'] = citation_text\n",
    "        output_dict['all_labels'] = [self.vocab.get_index_to_token_vocabulary(namespace=\"labels\")\n",
    "                                     for _ in range(output_dict['logits'].shape[0])]\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metric_dict = {}\n",
    "\n",
    "        sum_f1 = 0.0\n",
    "        for name, metric in self.label_f1_metrics.items():\n",
    "            metric_val = metric.get_metric(reset)\n",
    "            metric_dict[name + '_P'] = metric_val[0]\n",
    "            metric_dict[name + '_R'] = metric_val[1]\n",
    "            metric_dict[name + '_F1'] = metric_val[2]\n",
    "            if name != 'none':  # do not consider `none` label in averaging F1\n",
    "                sum_f1 += metric_val[2]\n",
    "\n",
    "        names = list(self.label_f1_metrics.keys())\n",
    "        total_len = len(names) if 'none' not in names else len(names) - 1\n",
    "        average_f1 = sum_f1 / total_len\n",
    "        # metric_dict['combined_metric'] = (accuracy + average_f1) / 2\n",
    "        metric_dict['average_F1'] = average_f1\n",
    "\n",
    "        if self.report_auxiliary_metrics:\n",
    "            sum_f1 = 0.0\n",
    "            for name, metric in self.label_f1_metrics_sections.items():\n",
    "                metric_val = metric.get_metric(reset)\n",
    "                metric_dict['aux-sec--' + name + '_P'] = metric_val[0]\n",
    "                metric_dict['aux-sec--' + name + '_R'] = metric_val[1]\n",
    "                metric_dict['aux-sec--' + name + '_F1'] = metric_val[2]\n",
    "                if name != 'none':  # do not consider `none` label in averaging F1\n",
    "                    sum_f1 += metric_val[2]\n",
    "            names = list(self.label_f1_metrics_sections.keys())\n",
    "            total_len = len(names) if 'none' not in names else len(names) - 1\n",
    "            average_f1 = sum_f1 / total_len\n",
    "            # metric_dict['combined_metric'] = (accuracy + average_f1) / 2\n",
    "            metric_dict['aux-sec--' + 'average_F1'] = average_f1\n",
    "\n",
    "            sum_f1 = 0.0\n",
    "            for name, metric in self.label_f1_metrics_cite_worthiness.items():\n",
    "                metric_val = metric.get_metric(reset)\n",
    "                metric_dict['aux-worth--' + name + '_P'] = metric_val[0]\n",
    "                metric_dict['aux-worth--' + name + '_R'] = metric_val[1]\n",
    "                metric_dict['aux-worth--' + name + '_F1'] = metric_val[2]\n",
    "                if name != 'none':  # do not consider `none` label in averaging F1\n",
    "                    sum_f1 += metric_val[2]\n",
    "            names = list(self.label_f1_metrics_cite_worthiness.keys())\n",
    "            total_len = len(names) if 'none' not in names else len(names) - 1\n",
    "            average_f1 = sum_f1 / total_len\n",
    "            # metric_dict['combined_metric'] = (accuracy + average_f1) / 2\n",
    "            metric_dict['aux-worth--' + 'average_F1'] = average_f1\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'ScaffoldBilstmAttentionClassifier':\n",
    "        with_elmo = params.pop_bool(\"with_elmo\", False)\n",
    "        if with_elmo:\n",
    "            embedder_params = params.pop(\"elmo_text_field_embedder\")\n",
    "        else:\n",
    "            embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(embedder_params, vocab=vocab)\n",
    "        # citation_text_encoder = Seq2VecEncoder.from_params(params.pop(\"citation_text_encoder\"))\n",
    "        citation_text_encoder = Seq2SeqEncoder.from_params(params.pop(\"citation_text_encoder\"))\n",
    "        classifier_feedforward = FeedForward.from_params(params.pop(\"classifier_feedforward\"))\n",
    "        classifier_feedforward_2 = FeedForward.from_params(params.pop(\"classifier_feedforward_2\"))\n",
    "        classifier_feedforward_3 = FeedForward.from_params(params.pop(\"classifier_feedforward_3\"))\n",
    "\n",
    "        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n",
    "        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n",
    "\n",
    "        use_lexicon = params.pop_bool(\"use_lexicon_features\", False)\n",
    "        use_sparse_lexicon_features = params.pop_bool(\"use_sparse_lexicon_features\", False)\n",
    "        data_format = params.pop('data_format')\n",
    "\n",
    "        report_auxiliary_metrics = params.pop_bool(\"report_auxiliary_metrics\", False)\n",
    "\n",
    "        predict_mode = params.pop_bool(\"predict_mode\", False)\n",
    "        print(f\"pred mode: {predict_mode}\")\n",
    "\n",
    "        return cls(vocab=vocab,\n",
    "                   text_field_embedder=text_field_embedder,\n",
    "                   citation_text_encoder=citation_text_encoder,\n",
    "                   classifier_feedforward=classifier_feedforward,\n",
    "                   classifier_feedforward_2=classifier_feedforward_2,\n",
    "                   classifier_feedforward_3=classifier_feedforward_3,\n",
    "                   initializer=initializer,\n",
    "                   regularizer=regularizer,\n",
    "                   report_auxiliary_metrics=report_auxiliary_metrics,\n",
    "                   predict_mode=predict_mode)\n",
    "\n",
    "\n",
    "def new_parameter(*size):\n",
    "    out = Parameter(torch.FloatTensor(*size))\n",
    "    torch.nn.init.xavier_normal_(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" Simple multiplicative attention\"\"\"\n",
    "    def __init__(self, attention_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = new_parameter(attention_size, 1)\n",
    "\n",
    "    def forward(self, x_in, reduction_dim=-2, return_attn_distribution=False):\n",
    "        \"\"\"\n",
    "        return_attn_distribution: if True it will also return the original attention distribution\n",
    "\n",
    "        this reduces the one before last dimension in x_in to a weighted sum of the last dimension\n",
    "        e.g., x_in.shape == [64, 30, 100] -> output.shape == [64, 100]\n",
    "        Usage: You have a sentence of shape [batch, sent_len, embedding_dim] and you want to\n",
    "            represent sentence to a single vector using attention [batch, embedding_dim]\n",
    "\n",
    "        Here we use it to aggregate the lexicon-aware representation of the sentence\n",
    "        In two steps we convert [batch, sent_len, num_words_in_category, num_categories] into [batch, num_categories]\n",
    "        \"\"\"\n",
    "        # calculate attn weights\n",
    "        attn_score = torch.matmul(x_in, self.attention).squeeze()\n",
    "        # add one dimension at the end and get a distribution out of scores\n",
    "        attn_distrib = F.softmax(attn_score.squeeze(), dim=-1).unsqueeze(-1)\n",
    "        scored_x = x_in * attn_distrib\n",
    "        weighted_sum = torch.sum(scored_x, dim=reduction_dim)\n",
    "        if return_attn_distribution:\n",
    "            return attn_distrib.reshape(x_in.shape[0], -1), weighted_sum\n",
    "        else:\n",
    "            return weighted_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62add80c-7dac-4a20-87ba-f332ba1bcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params.from_file('experiment_configs/custom_config.json', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "be984fcb-dae9-40dd-a004-6de8a1ba6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScaffoldBilstmAttentionClassifier1.from_params(vocab=vocab, params=params.pop('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2f28276e-7e3d-4d64-b6fe-70c8f38e27d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicTextFieldEmbedder(\n",
       "  (token_embedder_elmo): ElmoTokenEmbedder(\n",
       "    (_elmo): Elmo(\n",
       "      (_elmo_lstm): _ElmoBiLm(\n",
       "        (_token_embedder): _ElmoCharacterEncoder(\n",
       "          (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "          (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "          (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "          (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "          (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "          (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "          (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "          (_highways): Highway(\n",
       "            (_layers): ModuleList(\n",
       "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "              (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (_elmo_lstm): ElmoLstm(\n",
       "          (forward_layer_0): LstmCellWithProjection(\n",
       "            (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "            (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "            (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          )\n",
       "          (backward_layer_0): LstmCellWithProjection(\n",
       "            (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "            (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "            (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          )\n",
       "          (forward_layer_1): LstmCellWithProjection(\n",
       "            (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "            (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "            (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          )\n",
       "          (backward_layer_1): LstmCellWithProjection(\n",
       "            (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "            (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "            (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (_dropout): Dropout(p=0.5, inplace=False)\n",
       "      (scalar_mix_0): ScalarMix(\n",
       "        (scalar_parameters): ParameterList(\n",
       "            (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "            (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "            (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedder_tokens): Embedding()\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1fec676b-5abb-4fdd-92da-0b12570bd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda(0)\n",
    "batch = util.move_to_device(batch, 0)\n",
    "output_dict = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8c7c67e9-279f-434e-b6f9-9d450f6a04c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pylint: disable=arguments-differ\n",
    "citation_text = batch['citation_text'] \n",
    "citation_text_embedding = model.text_field_embedder(citation_text)\n",
    "print(\"citation_text_embedding\", citation_text_embedding.shape)\n",
    "# print(\"citation_text\", citation_text) # {'elmo': tensor, 'tokens': tensor}\n",
    "# tokens are converted into numbers using vocab\n",
    "# print(\"citation_text_embedding\", citation_text_embedding) # tensor\n",
    "citation_text_mask = util.get_text_field_mask(citation_text)\n",
    "print(\"citation_text_mask\", citation_text_mask.shape)\n",
    "\n",
    "# shape: [batch, sent, output_dim]\n",
    "encoded_citation_text = model.citation_text_encoder(citation_text_embedding, citation_text_mask)\n",
    "print(\"encoded_citation_text\", encoded_citation_text.shape)\n",
    "\n",
    "# shape: [batch, output_dim]\n",
    "# attn_dist, encoded_citation_text = model.attention_seq2seq(encoded_citation_text, return_attn_distribution=True)\n",
    "# print(\"encoded_citation_text\", encoded_citation_text.shape)\n",
    "\n",
    "# In training mode, labels are the citation intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "451ef90c-c3d9-4536-ac7b-37e6ac3e7e1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elmo': tensor([[[259,  87, 106,  ..., 261, 261, 261],\n",
       "         [259,  66, 260,  ..., 261, 261, 261],\n",
       "         [259, 116, 118,  ..., 261, 261, 261],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[259,  71, 112,  ..., 261, 261, 261],\n",
       "         [259,  72, 118,  ..., 261, 261, 261],\n",
       "         [259,  98, 111,  ..., 261, 261, 261],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[259,  72, 106,  ..., 261, 261, 261],\n",
       "         [259, 117, 105,  ..., 261, 261, 261],\n",
       "         [259, 108, 111,  ..., 261, 261, 261],\n",
       "         ...,\n",
       "         [259, 117, 105,  ..., 261, 261, 261],\n",
       "         [259, 101, 106,  ..., 261, 261, 261],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[259,  85, 105,  ..., 261, 261, 261],\n",
       "         [259, 115, 102,  ..., 261, 261, 261],\n",
       "         [259, 112, 103,  ..., 261, 261, 261],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[259,  66, 116,  ..., 261, 261, 261],\n",
       "         [259,  98, 260,  ..., 261, 261, 261],\n",
       "         [259, 113,  98,  ..., 261, 261, 261],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[259,  72,  71,  ..., 261, 261, 261],\n",
       "         [259, 103, 109,  ..., 261, 261, 261],\n",
       "         [259, 112, 103,  ..., 261, 261, 261],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]]], device='cuda:0'), 'tokens': tensor([[22229,    88,  3834,     9, 23939,   369,  1717,     4,  1425,     5,\n",
       "         31596,   283,     8,   129,     7,     2, 26269,  4447,     8,   128,\n",
       "             7,     2, 21872,     8,   169,     2,   191,     7,     2,     6,\n",
       "          6433,    12,    10,    12,  1434,  1029,     5,  2059,     8,  2069,\n",
       "             3,     0,     0,     0,     0,     0],\n",
       "        [ 1524, 29006,     6, 29007,     8,   118,     7,     2,    15,  2006,\n",
       "            12,  2083,   351,  2954,   794,    31,    45,    10,  1182,   475,\n",
       "          3799,  2006,    12,  4127,    25,     4, 20456,  1028,  2633,     2,\n",
       "            22,   882,     9,     4,  1277, 29008,   174,     8, 29009,   316,\n",
       "             7,     3,     0,     0,     0,     0],\n",
       "        [ 1243,     4,   213, 20214,     9,  1272,   352,     5, 10668,     9,\n",
       "           369,    19,  2799,     8, 12535,    11,    14,     3,   274,    13,\n",
       "           541,    11,    14,     3,    89,    13,  9560,    11,    14,     3,\n",
       "            61,    13, 11528,    11,    14,     3,    70,     7,     2,    77,\n",
       "            66,  6344,    17,     4, 30384,     0],\n",
       "        [    8,   662,     7,   259,    17,  7205,   465,    19,   937,   359,\n",
       "         22865,    31,   139,    10,    17,   465,    19, 38690,   310,   493,\n",
       "           287,     9, 23465,    12,  7393,   412,     2,    32,   765,    17,\n",
       "             4, 20683,   727,    16,   270,   310,    50,   287,     3,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  153,    16,   713,  3700,    17, 16713,   917,    15,  1865,   219,\n",
       "             9,     4,  2282,     5,  5170,   879,     2,    50,    22,     4,\n",
       "          3134,   586,  1432,     9,   340,  3064,  2644,     6, 14925,     5,\n",
       "         40130,     9,  1201,    91,     8,   240,     2,   224,     7,     3,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [    8,   316,     7,   238,    17,  4795, 13180,  1239,   527,    19,\n",
       "            15,  3049,   325,  2256,     2, 13181,    12, 13182,    21, 13183,\n",
       "             8, 13184,    12, 12909,     7,     2,    38,   975,    18,  2449,\n",
       "             6,  1063,  1901,     6,  4400,     9,   412,     8,  3271,    11,\n",
       "            14,     3,     2,   316,     7,     3],\n",
       "        [   93,  1851,    71,  2037,    28,    15,   184,    62,    17,    82,\n",
       "           387,   381,  1809,     5,  3917,     5,  4906,   789,     9,  1065,\n",
       "          1156,   121,  1638,   119,    31,   228,    19,   612,   348,    67,\n",
       "           121,    51,    31,  3153,     8, 17065,     2,    61,     7,     3,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [   93,   338,    92,    17,  1100,     8,  7204,    11,    14,     3,\n",
       "             2,   113,     7,    22,   103,    22,   161,     8,  7204,    11,\n",
       "            14,     3,    85,     7,  3016,    24,  1546,     9,  6647,  1372,\n",
       "             6,   180,     2,  1977,    15,   628,   219,    18,     4,  6647,\n",
       "           472,     9,  2451,  6249,     3,     0],\n",
       "        [   41, 22882,   928,     2,  2516,     8,  3101,     7, 20530,   551,\n",
       "         22883,    24,  4954,    28,     4,  3104,     2,     6,   917,    34,\n",
       "          1122,   219,     9,  9384, 15738,  3455,  3101,     9,     4, 15362,\n",
       "             8, 12569,    11,    14,     3,    97,    13, 10360,    11,    14,\n",
       "             3,    97,     7,     3,     0,     0],\n",
       "        [  144,    52,    37,  8701,    19, 15584,    11,    14,     3,     2,\n",
       "             8,    83,     7,   199,   192,  6080,  1541,     5, 12807,   434,\n",
       "            31,   214,   231,    10,   737,   434,     6,    82,   176,   241,\n",
       "           186,     5,  1149,    25,  6080,  1541,     9, 10243,     3,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  770,     5,    65,   356,   922,   377,     9,  1112,  6055,     6,\n",
       "          3341,  8509,     2,    50,    22,  4218,   752,    12, 27146,     8,\n",
       "         12109,     7,     2, 12593, 12594,  1633,     8, 32964,     7,     6,\n",
       "         14936,    12,  6010,  8578,    21,  1026,     2,  1898,     2,  1455,\n",
       "            20,     3,     0,     0,     0,     0],\n",
       "        [ 1484,     2,   184,   126,    23, 23790,     2, 19207,     6, 16138,\n",
       "            21,    72,    20,   520,    34,  1470,  1468,    25,  1722,    12,\n",
       "          1036,     5, 23791,  1962,    57,     7,    18,   127,  1744,    12,\n",
       "          2542,   310,     4,  3171,     2,     9,    15,   700, 15737,  2559,\n",
       "             2,   525,    34, 11251,   215,     3],\n",
       "        [   26,   561, 12851,  3174,     9,     4, 32876,    38,   275,    33,\n",
       "           292,    10,    27,     4, 15263,    21,   129,    20,     2,     6,\n",
       "             4,   195, 21902,  8081,     6,  3691,    24,  1228,    10,    27,\n",
       "             4,   561,  4499,     5,  2486,     9,     4, 35181,    21,    57,\n",
       "             2,    72,    20,     3,     0,     0],\n",
       "        [   26,    52,     5,  5971,    11,    14,     3,     8,    76,     7,\n",
       "            46,   362,    17,   145,    16,    15,   250,   114,   675,    58,\n",
       "             4,  1429,  2920,     6,     4,  2524,  1129,     5,     4,  1472,\n",
       "             6,  1103,   751,    18,    96,     4,  5326,     5,     4,  5522,\n",
       "          9105,     3,     0,     0,     0,     0],\n",
       "        [  205,    15,   377,     5,  1841,  1168,   505,    18,     4,    62,\n",
       "          2770,  3367,  3330,   817,    48,     4, 30583,    21,   169,    20,\n",
       "             6,  4881,  1180,    48,     4,  2625, 12151,    10,  1589,  1858,\n",
       "         20275,  6334,   166, 15617, 30584, 18365,     8, 18366,     7,    21,\n",
       "           191,    20,     3,     0,     0,     0],\n",
       "        [ 2663,  3123,     5, 24845,    91,  2680,  2663,    12, 24846,     6,\n",
       "             4,  3044,  2622, 24847,    12, 12018,     7,     2,  3170,   989,\n",
       "          1259,    57,    12,   143,     2,     6,  2663,    12, 24848,    12,\n",
       "         12018,     7,     2,  3170,   989,  1259,    57,    12,   726,     3,\n",
       "             0,     0,     0,     0,     0,     0]], device='cuda:0')}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['citation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "097ebebe-6986-4c84-99a5-72b0a2f1042b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 200])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_citation_text[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9c229dfa-998c-4860-8289-87fbcc747b25",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'pop_choice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-5dbd88bdd2e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m        }\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cs4248_scicite_torch/lib/python3.6/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mas_registrable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRegistrable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mdefault_to_first_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_registrable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_implementation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             choice = params.pop_choice(\"type\",\n\u001b[0m\u001b[1;32m    262\u001b[0m                                        \u001b[0mchoices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_registrable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                                        default_to_first_choice=default_to_first_choice)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'pop_choice'"
     ]
    }
   ],
   "source": [
    "params1={'model': {\n",
    "      \"type\": \"gru\",\n",
    "      \"bidirectional\": 'true',\n",
    "      \"input_size\": 1324,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.3\n",
    "    }\n",
    "       }\n",
    "Model.from_params(vocab=vocab, params=params1.pop('model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90254d2-5c77-46f3-83ad-fa1fb6891f46",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6073099-0e09-4035-b246-a59d89c2afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kanhon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
    "\n",
    " \n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b91fbd32-e497-4fa0-ade3-2d5350e5f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = '''\n",
    "Activated Are PBMC are the basis of the standard PBMC blast assay for HIV-1 neutralization, whereas the various GHOST and HeLa cell lines have all been used in neutralization assays (42, 66)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a654039-e910-480c-a49a-95edbe8f4854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st_t = WordTokenizer().tokenize(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be25d085-5aa6-45ad-b9e4-e0415263a26e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Activated,\n",
       " Are,\n",
       " PBMC,\n",
       " basis,\n",
       " standard,\n",
       " PBMC,\n",
       " blast,\n",
       " assay,\n",
       " HIV-1,\n",
       " neutralization,\n",
       " ,,\n",
       " GHOST,\n",
       " HeLa,\n",
       " cell,\n",
       " lines,\n",
       " neutralization,\n",
       " assays,\n",
       " (,\n",
       " 42,\n",
       " ,,\n",
       " 66,\n",
       " )]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in st_t if not x.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a534b-599b-4f43-963a-8e82ca1706d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deaad2d5-eed7-4ae9-8ce9-0e3a1b28fa0d",
   "metadata": {},
   "source": [
    "## Dataset remove citation brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847c5c51-05a0-4ec4-bd80-b4c2bb8cb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a59da04-be28-4d77-a580-39eebd3eaced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"./scicite_data/train.jsonl\", encoding=\"utf8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    lines = [json.loads(x) for x in lines]\n",
    "df_train = pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d052198-a802-4530-b9bb-a74207e825c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = df_train['string'].iloc[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1daa33e2-27bd-482d-9c05-e2eb190e3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Previous empirical analyses of subnational consumer subsidies found that a majority of solar adoption was attributable to subsidies (e.g., Hughes and Podolefsky, 2015; Burr, 2016; De Groote and Verboven, 2016; Gillingham and Tsvetanov, 2017).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e72342-5ddb-4a0d-bfa9-9ad70da18819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Previous empirical analyses of subnational consumer subsidies found that a majority of solar adoption was attributable to subsidies (e.g., Hughes and Podolefsky, 2015; Burr, 2016; De Groote and Verboven, 2016; Gillingham and Tsvetanov, 2017).'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6be16740-8c5f-48e6-935d-f64a5f04f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).\n",
      "However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).\n",
      "However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS @@@CITE@@@, IscU/Isu1 @@@CITE@@@ or ISD11/Isd11 @@@CITE@@@).\n",
      "In the study by Hickey et al. (2012), spikes were sampled from the field at the point of physiological\n",
      "robinson et al.: genomic regions influencing root traits in barley 11 of 13\n",
      "maturity, dried, grain threshed by hand, and stored at 20C to preserve grain dormancy before germination testing.\n",
      "In the study by Hickey et al. (2012), spikes were sampled from the field at the point of physiological\n",
      "robinson et al.: genomic regions influencing root traits in barley 11 of 13\n",
      "maturity, dried, grain threshed by hand, and stored at 20C to preserve grain dormancy before germination testing.\n",
      "In the study by Hickey et al. (2012), spikes were sampled from the field at the point of physiological\n",
      "robinson et al.: genomic regions influencing root traits in barley 11 of 13\n",
      "maturity, dried, grain threshed by hand, and stored at 20C to preserve grain dormancy before germination testing.\n",
      "The drug also reduces catecholamine secretion, thereby reducing stress and leading to a modest (10-20%) reduction in heart rate and blood pressure, which may be particularly beneficial in patients with cardiovascular disease.(7) Unlike midazolam, dexmedetomidine does not affect the ventilatory response to carbon dioxide.\n",
      "The drug also reduces catecholamine secretion, thereby reducing stress and leading to a modest (10-20%) reduction in heart rate and blood pressure, which may be particularly beneficial in patients with cardiovascular disease.(7) Unlike midazolam, dexmedetomidine does not affect the ventilatory response to carbon dioxide.\n",
      "The drug also reduces catecholamine secretion, thereby reducing stress and leading to a modest (10-20%) reduction in heart rate and blood pressure, which may be particularly beneficial in patients with cardiovascular disease.(7) Unlike midazolam, dexmedetomidine does not affect the ventilatory response to carbon dioxide.\n",
      "By clustering with lowly aggressive close kin (King 1989a,b; Viblanc et al. 2010; Arnaud, Dobson & Murie 2012), breeding females may decrease the time/energy cost of maintaining territorial boundaries (Festa-Bianchet & Boag 1982; Murie & Harris 1988), which could ultimately lead to increases in net energy income (TA) or higher allocations in somatic or reproductive functions.\n",
      "By clustering with lowly aggressive close kin @@@CITE@@@, breeding females may decrease the time/energy cost of maintaining territorial boundaries @@@CITE@@@, which could ultimately lead to increases in net energy income (TA) or higher allocations in somatic or reproductive functions.\n",
      "By clustering with lowly aggressive close kin (King 1989a,b; Viblanc et al. 2010; Arnaud, Dobson & Murie 2012), breeding females may decrease the time/energy cost of maintaining territorial boundaries (Festa-Bianchet & Boag 1982; Murie & Harris 1988), which could ultimately lead to increases in net energy income (TA) or higher allocations in somatic or reproductive functions.\n",
      "Ophthalmic symptoms are rare manifestations of the intracranial arachnoid cyst, and include unilateral exophthalmos, visual field abnormality, decreased visual acuity and isolated palsies of the third, fourth and sixth cranial nerves [15].\n",
      "Ophthalmic symptoms are rare manifestations of the intracranial arachnoid cyst, and include unilateral exophthalmos, visual field abnormality, decreased visual acuity and isolated palsies of the third, fourth and sixth cranial nerves [15].\n",
      "Ophthalmic symptoms are rare manifestations of the intracranial arachnoid cyst, and include unilateral exophthalmos, visual field abnormality, decreased visual acuity and isolated palsies of the third, fourth and sixth cranial nerves @@@CITE@@@.\n",
      "Recent studies identified Wee1 as a potential molecular target in cancer cells and the selective small molecule Wee1-inhibitor MK-1775 demonstrated promising results in cancer cells with enhanced levels of Wee1 (96-98).\n",
      "Recent studies identified Wee1 as a potential molecular target in cancer cells and the selective small molecule Wee1-inhibitor MK-1775 demonstrated promising results in cancer cells with enhanced levels of Wee1 (96-98).\n",
      "Recent studies identified Wee1 as a potential molecular target in cancer cells and the selective small molecule Wee1-inhibitor MK-1775 demonstrated promising results in cancer cells with enhanced levels of Wee1 (96-98).\n",
      "These problems combine to make early diagnosis essential and immediate treatment a necessity, even for the youngest patients [17, 18].\n",
      "These problems combine to make early diagnosis essential and immediate treatment a necessity, even for the youngest patients [17, 18].\n",
      "These problems combine to make early diagnosis essential and immediate treatment a necessity, even for the youngest patients @@@CITE@@@.\n",
      "Also, results demonstrated that the molecular weight and G/M ratio were important factors in controlling the antioxidant properties of sodium alginate (en 2011).\n",
      "Also, results demonstrated that the molecular weight and G/M ratio were important factors in controlling the antioxidant properties of sodium alginate @@@CITE@@@.\n",
      "Also, results demonstrated that the molecular weight and G/M ratio were important factors in controlling the antioxidant properties of sodium alginate (en 2011).\n",
      "Recently, the light-induced method, which is based on the changes of surface wettability of certain materials [12], has been developed and provides amore convenient approach for cell harvesting [13].\n",
      "Recently, the light-induced method, which is based on the changes of surface wettability of certain materials [12], has been developed and provides amore convenient approach for cell harvesting [13].\n",
      "Recently, the light-induced method, which is based on the changes of surface wettability of certain materials @@@CITE@@@, has been developed and provides amore convenient approach for cell harvesting @@@CITE@@@.\n",
      "Currently, with advances in radiotherapeutic, chemotherapeutic, and surgical techniques, limb-salvage surgery has become an accepted treatment [29].\n",
      "Currently, with advances in radiotherapeutic, chemotherapeutic, and surgical techniques, limb-salvage surgery has become an accepted treatment [29].\n",
      "Currently, with advances in radiotherapeutic, chemotherapeutic, and surgical techniques, limb-salvage surgery has become an accepted treatment @@@CITE@@@.\n"
     ]
    }
   ],
   "source": [
    "for sample in samples: \n",
    "    print(sample)\n",
    "    # print(re.findall(\"[(](\\D+\\d{4})*?[)]\", sample))\n",
    "    # print(re.findall(\"([\\[]\\d+([,-]?\\s*\\W*\\d+)*[\\]])\", sample))\n",
    "    print(re.sub(\"[\\(\\[](\\D+\\d{4})*?[\\)\\]]\", \"@@@CITE@@@\", sample))\n",
    "    print(re.sub(\"([\\[\\(]\\d+([,-]?\\s*\\W*\\d+)*[\\]\\)])\", \"@@@CITE@@@\", sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb97fe3-6af4-4747-b913-ba9cd5debee1",
   "metadata": {},
   "source": [
    "## JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd6873ad-7143-46c9-a00f-17206528d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "074287da-4cff-45de-9700-7276b6ed99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./scicite_data/dev.jsonl\", encoding=\"utf8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    lines = [json.loads(x) for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16b2f9ad-3cd3-4140-93df-e21e5fd028a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3bc2c82-cb47-4007-86cf-62ac074048f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>citeEnd</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>citeStart</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "      <th>citingPaperId</th>\n",
       "      <th>citedPaperId</th>\n",
       "      <th>isKeyCitation</th>\n",
       "      <th>id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>excerpt_index</th>\n",
       "      <th>label_confidence</th>\n",
       "      <th>label2_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explicit</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>64.0</td>\n",
       "      <td>These results are in contrast with the finding...</td>\n",
       "      <td>result</td>\n",
       "      <td>supportive</td>\n",
       "      <td>8f1fbe460a901d994e9b81d69f77bfbe32719f4c</td>\n",
       "      <td>5e413c7872f5df231bf4a4f694504384560e98ca</td>\n",
       "      <td>False</td>\n",
       "      <td>8f1fbe460a901d994e9b81d69f77bfbe32719f4c&gt;5e413...</td>\n",
       "      <td>8f1fbe460a901d994e9b81d69f77bfbe32719f4c&gt;5e413...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>explicit</td>\n",
       "      <td>241.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>222.0</td>\n",
       "      <td>nest burrows in close proximity of one anothe...</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b</td>\n",
       "      <td>2cc6ff899bf17666ad35893524a4d61624555ed7</td>\n",
       "      <td>False</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7337</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explicit</td>\n",
       "      <td>94.0</td>\n",
       "      <td>. 6 Discussion</td>\n",
       "      <td>71.0</td>\n",
       "      <td>This is clearly in contrast to the results of ...</td>\n",
       "      <td>result</td>\n",
       "      <td>supportive</td>\n",
       "      <td>226f798d30e5523c5b9deafb826ddb04d47c11dc</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>226f798d30e5523c5b9deafb826ddb04d47c11dc&gt;None</td>\n",
       "      <td>226f798d30e5523c5b9deafb826ddb04d47c11dc&gt;None_0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>explicit</td>\n",
       "      <td>170.0</td>\n",
       "      <td></td>\n",
       "      <td>148.0</td>\n",
       "      <td>in a subset of alcoholics (Chen et al., 2004;...</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59dba7cd80edcce831d20b35f9eb597bba290154</td>\n",
       "      <td>273996fbf99465211eb8306abe8c56c5835f332e</td>\n",
       "      <td>False</td>\n",
       "      <td>59dba7cd80edcce831d20b35f9eb597bba290154&gt;27399...</td>\n",
       "      <td>59dba7cd80edcce831d20b35f9eb597bba290154&gt;27399...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explicit</td>\n",
       "      <td>89.0</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>85.0</td>\n",
       "      <td>This result is consistent with the conclusions...</td>\n",
       "      <td>result</td>\n",
       "      <td>not_supportive</td>\n",
       "      <td>0640f6e098a9d241cd680473e8705357ae101e04</td>\n",
       "      <td>e33da0584b8db37816d510fd9ba7c1216858fd5f</td>\n",
       "      <td>False</td>\n",
       "      <td>0640f6e098a9d241cd680473e8705357ae101e04&gt;e33da...</td>\n",
       "      <td>0640f6e098a9d241cd680473e8705357ae101e04&gt;e33da...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>explicit</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Our results are consistent with those of a pre...</td>\n",
       "      <td>result</td>\n",
       "      <td>not_supportive</td>\n",
       "      <td>d9ef5d1cb543d0f1330908b36f7106f23cbad404</td>\n",
       "      <td>ae7c65d4d7710bf5f36309faea886648f9a51d4a</td>\n",
       "      <td>False</td>\n",
       "      <td>d9ef5d1cb543d0f1330908b36f7106f23cbad404&gt;ae7c6...</td>\n",
       "      <td>d9ef5d1cb543d0f1330908b36f7106f23cbad404&gt;ae7c6...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>explicit</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1. Introduction</td>\n",
       "      <td>129.0</td>\n",
       "      <td>Some of these peptides act as neurotoxins on t...</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d937bc4d0722d5b45366b3c4dfde4732224bc048</td>\n",
       "      <td>9200a75534f44836ca7651c9d63d11b884947fa6</td>\n",
       "      <td>True</td>\n",
       "      <td>d937bc4d0722d5b45366b3c4dfde4732224bc048&gt;9200a...</td>\n",
       "      <td>d937bc4d0722d5b45366b3c4dfde4732224bc048&gt;9200a...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>explicit</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4. Discussion</td>\n",
       "      <td>144.0</td>\n",
       "      <td>Therefore, despite an apparent higher number o...</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3f50975c58d861e4fbd3b4fd065f0658b1aa1e10</td>\n",
       "      <td>d16a1d95e6947da69797bb0cb59148057174e35a</td>\n",
       "      <td>True</td>\n",
       "      <td>3f50975c58d861e4fbd3b4fd065f0658b1aa1e10&gt;d16a1...</td>\n",
       "      <td>3f50975c58d861e4fbd3b4fd065f0658b1aa1e10&gt;d16a1...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>explicit</td>\n",
       "      <td>28.0</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>13.0</td>\n",
       "      <td>According to Xu et al (2011) the factors that ...</td>\n",
       "      <td>method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d776ade6bb4898c032b971d2cec145976408e838</td>\n",
       "      <td>22e3889f93c19c15b746c1339ce9d7439ccb632e</td>\n",
       "      <td>False</td>\n",
       "      <td>d776ade6bb4898c032b971d2cec145976408e838&gt;22e38...</td>\n",
       "      <td>d776ade6bb4898c032b971d2cec145976408e838&gt;22e38...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>explicit</td>\n",
       "      <td>259.0</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>242.0</td>\n",
       "      <td>for an asexual and recombining population: Th...</td>\n",
       "      <td>method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6406d384e741a6fde6f4aa5e0100bb19f7259842</td>\n",
       "      <td>df384ef1a0677857d0118b9852bff69ee3925e06</td>\n",
       "      <td>False</td>\n",
       "      <td>6406d384e741a6fde6f4aa5e0100bb19f7259842&gt;df384...</td>\n",
       "      <td>6406d384e741a6fde6f4aa5e0100bb19f7259842&gt;df384...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source  citeEnd      sectionName  citeStart  \\\n",
       "0    explicit     68.0       Discussion       64.0   \n",
       "1    explicit    241.0       Discussion      222.0   \n",
       "2    explicit     94.0   . 6 Discussion       71.0   \n",
       "3    explicit    170.0                       148.0   \n",
       "4    explicit     89.0       DISCUSSION       85.0   \n",
       "..        ...      ...              ...        ...   \n",
       "911  explicit     99.0       Discussion       71.0   \n",
       "912  explicit    136.0  1. Introduction      129.0   \n",
       "913  explicit    150.0    4. Discussion      144.0   \n",
       "914  explicit     28.0     INTRODUCTION       13.0   \n",
       "915  explicit    259.0          METHODS      242.0   \n",
       "\n",
       "                                                string       label  \\\n",
       "0    These results are in contrast with the finding...      result   \n",
       "1    nest burrows in close proximity of one anothe...  background   \n",
       "2    This is clearly in contrast to the results of ...      result   \n",
       "3    in a subset of alcoholics (Chen et al., 2004;...  background   \n",
       "4    This result is consistent with the conclusions...      result   \n",
       "..                                                 ...         ...   \n",
       "911  Our results are consistent with those of a pre...      result   \n",
       "912  Some of these peptides act as neurotoxins on t...  background   \n",
       "913  Therefore, despite an apparent higher number o...  background   \n",
       "914  According to Xu et al (2011) the factors that ...      method   \n",
       "915  for an asexual and recombining population: Th...      method   \n",
       "\n",
       "             label2                             citingPaperId  \\\n",
       "0        supportive  8f1fbe460a901d994e9b81d69f77bfbe32719f4c   \n",
       "1               NaN  d9f3207db0c79a3b154f3875c9760cc6b056904b   \n",
       "2        supportive  226f798d30e5523c5b9deafb826ddb04d47c11dc   \n",
       "3               NaN  59dba7cd80edcce831d20b35f9eb597bba290154   \n",
       "4    not_supportive  0640f6e098a9d241cd680473e8705357ae101e04   \n",
       "..              ...                                       ...   \n",
       "911  not_supportive  d9ef5d1cb543d0f1330908b36f7106f23cbad404   \n",
       "912             NaN  d937bc4d0722d5b45366b3c4dfde4732224bc048   \n",
       "913             NaN  3f50975c58d861e4fbd3b4fd065f0658b1aa1e10   \n",
       "914             NaN  d776ade6bb4898c032b971d2cec145976408e838   \n",
       "915             NaN  6406d384e741a6fde6f4aa5e0100bb19f7259842   \n",
       "\n",
       "                                 citedPaperId  isKeyCitation  \\\n",
       "0    5e413c7872f5df231bf4a4f694504384560e98ca          False   \n",
       "1    2cc6ff899bf17666ad35893524a4d61624555ed7          False   \n",
       "2                                        None          False   \n",
       "3    273996fbf99465211eb8306abe8c56c5835f332e          False   \n",
       "4    e33da0584b8db37816d510fd9ba7c1216858fd5f          False   \n",
       "..                                        ...            ...   \n",
       "911  ae7c65d4d7710bf5f36309faea886648f9a51d4a          False   \n",
       "912  9200a75534f44836ca7651c9d63d11b884947fa6           True   \n",
       "913  d16a1d95e6947da69797bb0cb59148057174e35a           True   \n",
       "914  22e3889f93c19c15b746c1339ce9d7439ccb632e          False   \n",
       "915  df384ef1a0677857d0118b9852bff69ee3925e06          False   \n",
       "\n",
       "                                                    id  \\\n",
       "0    8f1fbe460a901d994e9b81d69f77bfbe32719f4c>5e413...   \n",
       "1    d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...   \n",
       "2        226f798d30e5523c5b9deafb826ddb04d47c11dc>None   \n",
       "3    59dba7cd80edcce831d20b35f9eb597bba290154>27399...   \n",
       "4    0640f6e098a9d241cd680473e8705357ae101e04>e33da...   \n",
       "..                                                 ...   \n",
       "911  d9ef5d1cb543d0f1330908b36f7106f23cbad404>ae7c6...   \n",
       "912  d937bc4d0722d5b45366b3c4dfde4732224bc048>9200a...   \n",
       "913  3f50975c58d861e4fbd3b4fd065f0658b1aa1e10>d16a1...   \n",
       "914  d776ade6bb4898c032b971d2cec145976408e838>22e38...   \n",
       "915  6406d384e741a6fde6f4aa5e0100bb19f7259842>df384...   \n",
       "\n",
       "                                             unique_id  excerpt_index  \\\n",
       "0    8f1fbe460a901d994e9b81d69f77bfbe32719f4c>5e413...              0   \n",
       "1    d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...             10   \n",
       "2      226f798d30e5523c5b9deafb826ddb04d47c11dc>None_0              0   \n",
       "3    59dba7cd80edcce831d20b35f9eb597bba290154>27399...              0   \n",
       "4    0640f6e098a9d241cd680473e8705357ae101e04>e33da...              0   \n",
       "..                                                 ...            ...   \n",
       "911  d9ef5d1cb543d0f1330908b36f7106f23cbad404>ae7c6...              0   \n",
       "912  d937bc4d0722d5b45366b3c4dfde4732224bc048>9200a...              4   \n",
       "913  3f50975c58d861e4fbd3b4fd065f0658b1aa1e10>d16a1...              0   \n",
       "914  d776ade6bb4898c032b971d2cec145976408e838>22e38...              0   \n",
       "915  6406d384e741a6fde6f4aa5e0100bb19f7259842>df384...              0   \n",
       "\n",
       "     label_confidence  label2_confidence  \n",
       "0                 NaN                NaN  \n",
       "1              0.7337                NaN  \n",
       "2                 NaN                NaN  \n",
       "3              1.0000                NaN  \n",
       "4                 NaN                NaN  \n",
       "..                ...                ...  \n",
       "911               NaN                NaN  \n",
       "912            1.0000                NaN  \n",
       "913            1.0000                NaN  \n",
       "914               NaN                NaN  \n",
       "915            1.0000                NaN  \n",
       "\n",
       "[916 rows x 15 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "963242f5-e1a1-4100-b14b-8ff3fdf50896",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"./output_pretrained_test.txt\"\n",
    "with open(output_path, encoding=\"utf8\") as f:\n",
    "    output_pretrained = f.read()\n",
    "    output_pretrained = \"[\" + output_pretrained + \"]\"\n",
    "    # output_pretrained = [json.loads(x) for x in output_pretrained]\n",
    "    output_pretrained = json.loads(output_pretrained.replace('\\n', ', ')[:-3]+']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e73b84ac-a3a9-4c8d-bebb-00c09de72fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperId</th>\n",
       "      <th>citedPaperId</th>\n",
       "      <th>prediction</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2c6797dab4c118cb73197f65ba39dacc99ac743d</td>\n",
       "      <td>95c37bc99982d33873fd141ee00857160fd717a0</td>\n",
       "      <td>method</td>\n",
       "      <td>2c6797dab4c118cb73197f65ba39dacc99ac743d&gt;95c37...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fa7145adc9f8cfb8af7a189d9040c13c84ced094</td>\n",
       "      <td>20e23b4f76761d246a7c3b00b80e139e2008f77d</td>\n",
       "      <td>result</td>\n",
       "      <td>fa7145adc9f8cfb8af7a189d9040c13c84ced094&gt;20e23...</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98a8d8c0c5dae246720d4f339b88e8a9f44e3002</td>\n",
       "      <td>bd222c7ec83dadefba513738290b3624f6dd6b21</td>\n",
       "      <td>result</td>\n",
       "      <td>98a8d8c0c5dae246720d4f339b88e8a9f44e3002&gt;bd222...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aeb178ef1910a61152cd74209c28641199c82855</td>\n",
       "      <td>754c04953c261072fa367f4104e3deff082d9484</td>\n",
       "      <td>method</td>\n",
       "      <td>aeb178ef1910a61152cd74209c28641199c82855&gt;754c0...</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4d2591ac3bb65e2ec59f092884a7b15b8018592</td>\n",
       "      <td>f0fb468a54fe8021bc7986a1618222c4fcd16df4</td>\n",
       "      <td>method</td>\n",
       "      <td>e4d2591ac3bb65e2ec59f092884a7b15b8018592&gt;f0fb4...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>3cf9c7cd259a356839f42ecf143af3a8f6ef8b54</td>\n",
       "      <td>74cbd6d0eeb051b036f806d8a86c3a85859f9d7d</td>\n",
       "      <td>result</td>\n",
       "      <td>3cf9c7cd259a356839f42ecf143af3a8f6ef8b54&gt;74cbd...</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>e609824e9ea6bee5aca817238d81d1cdd6b462ad</td>\n",
       "      <td>f7bfdcf8892a561b6030ed541924551fb78acf1f</td>\n",
       "      <td>background</td>\n",
       "      <td>e609824e9ea6bee5aca817238d81d1cdd6b462ad&gt;f7bfd...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>19317f7188bc6ecad985c46277969c0ac03dbcf8</td>\n",
       "      <td>0c86f0d577f04534edc14a509a68ae80ce6fbb74</td>\n",
       "      <td>method</td>\n",
       "      <td>19317f7188bc6ecad985c46277969c0ac03dbcf8&gt;0c86f...</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>62ac94ab9227b84f1317edad1b6312e311981961</td>\n",
       "      <td>df5084196ea93af9250fae27c981ea3d7959599d</td>\n",
       "      <td>background</td>\n",
       "      <td>62ac94ab9227b84f1317edad1b6312e311981961&gt;df508...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>c49e8f550ff648b75835c2b0b359d2e5a5d7140e</td>\n",
       "      <td>815c1d29608d7365b396acf880138ceddd38119f</td>\n",
       "      <td>method</td>\n",
       "      <td>c49e8f550ff648b75835c2b0b359d2e5a5d7140e&gt;815c1...</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1861 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 citingPaperId  \\\n",
       "0     2c6797dab4c118cb73197f65ba39dacc99ac743d   \n",
       "1     fa7145adc9f8cfb8af7a189d9040c13c84ced094   \n",
       "2     98a8d8c0c5dae246720d4f339b88e8a9f44e3002   \n",
       "3     aeb178ef1910a61152cd74209c28641199c82855   \n",
       "4     e4d2591ac3bb65e2ec59f092884a7b15b8018592   \n",
       "...                                        ...   \n",
       "1856  3cf9c7cd259a356839f42ecf143af3a8f6ef8b54   \n",
       "1857  e609824e9ea6bee5aca817238d81d1cdd6b462ad   \n",
       "1858  19317f7188bc6ecad985c46277969c0ac03dbcf8   \n",
       "1859  62ac94ab9227b84f1317edad1b6312e311981961   \n",
       "1860  c49e8f550ff648b75835c2b0b359d2e5a5d7140e   \n",
       "\n",
       "                                  citedPaperId  prediction  \\\n",
       "0     95c37bc99982d33873fd141ee00857160fd717a0      method   \n",
       "1     20e23b4f76761d246a7c3b00b80e139e2008f77d      result   \n",
       "2     bd222c7ec83dadefba513738290b3624f6dd6b21      result   \n",
       "3     754c04953c261072fa367f4104e3deff082d9484      method   \n",
       "4     f0fb468a54fe8021bc7986a1618222c4fcd16df4      method   \n",
       "...                                        ...         ...   \n",
       "1856  74cbd6d0eeb051b036f806d8a86c3a85859f9d7d      result   \n",
       "1857  f7bfdcf8892a561b6030ed541924551fb78acf1f  background   \n",
       "1858  0c86f0d577f04534edc14a509a68ae80ce6fbb74      method   \n",
       "1859  df5084196ea93af9250fae27c981ea3d7959599d  background   \n",
       "1860  815c1d29608d7365b396acf880138ceddd38119f      method   \n",
       "\n",
       "                                              unique_id       label  \n",
       "0     2c6797dab4c118cb73197f65ba39dacc99ac743d>95c37...  background  \n",
       "1     fa7145adc9f8cfb8af7a189d9040c13c84ced094>20e23...      result  \n",
       "2     98a8d8c0c5dae246720d4f339b88e8a9f44e3002>bd222...  background  \n",
       "3     aeb178ef1910a61152cd74209c28641199c82855>754c0...      method  \n",
       "4     e4d2591ac3bb65e2ec59f092884a7b15b8018592>f0fb4...  background  \n",
       "...                                                 ...         ...  \n",
       "1856  3cf9c7cd259a356839f42ecf143af3a8f6ef8b54>74cbd...      result  \n",
       "1857  e609824e9ea6bee5aca817238d81d1cdd6b462ad>f7bfd...  background  \n",
       "1858  19317f7188bc6ecad985c46277969c0ac03dbcf8>0c86f...      method  \n",
       "1859  62ac94ab9227b84f1317edad1b6312e311981961>df508...  background  \n",
       "1860  c49e8f550ff648b75835c2b0b359d2e5a5d7140e>815c1...      method  \n",
       "\n",
       "[1861 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_predict = pd.DataFrame(output_pretrained)\n",
    "df_dev_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd0e8063-daf2-498b-829d-80e34f9ab00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperId</th>\n",
       "      <th>citedPaperId</th>\n",
       "      <th>prediction</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2c6797dab4c118cb73197f65ba39dacc99ac743d</td>\n",
       "      <td>95c37bc99982d33873fd141ee00857160fd717a0</td>\n",
       "      <td>method</td>\n",
       "      <td>2c6797dab4c118cb73197f65ba39dacc99ac743d&gt;95c37...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fa7145adc9f8cfb8af7a189d9040c13c84ced094</td>\n",
       "      <td>20e23b4f76761d246a7c3b00b80e139e2008f77d</td>\n",
       "      <td>result</td>\n",
       "      <td>fa7145adc9f8cfb8af7a189d9040c13c84ced094&gt;20e23...</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98a8d8c0c5dae246720d4f339b88e8a9f44e3002</td>\n",
       "      <td>bd222c7ec83dadefba513738290b3624f6dd6b21</td>\n",
       "      <td>result</td>\n",
       "      <td>98a8d8c0c5dae246720d4f339b88e8a9f44e3002&gt;bd222...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aeb178ef1910a61152cd74209c28641199c82855</td>\n",
       "      <td>754c04953c261072fa367f4104e3deff082d9484</td>\n",
       "      <td>method</td>\n",
       "      <td>aeb178ef1910a61152cd74209c28641199c82855&gt;754c0...</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4d2591ac3bb65e2ec59f092884a7b15b8018592</td>\n",
       "      <td>f0fb468a54fe8021bc7986a1618222c4fcd16df4</td>\n",
       "      <td>method</td>\n",
       "      <td>e4d2591ac3bb65e2ec59f092884a7b15b8018592&gt;f0fb4...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>3cf9c7cd259a356839f42ecf143af3a8f6ef8b54</td>\n",
       "      <td>74cbd6d0eeb051b036f806d8a86c3a85859f9d7d</td>\n",
       "      <td>result</td>\n",
       "      <td>3cf9c7cd259a356839f42ecf143af3a8f6ef8b54&gt;74cbd...</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>e609824e9ea6bee5aca817238d81d1cdd6b462ad</td>\n",
       "      <td>f7bfdcf8892a561b6030ed541924551fb78acf1f</td>\n",
       "      <td>background</td>\n",
       "      <td>e609824e9ea6bee5aca817238d81d1cdd6b462ad&gt;f7bfd...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>19317f7188bc6ecad985c46277969c0ac03dbcf8</td>\n",
       "      <td>0c86f0d577f04534edc14a509a68ae80ce6fbb74</td>\n",
       "      <td>method</td>\n",
       "      <td>19317f7188bc6ecad985c46277969c0ac03dbcf8&gt;0c86f...</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>62ac94ab9227b84f1317edad1b6312e311981961</td>\n",
       "      <td>df5084196ea93af9250fae27c981ea3d7959599d</td>\n",
       "      <td>background</td>\n",
       "      <td>62ac94ab9227b84f1317edad1b6312e311981961&gt;df508...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>c49e8f550ff648b75835c2b0b359d2e5a5d7140e</td>\n",
       "      <td>815c1d29608d7365b396acf880138ceddd38119f</td>\n",
       "      <td>method</td>\n",
       "      <td>c49e8f550ff648b75835c2b0b359d2e5a5d7140e&gt;815c1...</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1856 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 citingPaperId  \\\n",
       "0     2c6797dab4c118cb73197f65ba39dacc99ac743d   \n",
       "1     fa7145adc9f8cfb8af7a189d9040c13c84ced094   \n",
       "2     98a8d8c0c5dae246720d4f339b88e8a9f44e3002   \n",
       "3     aeb178ef1910a61152cd74209c28641199c82855   \n",
       "4     e4d2591ac3bb65e2ec59f092884a7b15b8018592   \n",
       "...                                        ...   \n",
       "1856  3cf9c7cd259a356839f42ecf143af3a8f6ef8b54   \n",
       "1857  e609824e9ea6bee5aca817238d81d1cdd6b462ad   \n",
       "1858  19317f7188bc6ecad985c46277969c0ac03dbcf8   \n",
       "1859  62ac94ab9227b84f1317edad1b6312e311981961   \n",
       "1860  c49e8f550ff648b75835c2b0b359d2e5a5d7140e   \n",
       "\n",
       "                                  citedPaperId  prediction  \\\n",
       "0     95c37bc99982d33873fd141ee00857160fd717a0      method   \n",
       "1     20e23b4f76761d246a7c3b00b80e139e2008f77d      result   \n",
       "2     bd222c7ec83dadefba513738290b3624f6dd6b21      result   \n",
       "3     754c04953c261072fa367f4104e3deff082d9484      method   \n",
       "4     f0fb468a54fe8021bc7986a1618222c4fcd16df4      method   \n",
       "...                                        ...         ...   \n",
       "1856  74cbd6d0eeb051b036f806d8a86c3a85859f9d7d      result   \n",
       "1857  f7bfdcf8892a561b6030ed541924551fb78acf1f  background   \n",
       "1858  0c86f0d577f04534edc14a509a68ae80ce6fbb74      method   \n",
       "1859  df5084196ea93af9250fae27c981ea3d7959599d  background   \n",
       "1860  815c1d29608d7365b396acf880138ceddd38119f      method   \n",
       "\n",
       "                                              unique_id       label  \n",
       "0     2c6797dab4c118cb73197f65ba39dacc99ac743d>95c37...  background  \n",
       "1     fa7145adc9f8cfb8af7a189d9040c13c84ced094>20e23...      result  \n",
       "2     98a8d8c0c5dae246720d4f339b88e8a9f44e3002>bd222...  background  \n",
       "3     aeb178ef1910a61152cd74209c28641199c82855>754c0...      method  \n",
       "4     e4d2591ac3bb65e2ec59f092884a7b15b8018592>f0fb4...  background  \n",
       "...                                                 ...         ...  \n",
       "1856  3cf9c7cd259a356839f42ecf143af3a8f6ef8b54>74cbd...      result  \n",
       "1857  e609824e9ea6bee5aca817238d81d1cdd6b462ad>f7bfd...  background  \n",
       "1858  19317f7188bc6ecad985c46277969c0ac03dbcf8>0c86f...      method  \n",
       "1859  62ac94ab9227b84f1317edad1b6312e311981961>df508...  background  \n",
       "1860  c49e8f550ff648b75835c2b0b359d2e5a5d7140e>815c1...      method  \n",
       "\n",
       "[1856 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_dev_predict.dropna(subset=['prediction'])\n",
    "df_dev_predict = df_dev_predict[~(df_dev_predict['prediction'] == '')]\n",
    "df_dev_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "201539ae-6224-4d51-99db-05e87f357d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev_merged = df_dev.merge(df_dev_predict, how='left', left_on=\"unique_id\", right_on=\"unique_id\").dropna(subset=['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "040f58d0-da6b-4d4c-b778-d2bb377e6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_res(df_row):\n",
    "#     if df_row[\"label\"] == df_row[\"prediction\"]:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2ffea5cd-3310-4980-a24a-02c75a0b5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev_merged[\"correct_pred\"] = df_dev_merged.apply(compare_res, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "461f65da-cb6d-4775-ae50-aad17a77f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev_merged[df_dev_merged[\"correct_pred\"] == True][\"correct_pred\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32fc3c6a-57fa-4485-a875-39da2d489f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev_merged[\"correct_pred\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c7e3adf2-b892-46cc-a31e-7a55a99bec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6d35d69-0aa4-4b80-a5bb-a7506d4b4677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.88      0.87      0.88       992\n",
      "      method       0.88      0.81      0.85       605\n",
      "      result       0.71      0.86      0.78       259\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1856\n",
      "   macro avg       0.82      0.85      0.83      1856\n",
      "weighted avg       0.86      0.85      0.85      1856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_dev_predict['label'], df_dev_predict['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26ddd61c-c9e6-43e9-910f-96ffd9111e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8328905208191819\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_dev_predict['label'], df_dev_predict['prediction'], average='macro'))\n",
    "# df_dev_predict['prediction'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2283854-8b1d-4d8d-8725-0fff02a09801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev_merged[df_dev_merged['prediction']!= df_dev_merged['prediction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444d42b-00a5-4dd7-b095-9f17ba5b7e74",
   "metadata": {},
   "source": [
    "## Experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a78d9023-d367-4213-99ca-6af09415b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_pairs = [(0,0), (0.05,0.05), (0.1,0.1), (0.1, 0.2), (0.1, 0.3), (0.2, 0.2), (0.3, 0.3)]\n",
    "\n",
    "# for lamb in lambda_pairs:\n",
    "#     print(lamb)\n",
    "#     with open(f\"./runs/experiments-_{lamb[0]}_{lamb[1]}/metrics.json\", encoding=\"utf8\") as f:\n",
    "#         metrics = f.read()\n",
    "#         metrics = json.loads(metrics)\n",
    "#     print(metrics['best_validation_average_F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c0e645-bf47-4af7-8bbe-c1caae791a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./runs/experiment-0.05-0.05-w2v-1/metrics.json\n",
      "0.8178103926306495\n",
      "./runs/experiment-0.05-0.05-citetokens/metrics.json\n",
      "0.8195389041330493\n",
      "./runs/experiments-_0.1_0.1/metrics.json\n",
      "0.8131806035859791\n",
      "./runs/experiments-_0.1_0.3/metrics.json\n",
      "0.8077549709636348\n",
      "./runs/experiments-_0.3_0.3/metrics.json\n",
      "0.8186008842495505\n",
      "./runs/experiments-_0_0/metrics.json\n",
      "0.8273689041357627\n",
      "./runs/experiment-0.05-0.05-numtokens2/metrics.json\n",
      "0.8116501788703033\n",
      "./runs/experiment-0.05-0.05-removecite/metrics.json\n",
      "0.8194678580833311\n",
      "./runs/experiment-0.05-0.05-numtokens/metrics.json\n",
      "0.8332833985516181\n",
      "./runs/experiment-0.05-0.05-elmo-lstm/metrics.json\n",
      "0.8194471827306655\n",
      "./runs/experiment-0.05-0.05-elmo-forwardgru/metrics.json\n",
      "0.81613775692173\n",
      "./runs/experiments-_0.2_0.2/metrics.json\n",
      "0.8216554689057259\n",
      "./runs/experiments-_0.05_0.05/metrics.json\n",
      "0.8294245106130006\n",
      "./runs/experiment-0.05-0.05-elmo-forwardgru-noattention/metrics.json\n",
      "0.3827258501262281\n",
      "./runs/experiment-0.05-0.05-no-elmo/metrics.json\n",
      "0.7909244219588548\n",
      "./runs/experiments-_0.1_0.2/metrics.json\n",
      "0.8185485261755949\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for fl in glob.glob(\"./runs/experiment*/metrics.json\"):\n",
    "    print(fl)\n",
    "    with open(fl, encoding=\"utf8\") as f:\n",
    "        metrics = f.read()\n",
    "        metrics = json.loads(metrics)\n",
    "    try:\n",
    "        print(metrics['best_validation_average_F1'])\n",
    "    except:\n",
    "        print(metrics['validation_average_F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "861b2e3a-c2f0-4a6f-981b-0c72c1b5efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./runs/experiment-0.05-0.05-w2v-1/metrics.json\n",
      "0.8310028720107511\n",
      "./runs/experiment-0.05-0.05-citetokens/metrics.json\n",
      "0.8204258964176881\n",
      "./runs/experiments-_0.1_0.1/metrics.json\n",
      "0.8203714018464888\n",
      "./runs/experiments-_0.1_0.3/metrics.json\n",
      "0.8050099186479746\n",
      "./runs/experiments-_0.3_0.3/metrics.json\n",
      "0.8320188112872264\n",
      "./runs/experiments-_0_0/metrics.json\n",
      "0.8231164879744503\n",
      "./runs/experiment-0.05-0.05-numtokens2/metrics.json\n",
      "0.8330661047322474\n",
      "./runs/experiment-0.05-0.05-removecite/metrics.json\n",
      "0.8277156189814104\n",
      "./runs/experiment-0.05-0.05-numtokens/metrics.json\n",
      "0.8064126960158635\n",
      "./runs/experiment-0.05-0.05-elmo-lstm/metrics.json\n",
      "0.8146276763446183\n",
      "./runs/experiment-0.05-0.05-elmo-forwardgru/metrics.json\n",
      "0.825328793589981\n",
      "./runs/experiments-_0.2_0.2/metrics.json\n",
      "0.8338795863810212\n",
      "./runs/experiments-_0.05_0.05/metrics.json\n",
      "0.8276466110976536\n",
      "./runs/experiment-0.05-0.05-elmo-forwardgru-noattention/metrics.json\n",
      "0.38035496254387585\n",
      "./runs/experiment-0.05-0.05-no-elmo/metrics.json\n",
      "0.76827555721624\n",
      "./runs/experiments-_0.1_0.2/metrics.json\n",
      "0.813433642029134\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for fl in glob.glob(\"./runs/experiment*/metrics.json\"):\n",
    "    print(fl)\n",
    "    with open(fl, encoding=\"utf8\") as f:\n",
    "        metrics = f.read()\n",
    "        metrics = json.loads(metrics)\n",
    "    print(metrics['test_average_F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab94865-7191-4611-92c2-843d0a555f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_state_path = os.path.join(serialization_dir, 'best.th')\n",
    "# best_model_state = torch.load(best_model_state_path)\n",
    "# best_model = model\n",
    "# best_model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd1e6d-1de7-4bc8-8db4-9dfbf622800f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248-scicite-torch",
   "language": "python",
   "name": "cs4248-scicite-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

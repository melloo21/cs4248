{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm,trange\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "# from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melloo21/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all dataset\n",
    "path = \"/Users/melloo21/Desktop/NUS Items/CS4248/2024_CS4248/Project/ImpactCite/ImpactCite_Intent/scicite_data/*.jsonl\"\n",
    "all_scicite = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in glob.glob(path):\n",
    "    name = g.split(\"/\")[-1].split(\".\")[0]\n",
    "    all_scicite[name] = pd.read_json(path_or_buf=g, lines=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8243 entries, 0 to 8242\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   source             8243 non-null   object \n",
      " 1   citeEnd            8241 non-null   float64\n",
      " 2   sectionName        8243 non-null   object \n",
      " 3   citeStart          8241 non-null   float64\n",
      " 4   string             8243 non-null   object \n",
      " 5   label              8243 non-null   object \n",
      " 6   label_confidence   6137 non-null   float64\n",
      " 7   citingPaperId      8243 non-null   object \n",
      " 8   citedPaperId       8243 non-null   object \n",
      " 9   isKeyCitation      8243 non-null   bool   \n",
      " 10  id                 8243 non-null   object \n",
      " 11  unique_id          8243 non-null   object \n",
      " 12  excerpt_index      8243 non-null   int64  \n",
      " 13  label2             835 non-null    object \n",
      " 14  label2_confidence  317 non-null    float64\n",
      "dtypes: bool(1), float64(4), int64(1), object(9)\n",
      "memory usage: 909.8+ KB\n"
     ]
    }
   ],
   "source": [
    "all_scicite[\"train\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citeEnd</th>\n",
       "      <th>citeStart</th>\n",
       "      <th>label_confidence</th>\n",
       "      <th>excerpt_index</th>\n",
       "      <th>label2_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8241.000000</td>\n",
       "      <td>8241.000000</td>\n",
       "      <td>6137.000000</td>\n",
       "      <td>8243.000000</td>\n",
       "      <td>317.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>163.965781</td>\n",
       "      <td>139.709744</td>\n",
       "      <td>0.876606</td>\n",
       "      <td>2.333495</td>\n",
       "      <td>0.741582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>121.478095</td>\n",
       "      <td>79.479383</td>\n",
       "      <td>0.144244</td>\n",
       "      <td>3.326236</td>\n",
       "      <td>0.191581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>103.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.744700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>152.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3305.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           citeEnd    citeStart  label_confidence  excerpt_index  \\\n",
       "count  8241.000000  8241.000000       6137.000000    8243.000000   \n",
       "mean    163.965781   139.709744          0.876606       2.333495   \n",
       "std     121.478095    79.479383          0.144244       3.326236   \n",
       "min       3.000000     0.000000          0.600000       0.000000   \n",
       "25%     103.000000    87.000000          0.744700       0.000000   \n",
       "50%     152.000000   134.000000          1.000000       1.000000   \n",
       "75%     208.000000   187.000000          1.000000       3.000000   \n",
       "max    3305.000000  1184.000000          1.000000      19.000000   \n",
       "\n",
       "       label2_confidence  \n",
       "count         317.000000  \n",
       "mean            0.741582  \n",
       "std             0.191581  \n",
       "min             0.249800  \n",
       "25%             0.616900  \n",
       "50%             0.738500  \n",
       "75%             1.000000  \n",
       "max             1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scicite[\"train\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='count', ylabel='label'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(all_scicite[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Citation input\n",
    "{\"source\": \"explicit\", \n",
    "\n",
    "\"citeEnd\": 175, \n",
    "\n",
    "\"sectionName\": \"Introduction\",\n",
    "\n",
    "\"citeStart\": 168, \n",
    "\n",
    "\"string\": \"However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as \n",
    "\n",
    "direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).\",\n",
    "\n",
    "\"label\": \"background\", \n",
    "\n",
    "\"label_confidence\": 1.0,\n",
    "\n",
    "\"citingPaperId\": \"1872080baa7d30ec8fb87be9a65358cd3a7fb649\", \n",
    "\n",
    "\"citedPaperId\": \"894be9b4ea46a5c422e81ef3c241072d4c73fdc0\", \n",
    "\n",
    "\"isKeyCitation\": true, \n",
    "\n",
    "\"id\": \"1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be9b4ea46a5c422e81ef3c241072d4c73fdc0\", \n",
    "\n",
    "\"unique_id\": \"1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be9b4ea46a5c422e81ef3c241072d4c73fdc0_11\",\n",
    "\n",
    "\"excerpt_index\": 11\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation Classes\n",
    "\n",
    "Background information - The citation states, mentions, or points to the background\n",
    "information giving more context about a problem, concept,\n",
    "approach, topic, or importance of the problem in the field.\n",
    "\n",
    "EXAMPLES: Recent evidence suggests that co-occurring alexithymia may explain deficits [12].\n",
    "Locally high-temperature melting regions can act as permanent termination sites [6-9].\n",
    "One line of work is focused on changing the objective function (Mao et al., 2016).\n",
    "\n",
    "Method - Making use of a method, tool, approach or dataset\n",
    "\n",
    "EXAMPLES: Fold differences were calculated by a mathematical model described in [4].\n",
    "We use Orthogonal Initialization (Saxe et al., 2014)\n",
    "\n",
    "Result comparison - Comparison of the paper’s results/findings with the\n",
    "results/findings of other work\n",
    "\n",
    "EXAMPLES: Weighted measurements were superior to T2-weighted contrast imaging which was in\n",
    "accordance with former studies [25-27]\n",
    "Similar results to our study were reported in the study of Lee et al (2010).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source', 'citeEnd', 'sectionName', 'citeStart', 'string', 'label',\n",
       "       'label_confidence', 'citingPaperId', 'citedPaperId', 'isKeyCitation',\n",
       "       'id', 'unique_id', 'excerpt_index', 'label2', 'label2_confidence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scicite[\"train\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperId</th>\n",
       "      <th>citedPaperId</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000549bb992125eecace11190d748278e53696ce</td>\n",
       "      <td>f6461aca4c7e239191580e0b59c5d4c93fd67b87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000d8d128bc8b44b25970374b4ac274b83e5cfc1</td>\n",
       "      <td>d610bcbd3f209564b658758dfa2be77a4517d658</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000ea376d16a73c3efdafc53068b95cc67ebc3ae</td>\n",
       "      <td>74533ebddadaaceb98d3bef47a919c515575b7f6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0010f312c554ae9586ef06a888ab56495bc37e17</td>\n",
       "      <td>5453aa977626eb6b4bb5dd31ed274a4e1d983c03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0012294b23155a9bc58c1b0c367111d7ef94798a</td>\n",
       "      <td>a643c7684944633c3c5b63d72316bf87d7f9a2f6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4686</th>\n",
       "      <td>ffbf501638a71327a4814a864a5ce4d2a52a42cd</td>\n",
       "      <td>ff025a7e007ee07a19d5cbfdaa26afb94aa08b92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4687</th>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2</td>\n",
       "      <td>de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4688</th>\n",
       "      <td>ffdddf8a39bd1b0dd65cfb0e090ee6337d5b2350</td>\n",
       "      <td>9714062de869e00c2ab31289b049b2f21c11e5fd</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>ffe3fc22a46718e6f702bbc13b87c3ff41c923fe</td>\n",
       "      <td>e403aa22bfa9b8c451749218c3670c25add6a027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>ffe9fbf5e1db48cdeb03b2790249a3ee0ecc960a</td>\n",
       "      <td>b4f3597485b8e161af04ce27f3d23ad2cbe90ccc</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4691 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 citingPaperId  \\\n",
       "0     000549bb992125eecace11190d748278e53696ce   \n",
       "1     000d8d128bc8b44b25970374b4ac274b83e5cfc1   \n",
       "2     000ea376d16a73c3efdafc53068b95cc67ebc3ae   \n",
       "3     0010f312c554ae9586ef06a888ab56495bc37e17   \n",
       "4     0012294b23155a9bc58c1b0c367111d7ef94798a   \n",
       "...                                        ...   \n",
       "4686  ffbf501638a71327a4814a864a5ce4d2a52a42cd   \n",
       "4687  ffda3e25293d09832e8a6e7b1378b1670af725b2   \n",
       "4688  ffdddf8a39bd1b0dd65cfb0e090ee6337d5b2350   \n",
       "4689  ffe3fc22a46718e6f702bbc13b87c3ff41c923fe   \n",
       "4690  ffe9fbf5e1db48cdeb03b2790249a3ee0ecc960a   \n",
       "\n",
       "                                  citedPaperId  size  \n",
       "0     f6461aca4c7e239191580e0b59c5d4c93fd67b87     1  \n",
       "1     d610bcbd3f209564b658758dfa2be77a4517d658     2  \n",
       "2     74533ebddadaaceb98d3bef47a919c515575b7f6     1  \n",
       "3     5453aa977626eb6b4bb5dd31ed274a4e1d983c03     1  \n",
       "4     a643c7684944633c3c5b63d72316bf87d7f9a2f6     4  \n",
       "...                                        ...   ...  \n",
       "4686  ff025a7e007ee07a19d5cbfdaa26afb94aa08b92     1  \n",
       "4687  de5d6cbd75278792715b53a57b73910d69d0abc8     5  \n",
       "4688  9714062de869e00c2ab31289b049b2f21c11e5fd     3  \n",
       "4689  e403aa22bfa9b8c451749218c3670c25add6a027     2  \n",
       "4690  b4f3597485b8e161af04ce27f3d23ad2cbe90ccc     5  \n",
       "\n",
       "[4691 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scicite[\"train\"].groupby(['citingPaperId', 'citedPaperId'],as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>citeEnd</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>citeStart</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>label_confidence</th>\n",
       "      <th>citingPaperId</th>\n",
       "      <th>citedPaperId</th>\n",
       "      <th>isKeyCitation</th>\n",
       "      <th>id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>excerpt_index</th>\n",
       "      <th>label2</th>\n",
       "      <th>label2_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>explicit</td>\n",
       "      <td>269.0</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>247.0</td>\n",
       "      <td>…of contractile stress fibers (SFs), transmembrane actin-associated (TAN) lines on the dorsal cell surface that position the nucleus, filopodia at the leading edge, isotropic cortical actin networks, and actin important for mitochondrial fission (Skau and Waterman 2015; Campellone and Welch 2010).</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2</td>\n",
       "      <td>de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>True</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8_1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>explicit</td>\n",
       "      <td>161.0</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>137.0</td>\n",
       "      <td>The regulation and localization of actin nucleation factors are likely critical for specifying the function of actin structures in cells (Skau and Waterman 2015).</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2</td>\n",
       "      <td>de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>True</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8_0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641</th>\n",
       "      <td>explicit</td>\n",
       "      <td>331.0</td>\n",
       "      <td>INTRODUCTION</td>\n",
       "      <td>280.0</td>\n",
       "      <td>Specific formins mediate formation of contractile stress fibers (SFs), transmembrane actin-associated (TAN) lines on the dorsal cell surface that position the nucleus, filopodia at the leading edge, isotropic cortical actin networks, and actin important for mitochondrial fission (Skau and Waterman 2015; Campellone and Welch 2010).</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2</td>\n",
       "      <td>de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>True</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8_2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244</th>\n",
       "      <td>explicit</td>\n",
       "      <td>292.0</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>270.0</td>\n",
       "      <td>…Actin to Protect Nuclei during Confined Migration and Promote Metastasis, Cell (2016), http://dx.doi.org/10.1016/j.cell.2016.10.023\\nThe regulation and localization of actin nucleation factors are likely critical for specifying the function of actin structures in cells (Skau and Waterman 2015).</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2</td>\n",
       "      <td>de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>True</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8_4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5604</th>\n",
       "      <td>explicit</td>\n",
       "      <td>247.0</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>225.0</td>\n",
       "      <td>Individual members of the formin family have been shown to mediate formation of functionally distinct actin structures in cells, including SFs, filopodia, isotropic cortical actin networks, and mitochondria-associated actin (Skau and Waterman 2015; Campellone and Welch 2010).</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2</td>\n",
       "      <td>de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>True</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8</td>\n",
       "      <td>ffda3e25293d09832e8a6e7b1378b1670af725b2&gt;de5d6cbd75278792715b53a57b73910d69d0abc8_3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  citeEnd   sectionName  citeStart  \\\n",
       "433   explicit    269.0  INTRODUCTION      247.0   \n",
       "1506  explicit    161.0    DISCUSSION      137.0   \n",
       "4641  explicit    331.0  INTRODUCTION      280.0   \n",
       "5244  explicit    292.0    DISCUSSION      270.0   \n",
       "5604  explicit    247.0    DISCUSSION      225.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                            string  \\\n",
       "433                                     …of contractile stress fibers (SFs), transmembrane actin-associated (TAN) lines on the dorsal cell surface that position the nucleus, filopodia at the leading edge, isotropic cortical actin networks, and actin important for mitochondrial fission (Skau and Waterman 2015; Campellone and Welch 2010).   \n",
       "1506                                                                                                                                                                            The regulation and localization of actin nucleation factors are likely critical for specifying the function of actin structures in cells (Skau and Waterman 2015).   \n",
       "4641  Specific formins mediate formation of contractile stress fibers (SFs), transmembrane actin-associated (TAN) lines on the dorsal cell surface that position the nucleus, filopodia at the leading edge, isotropic cortical actin networks, and actin important for mitochondrial fission (Skau and Waterman 2015; Campellone and Welch 2010).   \n",
       "5244                                      …Actin to Protect Nuclei during Confined Migration and Promote Metastasis, Cell (2016), http://dx.doi.org/10.1016/j.cell.2016.10.023\\nThe regulation and localization of actin nucleation factors are likely critical for specifying the function of actin structures in cells (Skau and Waterman 2015).   \n",
       "5604                                                          Individual members of the formin family have been shown to mediate formation of functionally distinct actin structures in cells, including SFs, filopodia, isotropic cortical actin networks, and mitochondria-associated actin (Skau and Waterman 2015; Campellone and Welch 2010).   \n",
       "\n",
       "           label  label_confidence                             citingPaperId  \\\n",
       "433   background               1.0  ffda3e25293d09832e8a6e7b1378b1670af725b2   \n",
       "1506  background               1.0  ffda3e25293d09832e8a6e7b1378b1670af725b2   \n",
       "4641  background               1.0  ffda3e25293d09832e8a6e7b1378b1670af725b2   \n",
       "5244  background               1.0  ffda3e25293d09832e8a6e7b1378b1670af725b2   \n",
       "5604  background               1.0  ffda3e25293d09832e8a6e7b1378b1670af725b2   \n",
       "\n",
       "                                  citedPaperId  isKeyCitation  \\\n",
       "433   de5d6cbd75278792715b53a57b73910d69d0abc8           True   \n",
       "1506  de5d6cbd75278792715b53a57b73910d69d0abc8           True   \n",
       "4641  de5d6cbd75278792715b53a57b73910d69d0abc8           True   \n",
       "5244  de5d6cbd75278792715b53a57b73910d69d0abc8           True   \n",
       "5604  de5d6cbd75278792715b53a57b73910d69d0abc8           True   \n",
       "\n",
       "                                                                                     id  \\\n",
       "433   ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8   \n",
       "1506  ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8   \n",
       "4641  ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8   \n",
       "5244  ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8   \n",
       "5604  ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8   \n",
       "\n",
       "                                                                                unique_id  \\\n",
       "433   ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8_1   \n",
       "1506  ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8_0   \n",
       "4641  ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8_2   \n",
       "5244  ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8_4   \n",
       "5604  ffda3e25293d09832e8a6e7b1378b1670af725b2>de5d6cbd75278792715b53a57b73910d69d0abc8_3   \n",
       "\n",
       "      excerpt_index label2  label2_confidence  \n",
       "433               1    NaN                NaN  \n",
       "1506              0    NaN                NaN  \n",
       "4641              2    NaN                NaN  \n",
       "5244              4    NaN                NaN  \n",
       "5604              3    NaN                NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citingPaperId_ref = \"ffda3e25293d09832e8a6e7b1378b1670af725b2\"\n",
    "citedPaperId_ref = \"de5d6cbd75278792715b53a57b73910d69d0abc8\"\n",
    "all_scicite[\"train\"][(all_scicite[\"train\"].citingPaperId == citingPaperId_ref) \n",
    "            & (all_scicite[\"train\"].citedPaperId == citedPaperId_ref)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw train data\n",
    "df_train = all_scicite[\"train\"][[\"label\", \"string\"]]\n",
    "# Mapping labels into numerical classes e.g. \n",
    "mapping_dict = {'background': 0, 'method': 1, 'result': 2}\n",
    "\n",
    "df_train.loc[:,\"label\"] = df_train.label.map(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Getting rough counts of all string lenghts to be applied to the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m stringlengths \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Getting rough counts of all string lenghts to be applied to the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m stringlengths \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mstring\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/spacy/ml/tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/spacy/ml/parser_model.pyx:263\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/spacy/util.py:1148\u001b[0m, in \u001b[0;36mget_cuda_stream\u001b[0;34m(require, non_blocking)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     same_code \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsourcelines(func1) \u001b[38;5;241m==\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsourcelines(func2)\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m same_name \u001b[38;5;129;01mand\u001b[39;00m same_file \u001b[38;5;129;01mand\u001b[39;00m same_code\n\u001b[0;32m-> 1148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cuda_stream\u001b[39m(\n\u001b[1;32m   1149\u001b[0m     require: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[CudaStream]:\n\u001b[1;32m   1151\u001b[0m     ops \u001b[38;5;241m=\u001b[39m get_current_ops()\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CudaStream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Getting rough counts of all string lenghts to be applied to the model\n",
    "stringlengths = df_train.string.apply(lambda x: len(nlp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7037     10\n",
       "4071     11\n",
       "2143     13\n",
       "2148     13\n",
       "4134     14\n",
       "       ... \n",
       "7740    450\n",
       "598     465\n",
       "4001    465\n",
       "6874    500\n",
       "6540    594\n",
       "Name: string, Length: 8243, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringlengths.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1998), to provide great £exibility in testing evolutionary hypotheses (Huelsenbeck & Rannala 1997), \\nand simply to give excellent results in the inference of evolutionary relationships (Kuhner & Felsenstein 1994;\\n Huelsenbeck 1995). Constraints imposed by computational complexity are becoming less restrictive as computers \\n improve. Given the recognition of phylogenetic inference as being inherently statistical in nature, \\n it is surprising that so little attention has been paid to experimental design. A number of topics \\n of relevance to experimental design in phylogenetics have been discussed previously. There is something \\n of a folklore surrounding the choice of genes or other genomic regions for investigating particular \\n evolutionary questions, but almost no published quantitative results. It is widely accepted that \\n sequences that have undergone very little evolutionary change since their divergence from a common ancestor, \\n through low substitution rates or short evolutionary times, will exhibit too few di¡erences to contain \\n useful evolutionary information. Equally, sequences that have undergone very large amounts of change \\n(high rates or long times) become `saturated' with changes and no evolutionary signal is detectable amid the noise. \\nConsequently, a happy medium is expected at some intermediate level of sequence divergence, but the àsymptotic' \\nresults (for extreme high and low levels of divergence) give no clue as to where this lies.\\nAlthough there is considerable experience with particular genes and organisms (see Hillis et al. \\n(1996, pp. 336^339) for an extensive list of studies), where comparisons among genes have been made \\nthey tend to be evaluated empirically by the congruence of the results obtained among themselves and with\\nresearchers'a priori expectations. In addition, a large laboratory e¡ort is needed before even these \\nqualitative conclusions can be reached. The methods introduced in this paper can quantify the e¡ects of\\nvarying levels of divergence. They con¢rm the belief that intermediate levels of sequence divergence are \\nmost useful, and are able to give estimates of optimal levels of divergence. The necessary analyses can\\nbe done before any data are collected. The only other method for assessing which genomic regions are\\nlikely to be most useful in phylogenetic questions is that of Yang (1998). This uses simulation \\nto estimate probabilities of successful tree-topology inference. The approach may be extremely \\ntime-consuming for realistic problems. The choice of taxa to include in phylogenetic studies has\\nalso rarely been discussed. Li et al. (1987) considered the e¡ects of adding outgroup taxa,\\nand Ritland & Clegg (1990) and Maddison et al. (1992) are agreed that if outgroups are to be added, \\nthey should not be too distantly related to the ingroup taxa (a conclusion that is con¢rmed in this paper). \\nMore recently, consideration of the estimation of large phylogenies has led to the contradictory \\nadvice that the number of sequences included in a study be reduced (Kim 1996) and increased (particularly to \\nbreak long branches in trees (Hillis 1996)) in order to improve inferences (see also Hillis 1998). \\nAlthough it is not clear that the latter strategy will always be successful (Zharkikh & Li 1993; Kim 1996),\\n    Strimmer & von Haeseler (1996) and Hillis (1996) have\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[6540]\n",
    "\"\"\"\n",
    "1998), to provide great £exibility in testing evolutionary hypotheses (Huelsenbeck & Rannala 1997), \n",
    "and simply to give excellent results in the inference of evolutionary relationships (Kuhner & Felsenstein 1994;\n",
    " Huelsenbeck 1995). Constraints imposed by computational complexity are becoming less restrictive as computers \n",
    " improve. Given the recognition of phylogenetic inference as being inherently statistical in nature, \n",
    " it is surprising that so little attention has been paid to experimental design. A number of topics \n",
    " of relevance to experimental design in phylogenetics have been discussed previously. There is something \n",
    " of a folklore surrounding the choice of genes or other genomic regions for investigating particular \n",
    " evolutionary questions, but almost no published quantitative results. It is widely accepted that \n",
    " sequences that have undergone very little evolutionary change since their divergence from a common ancestor, \n",
    " through low substitution rates or short evolutionary times, will exhibit too few di¡erences to contain \n",
    " useful evolutionary information. Equally, sequences that have undergone very large amounts of change \n",
    "(high rates or long times) become `saturated' with changes and no evolutionary signal is detectable amid the noise. \n",
    "Consequently, a happy medium is expected at some intermediate level of sequence divergence, but the àsymptotic' \n",
    "results (for extreme high and low levels of divergence) give no clue as to where this lies.\n",
    "Although there is considerable experience with particular genes and organisms (see Hillis et al. \n",
    "(1996, pp. 336^339) for an extensive list of studies), where comparisons among genes have been made \n",
    "they tend to be evaluated empirically by the congruence of the results obtained among themselves and with\n",
    "researchers'a priori expectations. In addition, a large laboratory e¡ort is needed before even these \n",
    "qualitative conclusions can be reached. The methods introduced in this paper can quantify the e¡ects of\n",
    "varying levels of divergence. They con¢rm the belief that intermediate levels of sequence divergence are \n",
    "most useful, and are able to give estimates of optimal levels of divergence. The necessary analyses can\n",
    "be done before any data are collected. The only other method for assessing which genomic regions are\n",
    "likely to be most useful in phylogenetic questions is that of Yang (1998). This uses simulation \n",
    "to estimate probabilities of successful tree-topology inference. The approach may be extremely \n",
    "time-consuming for realistic problems. The choice of taxa to include in phylogenetic studies has\n",
    "also rarely been discussed. Li et al. (1987) considered the e¡ects of adding outgroup taxa,\n",
    "and Ritland & Clegg (1990) and Maddison et al. (1992) are agreed that if outgroups are to be added, \n",
    "they should not be too distantly related to the ingroup taxa (a conclusion that is con¢rmed in this paper). \n",
    "More recently, consideration of the estimation of large phylogenies has led to the contradictory \n",
    "advice that the number of sequences included in a study be reduced (Kim 1996) and increased (particularly to \n",
    "break long branches in trees (Hillis 1996)) in order to improve inferences (see also Hillis 1998). \n",
    "Although it is not clear that the latter strategy will always be successful (Zharkikh & Li 1993; Kim 1996),\n",
    "    Strimmer & von Haeseler (1996) and Hillis (1996) have\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8243.000000\n",
       "mean       44.482227\n",
       "std        24.906518\n",
       "min        10.000000\n",
       "25%        30.000000\n",
       "50%        40.000000\n",
       "75%        54.000000\n",
       "max       594.000000\n",
       "Name: string, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringlengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8242     65\n",
       "4051     65\n",
       "7513     65\n",
       "4143     65\n",
       "5849     65\n",
       "       ... \n",
       "7740    450\n",
       "4001    465\n",
       "598     465\n",
       "6874    500\n",
       "6540    594\n",
       "Name: string, Length: 982, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringlengths[stringlengths>64].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentence data\n",
    "sentences = df_train.string.to_list()\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tag labels data\n",
    "labels = df_train.label.to_list()\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag to index \n",
    "tag2idx={'0': 0,'1': 1, '2':2}\n",
    "# Index to tag\n",
    "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "- Set gpu environment\n",
    "- Load tokenizer and tokenize\n",
    "- Set 3 embedding, token embedding, mask word embedding, segmentation embedding\n",
    "- Split data set into train and validate, then send them to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual define vocabulary address, if you download the model in local\n",
    "# The vocabulary can download from \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model\"\n",
    "# vocabulary = 'models/xlnet-base-cased/xlnet-base-cased-spiece.model'\n",
    "vocabulary = \"/Users/melloo21/Desktop/NUS Items/CS4248/2024_CS4248/Project/cs4248/xlnet-base-cased-spiece.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With cased model, set do_lower_case = False\n",
    "tokenizer = XLNetTokenizer(vocab_file=vocabulary,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set text input embedding\n",
    "\n",
    "- token id embedding \\\n",
    "input ids: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary\n",
    "\n",
    "- mask embedding \\\n",
    "(optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens\n",
    "\n",
    "- segment embedding \\\n",
    "(optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
    "\n",
    "Although we can have variable length input sentences, XLNet does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length.\n",
    "\n",
    "To “pad” our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n",
    "\n",
    "If a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n",
    "\n",
    "We pad and truncate our sequences so that they all become of length maxlen (“post” indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) pad_sequences is a utility function that we’re borrowing from Keras. It simply handles the truncating and padding of Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Len of the sentence must be the same as the training model\n",
    "# See model's 'max_position_embeddings' = 512\n",
    "\n",
    "max_len  = 64\n",
    "\n",
    "full_input_ids = []\n",
    "full_input_masks = []\n",
    "full_segment_ids = []\n",
    "\n",
    "# WHAT is this seg for?\n",
    "SEG_ID_A   = 0\n",
    "SEG_ID_B   = 1\n",
    "SEG_ID_CLS = 2\n",
    "SEG_ID_SEP = 3\n",
    "SEG_ID_PAD = 4\n",
    "\n",
    "# Unkown Encoding\n",
    "UNK_ID = tokenizer.encode(\"<unk>\")[0]\n",
    "CLS_ID = tokenizer.encode(\"<cls>\")[0]\n",
    "# Sep Encoding\n",
    "SEP_ID = tokenizer.encode(\"<sep>\")[0]\n",
    "MASK_ID = tokenizer.encode(\"<mask>\")[0]\n",
    "EOD_ID = tokenizer.encode(\"<eod>\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.:0\n",
      "sentence: However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).\n",
      "input_ids:[476, 19, 160, 17, 5706, 10693, 153, 9647, 23, 33, 18, 5063, 13, 83, 9774, 4749, 12681, 305, 6142, 4314, 1484, 7035, 34, 1568, 65, 13, 261, 13, 1112, 11401, 33, 231, 5528, 55, 550, 17, 10, 96, 23, 369, 83, 4145, 1396, 19, 2896, 3158, 19, 35, 23, 369, 1580, 167, 96, 3192, 174, 4145, 342, 19, 1545, 19, 1608, 3158, 4, 3]\n",
      "attention_masks:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:1000\n",
      "sentence: (2010). They collected verbal estimates and free-hand measures of outdoor hills using the present device for a set of campus paths ranging from nearly flat (0.5°) to fairly steep (8.6°). Note that the perceived orientation of any specific outdoor hill may depend on a variety of factors in addition to its physical slant (e.g., curvature and viewing distance), but if the free-hand measure represents the NSE orientation, then we should expect the relationship between verbal estimates and free hand (NSE) estimates to remain similar to that observed for near surfaces. In other words, we should be able to make a prediction about the verbal estimates based on the free-hand estimates. ----- Figure 7 to be inserted here ----Figure 7 shows Stigliani et al.’s (2010) numeric estimation data for outdoor slopes plotted against the corresponding free-hand estimates (NSE) for each physical path (physical path inclination is shown next to each data point).\n",
      "input_ids:[17, 10, 10802, 11, 9, 200, 3891, 12097, 4860, 21, 325, 13, 3595, 1858, 20, 5890, 8957, 381, 18, 858, 2774, 28, 24, 257, 20, 3344, 13412, 6221, 40, 896, 2621, 17, 10, 279, 9, 217, 9762, 11, 22, 4874, 7215, 17, 10, 385, 9, 342, 9762, 11, 9, 3828, 29, 18, 8634, 10848, 20, 124, 1240, 5890, 5998, 132, 6355, 31, 4, 3]\n",
      "attention_masks:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:2000\n",
      "sentence: The presence of such micro-cracks provides a potent site for fatigue cracks to nucleate under high-cycle fatigue loading (Peters et al., 2000, 2002).\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 2059, 20, 148, 4012, 13, 369, 16914, 23, 1176, 24, 14782, 335, 28, 12010, 17647, 22, 17, 4216, 6449, 1167, 168, 227, 13, 13324, 12010, 13496, 17, 10, 22570, 23, 17, 993, 1296, 9, 19, 1356, 19, 1760, 11, 9, 4, 3, 4, 3]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:3000\n",
      "sentence: , 1989) in general and to mobile technology acceptance literature in particular (Kim et al., 2007; Kim and Garrison, 2009; Lee and Park, 2008; Liang et al., 2007).\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 19, 3712, 11, 25, 580, 21, 22, 2487, 913, 8257, 4352, 25, 1244, 17, 10, 536, 1490, 17, 993, 1296, 9, 19, 1327, 97, 2927, 21, 27303, 19, 1307, 97, 1863, 21, 1095, 19, 1130, 97, 25832, 17, 993, 1296, 9, 19, 1327, 11, 9, 4, 3, 4, 3]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:4000\n",
      "sentence: A recent study indicated that COX-2 was related to UA-activated proliferation of smooth muscle cells possibly via TXA2 [7].\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 644, 757, 3624, 29, 330, 16304, 13, 184, 30, 1375, 22, 17, 20437, 13, 20769, 17, 12555, 20, 3893, 4965, 2706, 2906, 1647, 17, 14829, 246, 184, 4145, 377, 3158, 9, 4, 3, 4, 3]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:5000\n",
      "sentence: GATA4 is a critical transcription factor of most cardiacexpressed structural genes and hypertrophy-related genes, including the atrial natriuretic peptide (ANP) gene [13,14].\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 457, 20399, 265, 27, 24, 2083, 19654, 3350, 20, 127, 18478, 16508, 68, 8668, 9787, 21, 7666, 23815, 117, 13, 3361, 9787, 19, 208, 18, 24, 15557, 17, 597, 4935, 3196, 2139, 17, 23325, 17, 10, 246, 11526, 11, 7104, 4145, 1743, 19, 1878, 3158, 9, 4, 3, 4, 3]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:6000\n",
      "sentence: The network is based on at least two patterning mechanisms each capable of explaining patterning alone (Pesch and Hülskamp, 2009).\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 1090, 27, 515, 31, 38, 390, 87, 3748, 56, 10574, 231, 3834, 20, 9525, 3748, 56, 1412, 17, 10, 11246, 2800, 21, 27045, 3125, 1701, 19, 1307, 11, 9, 4, 3, 4, 3]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:7000\n",
      "sentence: 2002), may be more commonly observed in the environment (Arbeli and Fuentes 2010).\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1760, 11, 19, 132, 39, 70, 4246, 4887, 25, 18, 1536, 17, 10, 10208, 4751, 150, 21, 5116, 12409, 23, 1077, 11, 9, 4, 3, 4, 3]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:8000\n",
      "sentence: Still, the majority of speech apraxia cases do not seem to carry causal mutations in FOXP2, BCL11A or ERC1 [6, 11, 12].\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2797, 19, 18, 1472, 20, 2077, 24, 7627, 18223, 1057, 112, 50, 1589, 22, 1925, 27260, 14965, 23, 25, 518, 16304, 764, 184, 19, 322, 7416, 1545, 246, 49, 17, 30362, 174, 4145, 342, 19, 506, 19, 421, 3158, 9, 4, 3, 4, 3]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute for each sentence\n",
    "trimmed_sentences_idx = list()\n",
    "for i,sentence in enumerate(sentences):\n",
    "    # Tokenize sentence to token id list [Using the default transformer version -- Sentencepiece model]\n",
    "    tokens_a = tokenizer.encode(sentence)\n",
    "\n",
    "    # Trim the len of text []\n",
    "    # print(f\"sentence lenght = {len(tokens_a)}\")\n",
    "    if(len(tokens_a)>max_len-2):\n",
    "        tokens_a = tokens_a[:max_len-2]\n",
    "        trimmed_sentences_idx.append((i,len(tokens_a) ))\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "     \n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(SEG_ID_A)\n",
    "    # print(\"1 segment_ids length :: \", len(segment_ids))\n",
    "    \n",
    "    # Add <sep> token \n",
    "    tokens.append(SEP_ID)\n",
    "    segment_ids.append(SEG_ID_A)\n",
    "    # Add <cls> token\n",
    "    tokens.append(CLS_ID)\n",
    "    # print(\"2 tokens :: \", tokens)\n",
    "    segment_ids.append(SEG_ID_CLS)\n",
    "    # print(\"3 segment_ids:: \", segment_ids)\n",
    "\n",
    "    # Inputs\n",
    "    input_ids = tokens\n",
    "    # print(\"4 input_ids:: \", input_ids)\n",
    "    # The mask has 0 for real tokens and 1 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [0] * len(input_ids)\n",
    "    # print(\"5 input_mask :: \", input_mask)\n",
    "    \n",
    "    # Zero-pad up to the sequence length at front\n",
    "    if len(input_ids) < max_len:\n",
    "        # This is to ensure inputs are of the correct dimension\n",
    "        delta_len = max_len - len(input_ids)\n",
    "        input_ids = [0] * delta_len + input_ids\n",
    "        input_mask = [1] * delta_len + input_mask\n",
    "        segment_ids = [SEG_ID_PAD] * delta_len + segment_ids\n",
    "\n",
    "    assert len(input_ids) == max_len\n",
    "    assert len(input_mask) == max_len\n",
    "    assert len(segment_ids) == max_len\n",
    "\n",
    "    full_input_ids.append(input_ids)\n",
    "    full_input_masks.append(input_mask)\n",
    "    full_segment_ids.append(segment_ids)\n",
    "\n",
    "    # Every 1000 idx print\n",
    "    if i%1000==0:\n",
    "        print(\"No.:%d\"%(i))\n",
    "        print(\"sentence: %s\"%(sentence))\n",
    "        print(\"input_ids:%s\"%(input_ids))\n",
    "        print(\"attention_masks:%s\"%(input_mask))\n",
    "        print(\"segment_ids:%s\"%(segment_ids))\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsentence lenght = 80\\n1 segment_ids length ::  62\\n2 tokens ::  [476, 19, 160, 17, 5706, 10693, 153, 9647, 23, 33, 18, 5063, 13, 83, 9774, 4749, 12681, 305, 6142, 4314, 1484, 7035, 34, 1568, 65, 13, 261, 13, 1112, 11401, 33, 231, 5528, 55, 550, 17, 10, 96, 23, 369, 83, 4145, 1396, 19, 2896, 3158, 19, 35, 23, 369, 1580, 167, 96, 3192, 174, 4145, 342, 19, 1545, 19, 1608, 3158, 4, 3]\\n3 segment_ids::  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\\n4 input_ids::  [476, 19, 160, 17, 5706, 10693, 153, 9647, 23, 33, 18, 5063, 13, 83, 9774, 4749, 12681, 305, 6142, 4314, 1484, 7035, 34, 1568, 65, 13, 261, 13, 1112, 11401, 33, 231, 5528, 55, 550, 17, 10, 96, 23, 369, 83, 4145, 1396, 19, 2896, 3158, 19, 35, 23, 369, 1580, 167, 96, 3192, 174, 4145, 342, 19, 1545, 19, 1608, 3158, 4, 3]\\n5 input_mask ::  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\nNo.:0\\nsentence: However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).\\ninput_ids:[476, 19, 160, 17, 5706, 10693, 153, 9647, 23, 33, 18, 5063, 13, 83, 9774, 4749, 12681, 305, 6142, 4314, 1484, 7035, 34, 1568, 65, 13, 261, 13, 1112, 11401, 33, 231, 5528, 55, 550, 17, 10, 96, 23, 369, 83, 4145, 1396, 19, 2896, 3158, 19, 35, 23, 369, 1580, 167, 96, 3192, 174, 4145, 342, 19, 1545, 19, 1608, 3158, 4, 3]\\nattention_masks:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\nsegment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Output\n",
    "\"\"\"\n",
    "sentence lenght = 80\n",
    "1 segment_ids length ::  62\n",
    "2 tokens ::  [476, 19, 160, 17, 5706, 10693, 153, 9647, 23, 33, 18, 5063, 13, 83, 9774, 4749, 12681, 305, 6142, 4314, 1484, 7035, 34, 1568, 65, 13, 261, 13, 1112, 11401, 33, 231, 5528, 55, 550, 17, 10, 96, 23, 369, 83, 4145, 1396, 19, 2896, 3158, 19, 35, 23, 369, 1580, 167, 96, 3192, 174, 4145, 342, 19, 1545, 19, 1608, 3158, 4, 3]\n",
    "3 segment_ids::  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    "4 input_ids::  [476, 19, 160, 17, 5706, 10693, 153, 9647, 23, 33, 18, 5063, 13, 83, 9774, 4749, 12681, 305, 6142, 4314, 1484, 7035, 34, 1568, 65, 13, 261, 13, 1112, 11401, 33, 231, 5528, 55, 550, 17, 10, 96, 23, 369, 83, 4145, 1396, 19, 2896, 3158, 19, 35, 23, 369, 1580, 167, 96, 3192, 174, 4145, 342, 19, 1545, 19, 1608, 3158, 4, 3]\n",
    "5 input_mask ::  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "No.:0\n",
    "sentence: However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).\n",
    "input_ids:[476, 19, 160, 17, 5706, 10693, 153, 9647, 23, 33, 18, 5063, 13, 83, 9774, 4749, 12681, 305, 6142, 4314, 1484, 7035, 34, 1568, 65, 13, 261, 13, 1112, 11401, 33, 231, 5528, 55, 550, 17, 10, 96, 23, 369, 83, 4145, 1396, 19, 2896, 3158, 19, 35, 23, 369, 1580, 167, 96, 3192, 174, 4145, 342, 19, 1545, 19, 1608, 3158, 4, 3]\n",
    "attention_masks:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Make label into id\n",
    "tags = [tag2idx[str(lab)] for lab in labels]\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476,\n",
       " 19,\n",
       " 160,\n",
       " 17,\n",
       " 5706,\n",
       " 10693,\n",
       " 153,\n",
       " 9647,\n",
       " 23,\n",
       " 33,\n",
       " 18,\n",
       " 5063,\n",
       " 13,\n",
       " 83,\n",
       " 9774,\n",
       " 4749,\n",
       " 12681,\n",
       " 305,\n",
       " 6142,\n",
       " 4314,\n",
       " 1484,\n",
       " 7035,\n",
       " 34,\n",
       " 1568,\n",
       " 65,\n",
       " 13,\n",
       " 261,\n",
       " 13,\n",
       " 1112,\n",
       " 11401,\n",
       " 33,\n",
       " 231,\n",
       " 5528,\n",
       " 55,\n",
       " 550,\n",
       " 17,\n",
       " 10,\n",
       " 96,\n",
       " 23,\n",
       " 369,\n",
       " 83,\n",
       " 4145,\n",
       " 1396,\n",
       " 19,\n",
       " 2896,\n",
       " 3158,\n",
       " 19,\n",
       " 35,\n",
       " 23,\n",
       " 369,\n",
       " 1580,\n",
       " 167,\n",
       " 96,\n",
       " 3192,\n",
       " 174,\n",
       " 4145,\n",
       " 342,\n",
       " 19,\n",
       " 1545,\n",
       " 19,\n",
       " 1608,\n",
       " 3158,\n",
       " 4,\n",
       " 3]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(\n",
    "    full_input_ids, \n",
    "    tags,\n",
    "    full_input_masks,\n",
    "    full_segment_ids, \n",
    "    random_state=4, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5770, 2473, 5770, 2473)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_inputs),len(val_inputs),len(tr_segs),len(val_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "tr_segs = torch.tensor(tr_segs)\n",
    "val_segs = torch.tensor(val_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put data into data loader\n",
    "\n",
    "# Set batch num\n",
    "batch_num = 32\n",
    "\n",
    "# Set token embedding, attention embedding, segment embedding\n",
    "train_data = TensorDataset(tr_inputs, tr_masks,tr_segs, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks,val_segs, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this document, contain confg(txt) and weight(bin) files\n",
    "# The folder must contain: pytorch_model.bin, config.json\n",
    "model_file_address = '/Users/melloo21/Desktop/NUS Items/CS4248/2024_CS4248/Project/cs4248/model/models/xlnet-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at /Users/melloo21/Desktop/NUS Items/CS4248/2024_CS4248/Project/cs4248/model/models/xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Will load config and weight with from_pretrained()\n",
    "# Recommand download the model before using\n",
    "# Download model from \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin\"\n",
    "# Download model from \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json\" \n",
    "# ATTENTION!, rename \"xlnet-base-cased-pytorch_model.bin\" into \"pytorch_model.bin\"\n",
    "# rename \"xlnet-base-cased-config.json\" inot \"config.json\"\n",
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    model_file_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to GPU,if you are using GPU machine\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 5\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Cacluate train optimiazaion num\n",
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning method -- Manual Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: fine tuning all the layers \n",
    "# False: only fine tuning the classifier layers\n",
    "# Since XLNet in 'pytorch_transformer' did not contian classifier layers\n",
    "# FULL_FINETUNING = True need to set True\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5770\n",
      "  Batch size = 32\n",
      "  Num steps = 905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [01:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m b_input_ids, b_input_mask, b_segs,b_labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_segs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb_input_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_gpu\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# When multi gpu, average it\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1545\u001b[0m, in \u001b[0;36mXLNetForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1545\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperm_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperm_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_mems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_mems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1563\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_summary(output)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1237\u001b[0m, in \u001b[0;36mXLNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m   1235\u001b[0m     hidden_states\u001b[38;5;241m.\u001b[39mappend((output_h, output_g) \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m output_h)\n\u001b[0;32m-> 1237\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_tgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1249\u001b[0m output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:508\u001b[0m, in \u001b[0;36mXLNetLayer.forward\u001b[0;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    497\u001b[0m     output_h,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    507\u001b[0m ):\n\u001b[0;32m--> 508\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m     output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:439\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.forward\u001b[0;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    436\u001b[0m k_head_r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibh,hnd->ibnd\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# core attention ops\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m attn_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_attn_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_head_r\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    451\u001b[0m     attn_vec, attn_prob \u001b[38;5;241m=\u001b[39m attn_vec\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.rel_attn_core\u001b[0;34m(self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# attention probability\u001b[39;00m\n\u001b[1;32m    300\u001b[0m attn_prob \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attn_score, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m attn_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4248_project/lib/python3.10/site-packages/torch/nn/functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "# Fine tuning the model \n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_segs,b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids =b_input_ids,token_type_ids=b_segs, input_mask = b_input_mask,labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_address = \"/content/drive/MyDrive/CS4248/model xlnet-base-cased\"\n",
    "\n",
    "# Will load config and weight with from_pretrained()\n",
    "# Recommand download the model before using\n",
    "# Download model from \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin\"\n",
    "# Download model from \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json\"\n",
    "# ATTENTION!, rename \"xlnet-base-cased-pytorch_model.bin\" into \"pytorch_model.bin\"\n",
    "# rename \"xlnet-base-cased-config.json\" inot \"config.json\"\n",
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    model_file_address,num_labels=len(tag2idx))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Set epoch and grad max num\n",
    "epochs = 5\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Cacluate train optimiazaion num\n",
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs\n",
    "\n",
    "# True: fine tuning all the layers\n",
    "# False: only fine tuning the classifier layers\n",
    "# Since XLNet in 'pytorch_transformer' did not contian classifier layers\n",
    "# FULL_FINETUNING = True need to set True\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "# Fine tuning the model\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_segs,b_labels = batch\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids =b_input_ids,token_type_ids=b_segs, input_mask = b_input_mask,labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set acc funtion\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "# Setting model into eval state\n",
    "model.eval()\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "y_true = []\n",
    "y_predict = []\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_segs,b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids =b_input_ids,token_type_ids=b_segs, input_mask = b_input_mask,labels=b_labels)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "    # Get textclassification predict result\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "#     print(tmp_eval_accuracy)\n",
    "#     print(np.argmax(logits, axis=1))\n",
    "#     print(label_ids)\n",
    "\n",
    "    # Save predict and real label reuslt for analyze\n",
    "    for predict in np.argmax(logits, axis=1):\n",
    "        y_predict.append(predict)\n",
    "\n",
    "    for real_result in label_ids.tolist():\n",
    "        y_true.append(real_result)\n",
    "\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / len(val_inputs)\n",
    "loss = tr_loss/nb_tr_steps\n",
    "result = {'eval_loss': eval_loss,\n",
    "                  'eval_accuracy': eval_accuracy,\n",
    "                  'loss': loss}\n",
    "report = classification_report(y_pred=np.array(y_predict),y_true=np.array(y_true))\n",
    "\n",
    "# Save the report into file\n",
    "output_eval_file = os.path.join(savepath, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    print(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        print(\"  %s = %s\"%(key, str(result[key])))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    print(report)\n",
    "    writer.write(\"\\n\\n\")\n",
    "    writer.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
